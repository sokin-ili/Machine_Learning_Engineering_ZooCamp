{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:14.375368Z",
     "start_time": "2024-01-20T22:48:12.421792Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# os.chdir(\"/home/nikosili/projects/datatalks_zoomcamp/Machine_Learning_Engineering_ZooCamp/QA_challenge\")\n",
    "# \n",
    "warnings.filterwarnings('ignore')\n",
    "# sns.set_theme()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:17.574470Z",
     "start_time": "2024-01-20T22:48:17.466273Z"
    }
   },
   "outputs": [],
   "source": [
    "train_a = pd.read_csv('data/train_answers.csv')\n",
    "train_q = pd.read_csv('data/train_questions.csv')\n",
    "test_q = pd.read_csv('data/test_questions.csv')\n",
    "test_a = pd.read_csv('data/test_answers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:18.312294Z",
     "start_time": "2024-01-20T22:48:18.304294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For categorical target set, where the distribution is imbalanced (for example, 90/10) what approach should be used? \n",
      "\n",
      " Alexey\n",
      "Should we use something non-standard there or can we just go with the usual things we learned in the course?\n",
      "Hamed\n",
      "You just need to test different strategies. Something I noticed – if you have so many parse subclasses in your categorical [inaudible], you should be careful about using one-hot encoding. You might say you can use ordinal encoding, if your data in nature had some order. It will be useful. In my particular data, I couldn't have domain knowledge. I didn't know what the subclasses were, so I couldn't decide which strategy I should choose. But if you have the domain knowledge, that’s the key here, I think.\n"
     ]
    }
   ],
   "source": [
    "print(train_q.iloc[0][1],'\\n\\n',train_a.iloc[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:27.558520Z",
     "start_time": "2024-01-20T22:48:27.534372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_answers</th>\n",
       "      <th>answer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79062</td>\n",
       "      <td>For categorical target set, where the distribu...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>156400,754877,105368,643810,912439</td>\n",
       "      <td>156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>468946</td>\n",
       "      <td>Is there anything that we are not allowed to u...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>641330,634887,912439,425941,642829</td>\n",
       "      <td>634887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968800</td>\n",
       "      <td>I have been catching up and have been doing ho...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>954016,167856,75919,36798,838013</td>\n",
       "      <td>954016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>688404</td>\n",
       "      <td>Could you please explain what code we should l...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>198661,629898,686577,3699,141765</td>\n",
       "      <td>3699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63921</td>\n",
       "      <td>Is it just me or does the model have really ba...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>754877,604487,912439,858915,425941</td>\n",
       "      <td>858915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                           question  \\\n",
       "0        79062  For categorical target set, where the distribu...   \n",
       "1       468946  Is there anything that we are not allowed to u...   \n",
       "2       968800  I have been catching up and have been doing ho...   \n",
       "3       688404  Could you please explain what code we should l...   \n",
       "4        63921  Is it just me or does the model have really ba...   \n",
       "\n",
       "                      course  year                   candidate_answers  \\\n",
       "0  Machine Learning Zoomcamp  2021  156400,754877,105368,643810,912439   \n",
       "1  Machine Learning Zoomcamp  2021  641330,634887,912439,425941,642829   \n",
       "2  Data Engineering Zoomcamp  2022    954016,167856,75919,36798,838013   \n",
       "3  Data Engineering Zoomcamp  2022    198661,629898,686577,3699,141765   \n",
       "4  Machine Learning Zoomcamp  2021  754877,604487,912439,858915,425941   \n",
       "\n",
       "   answer_id  \n",
       "0     156400  \n",
       "1     634887  \n",
       "2     954016  \n",
       "3       3699  \n",
       "4     858915  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:27.835364Z",
     "start_time": "2024-01-20T22:48:27.820114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>course</th>\n",
       "      <th>year</th>\n",
       "      <th>attachments_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156400</td>\n",
       "      <td>Alexey\\nShould we use something non-standard t...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>634887</td>\n",
       "      <td>No, I don't think there is anything you cannot...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>954016</td>\n",
       "      <td>Alexey\\nYes, you will be. You can submit the p...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3699</td>\n",
       "      <td>Alexey\\nI think the question refers to the hom...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858915</td>\n",
       "      <td>Dmitry\\nIt's fine, because this is the showcas...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                             answer  \\\n",
       "0     156400  Alexey\\nShould we use something non-standard t...   \n",
       "1     634887  No, I don't think there is anything you cannot...   \n",
       "2     954016  Alexey\\nYes, you will be. You can submit the p...   \n",
       "3       3699  Alexey\\nI think the question refers to the hom...   \n",
       "4     858915  Dmitry\\nIt's fine, because this is the showcas...   \n",
       "\n",
       "                      course  year attachments_files  \n",
       "0  Machine Learning Zoomcamp  2021               NaN  \n",
       "1  Machine Learning Zoomcamp  2021               NaN  \n",
       "2  Data Engineering Zoomcamp  2022               NaN  \n",
       "3  Data Engineering Zoomcamp  2022               NaN  \n",
       "4  Machine Learning Zoomcamp  2021               NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:28.069207Z",
     "start_time": "2024-01-20T22:48:28.035945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>rec_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>647840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>605034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>622805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>619694</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>282338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>278678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>274935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>273744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>994378</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_id  rec_count\n",
       "279       647840          2\n",
       "0             26          1\n",
       "261       605034          1\n",
       "270       622805          1\n",
       "269       619694          1\n",
       "..           ...        ...\n",
       "129       282338          1\n",
       "128       278678          1\n",
       "127       274935          1\n",
       "126       273744          1\n",
       "395       994378          1\n",
       "\n",
       "[396 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_1 = train_q.groupby(['question_id', 'answer_id']).size().reset_index().rename(columns={0:'rec_count'})\n",
    "temp_1.groupby('question_id').size().reset_index().rename(columns={0:'rec_count'}).sort_values('rec_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:28.878502Z",
     "start_time": "2024-01-20T22:48:28.853962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>rec_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>774098</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>673096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>668438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>666820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>325935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>323402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>322444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>317865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>998091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     answer_id  rec_count\n",
       "310     774098          2\n",
       "0         3699          1\n",
       "270     673096          1\n",
       "269     668438          1\n",
       "268     666820          1\n",
       "..         ...        ...\n",
       "129     325935          1\n",
       "128     323402          1\n",
       "127     322444          1\n",
       "126     317865          1\n",
       "395     998091          1\n",
       "\n",
       "[396 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_1.groupby('answer_id').size().reset_index().rename(columns={0:'rec_count'}).sort_values('rec_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:29.071266Z",
     "start_time": "2024-01-20T22:48:29.055601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we use PyTorch instead of TensorFlow?\n",
      "For rating data (number of stars) can we use a linear regression model? \n",
      "\n",
      "261    Yes, you can.\n",
      "358    Yes, you can.\n",
      "Name: answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(train_q[train_q['answer_id'] == 774098].answr)\n",
    "print(train_q[train_q['answer_id'] == 774098].question.iloc[0])\n",
    "print(train_q[train_q['answer_id'] == 774098].question.iloc[1],'\\n')\n",
    "print(train_a[train_a['answer_id'] == 774098].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:29.271636Z",
     "start_time": "2024-01-20T22:48:29.239484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 6)\n",
      "(396, 6)\n",
      "Can we use PyTorch instead of TensorFlow?\n"
     ]
    }
   ],
   "source": [
    "print(train_q.shape)\n",
    "temp = train_q.drop_duplicates(subset=['answer_id'], keep='first')\n",
    "print(temp.shape)\n",
    "print(temp[temp['answer_id'] == 774098].question.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:29.471246Z",
     "start_time": "2024-01-20T22:48:29.455577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72     How many submissions were there this week?\n",
       "159    How many submissions were there this week?\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q[train_q['question_id'] == 647840].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:29.671866Z",
     "start_time": "2024-01-20T22:48:29.639655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 159]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_q[train_q['question_id'] == 647840].question.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:30.444824Z",
     "start_time": "2024-01-20T22:48:30.420277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think around 200? 204, I think. Something like this. \n",
      "\n",
      "Alexey\n",
      "246, which is 150 less than last week. Last week, it was 404. I don’t know if this homework was maybe a bit more difficult. But the next one is also fun. I think if you liked the homework we prepared for this week, the next one will also be fun. Right, Dmitry? \n",
      "Dmitry\n",
      "Yeah, for sure.\n"
     ]
    }
   ],
   "source": [
    "print(train_a[(train_a['answer_id'] == 276086) | (train_a['answer_id'] == 964992)].answer.iloc[0],'\\n')\n",
    "print(train_a[(train_a['answer_id'] == 276086) | (train_a['answer_id'] == 964992)].answer.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:31.275172Z",
     "start_time": "2024-01-20T22:48:31.267172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 6)\n",
      "(394, 6)\n"
     ]
    }
   ],
   "source": [
    "print(temp.shape)\n",
    "temp = temp.drop(list(train_q[train_q['question_id'] == 647840].question.index))\n",
    "print(temp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T22:48:41.549563Z",
     "start_time": "2024-01-20T22:48:41.524967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>course</th>\n",
       "      <th>year</th>\n",
       "      <th>attachments_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>774098</td>\n",
       "      <td>Yes, you can.</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    answer_id         answer                     course  year  \\\n",
       "73     774098  Yes, you can.  Data Engineering Zoomcamp  2023   \n",
       "\n",
       "   attachments_files  \n",
       "73               NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_a[test_a.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:00:19.000485Z",
     "start_time": "2024-01-20T23:00:18.976390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_answers</th>\n",
       "      <th>answer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79062</td>\n",
       "      <td>For categorical target set, where the distribu...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>156400,754877,105368,643810,912439</td>\n",
       "      <td>156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>468946</td>\n",
       "      <td>Is there anything that we are not allowed to u...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>641330,634887,912439,425941,642829</td>\n",
       "      <td>634887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968800</td>\n",
       "      <td>I have been catching up and have been doing ho...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>954016,167856,75919,36798,838013</td>\n",
       "      <td>954016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>688404</td>\n",
       "      <td>Could you please explain what code we should l...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>198661,629898,686577,3699,141765</td>\n",
       "      <td>3699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63921</td>\n",
       "      <td>Is it just me or does the model have really ba...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>754877,604487,912439,858915,425941</td>\n",
       "      <td>858915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                           question  \\\n",
       "0        79062  For categorical target set, where the distribu...   \n",
       "1       468946  Is there anything that we are not allowed to u...   \n",
       "2       968800  I have been catching up and have been doing ho...   \n",
       "3       688404  Could you please explain what code we should l...   \n",
       "4        63921  Is it just me or does the model have really ba...   \n",
       "\n",
       "                      course  year                   candidate_answers  \\\n",
       "0  Machine Learning Zoomcamp  2021  156400,754877,105368,643810,912439   \n",
       "1  Machine Learning Zoomcamp  2021  641330,634887,912439,425941,642829   \n",
       "2  Data Engineering Zoomcamp  2022    954016,167856,75919,36798,838013   \n",
       "3  Data Engineering Zoomcamp  2022    198661,629898,686577,3699,141765   \n",
       "4  Machine Learning Zoomcamp  2021  754877,604487,912439,858915,425941   \n",
       "\n",
       "   answer_id  \n",
       "0     156400  \n",
       "1     634887  \n",
       "2     954016  \n",
       "3       3699  \n",
       "4     858915  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:03:06.936781Z",
     "start_time": "2024-01-20T23:03:06.928753Z"
    }
   },
   "outputs": [],
   "source": [
    "can_ans = train_q.iloc[0]['candidate_answers'].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:11:06.657349Z",
     "start_time": "2024-01-20T23:11:06.633238Z"
    }
   },
   "outputs": [],
   "source": [
    "# for idx in can_ans:\n",
    "#     print(idx,'\\n', train_a[train_a['answer_id']==int(idx)].iloc[0].answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:24:41.681658Z",
     "start_time": "2024-01-20T23:24:41.657656Z"
    }
   },
   "outputs": [],
   "source": [
    "# text=''\n",
    "# for idx in can_ans:\n",
    "#     temp_text=train_a[train_a['answer_id']==int(idx)].iloc[0].answer\n",
    "#     text+=temp_text\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T23:29:22.636210Z",
     "start_time": "2024-01-20T23:29:22.601871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>course_question</th>\n",
       "      <th>year_question</th>\n",
       "      <th>candidate_answers</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>course_answer</th>\n",
       "      <th>year_answer</th>\n",
       "      <th>attachments_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79062</td>\n",
       "      <td>For categorical target set, where the distribu...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>156400,754877,105368,643810,912439</td>\n",
       "      <td>156400</td>\n",
       "      <td>Alexey\\nShould we use something non-standard t...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>468946</td>\n",
       "      <td>Is there anything that we are not allowed to u...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>641330,634887,912439,425941,642829</td>\n",
       "      <td>634887</td>\n",
       "      <td>No, I don't think there is anything you cannot...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968800</td>\n",
       "      <td>I have been catching up and have been doing ho...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>954016,167856,75919,36798,838013</td>\n",
       "      <td>954016</td>\n",
       "      <td>Alexey\\nYes, you will be. You can submit the p...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>688404</td>\n",
       "      <td>Could you please explain what code we should l...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>198661,629898,686577,3699,141765</td>\n",
       "      <td>3699</td>\n",
       "      <td>Alexey\\nI think the question refers to the hom...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63921</td>\n",
       "      <td>Is it just me or does the model have really ba...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>754877,604487,912439,858915,425941</td>\n",
       "      <td>858915</td>\n",
       "      <td>Dmitry\\nIt's fine, because this is the showcas...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                           question  \\\n",
       "0        79062  For categorical target set, where the distribu...   \n",
       "1       468946  Is there anything that we are not allowed to u...   \n",
       "2       968800  I have been catching up and have been doing ho...   \n",
       "3       688404  Could you please explain what code we should l...   \n",
       "4        63921  Is it just me or does the model have really ba...   \n",
       "\n",
       "             course_question  year_question  \\\n",
       "0  Machine Learning Zoomcamp           2021   \n",
       "1  Machine Learning Zoomcamp           2021   \n",
       "2  Data Engineering Zoomcamp           2022   \n",
       "3  Data Engineering Zoomcamp           2022   \n",
       "4  Machine Learning Zoomcamp           2021   \n",
       "\n",
       "                    candidate_answers  answer_id  \\\n",
       "0  156400,754877,105368,643810,912439     156400   \n",
       "1  641330,634887,912439,425941,642829     634887   \n",
       "2    954016,167856,75919,36798,838013     954016   \n",
       "3    198661,629898,686577,3699,141765       3699   \n",
       "4  754877,604487,912439,858915,425941     858915   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Alexey\\nShould we use something non-standard t...   \n",
       "1  No, I don't think there is anything you cannot...   \n",
       "2  Alexey\\nYes, you will be. You can submit the p...   \n",
       "3  Alexey\\nI think the question refers to the hom...   \n",
       "4  Dmitry\\nIt's fine, because this is the showcas...   \n",
       "\n",
       "               course_answer  year_answer attachments_files  \n",
       "0  Machine Learning Zoomcamp         2021               NaN  \n",
       "1  Machine Learning Zoomcamp         2021               NaN  \n",
       "2  Data Engineering Zoomcamp         2022               NaN  \n",
       "3  Data Engineering Zoomcamp         2022               NaN  \n",
       "4  Machine Learning Zoomcamp         2021               NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's marge the train questions and answers in one dataframe\n",
    "train_merged_df = pd.merge(\n",
    "    temp, train_a, on='answer_id', how='inner', suffixes=('_question', '_answer')\n",
    ")\n",
    "train_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T00:05:01.306148Z",
     "start_time": "2024-01-21T00:05:01.298146Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['context'])\n",
    "temp_2 = []\n",
    "for i in train_merged_df.candidate_answers:\n",
    "    temp_2.append(i.split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.context.iloc[0]\n",
    "len(temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T00:05:02.612339Z",
     "start_time": "2024-01-21T00:05:01.825712Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in temp_2:\n",
    "    text = ''\n",
    "    for j in i:\n",
    "        text_temp=train_a[train_a['answer_id']==int(j)].iloc[0].answer\n",
    "        text+=text_temp\n",
    "    con = {'context':text}\n",
    "    df_temp = pd.DataFrame([con])\n",
    "    # df = df.append(df_temp, ignore_index=True)\n",
    "    df = pd.concat([df, df_temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T00:05:06.183099Z",
     "start_time": "2024-01-21T00:05:06.167051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexey\\nShould we use something non-standard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44. Two people submitted twice. So actually, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexey\\nYes, you will be. You can submit the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexey\\nIt's not over for you, because we only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't know if you're referring to this. [ima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>I took this course a while ago. Back then it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>You mean another iteration of the course? Yes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Ankush\\nIf you're talking specifically about i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>If two people want to work on the same dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>No, you do not. Building locally is fine.I don...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context\n",
       "0    Alexey\\nShould we use something non-standard t...\n",
       "1    44. Two people submitted twice. So actually, i...\n",
       "2    Alexey\\nYes, you will be. You can submit the p...\n",
       "3    Alexey\\nIt's not over for you, because we only...\n",
       "4    I don't know if you're referring to this. [ima...\n",
       "..                                                 ...\n",
       "390  I took this course a while ago. Back then it w...\n",
       "391  You mean another iteration of the course? Yes....\n",
       "392  Ankush\\nIf you're talking specifically about i...\n",
       "393  If two people want to work on the same dataset...\n",
       "394  No, you do not. Building locally is fine.I don...\n",
       "\n",
       "[395 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T00:09:26.577996Z",
     "start_time": "2024-01-21T00:09:26.553840Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train_merged_df, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>course</th>\n",
       "      <th>year</th>\n",
       "      <th>attachments_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156400</td>\n",
       "      <td>Alexey\\nShould we use something non-standard t...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>634887</td>\n",
       "      <td>No, I don't think there is anything you cannot...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>954016</td>\n",
       "      <td>Alexey\\nYes, you will be. You can submit the p...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3699</td>\n",
       "      <td>Alexey\\nI think the question refers to the hom...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858915</td>\n",
       "      <td>Dmitry\\nIt's fine, because this is the showcas...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>831391</td>\n",
       "      <td>Yes, it can. It's really dataset dependent. Fo...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>651754</td>\n",
       "      <td>Let's say I do “import numpy as np” and then, ...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>733226</td>\n",
       "      <td>Alexey\\nSplunk – I don’t know. It's not a data...</td>\n",
       "      <td>Data Engineering Zoomcamp</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>422297</td>\n",
       "      <td>Yes, it was not mentioned. But what was mentio...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>558889</td>\n",
       "      <td>Alexey\\nI guess the question is why we need to...</td>\n",
       "      <td>Machine Learning Zoomcamp</td>\n",
       "      <td>2021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     answer_id                                             answer  \\\n",
       "0       156400  Alexey\\nShould we use something non-standard t...   \n",
       "1       634887  No, I don't think there is anything you cannot...   \n",
       "2       954016  Alexey\\nYes, you will be. You can submit the p...   \n",
       "3         3699  Alexey\\nI think the question refers to the hom...   \n",
       "4       858915  Dmitry\\nIt's fine, because this is the showcas...   \n",
       "..         ...                                                ...   \n",
       "392     831391  Yes, it can. It's really dataset dependent. Fo...   \n",
       "393     651754  Let's say I do “import numpy as np” and then, ...   \n",
       "394     733226  Alexey\\nSplunk – I don’t know. It's not a data...   \n",
       "395     422297  Yes, it was not mentioned. But what was mentio...   \n",
       "396     558889  Alexey\\nI guess the question is why we need to...   \n",
       "\n",
       "                        course  year attachments_files  \n",
       "0    Machine Learning Zoomcamp  2021               NaN  \n",
       "1    Machine Learning Zoomcamp  2021               NaN  \n",
       "2    Data Engineering Zoomcamp  2022               NaN  \n",
       "3    Data Engineering Zoomcamp  2022               NaN  \n",
       "4    Machine Learning Zoomcamp  2021               NaN  \n",
       "..                         ...   ...               ...  \n",
       "392  Machine Learning Zoomcamp  2021               NaN  \n",
       "393  Machine Learning Zoomcamp  2021               NaN  \n",
       "394  Data Engineering Zoomcamp  2022               NaN  \n",
       "395  Machine Learning Zoomcamp  2021               NaN  \n",
       "396  Machine Learning Zoomcamp  2021               NaN  \n",
       "\n",
       "[397 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Available device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [01:31<00:00, 4.83MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting pre-trained BERT model\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model.to(device) # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    \"\"\"\n",
    "    Function for getting text embeddings using BERT\n",
    "    Returns one embedding as an average of the words embeddings of the text \n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    return outputs['last_hidden_state'][0].mean(dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_sentence = tokenizer.encode(test_sentence, padding=True, truncation=True,max_length=50, add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get questions and answers embeddings for the train part\n",
    "train_question_embeddings = train['question'].apply(get_bert_embeddings)\n",
    "train_answer_embeddings = train['answer'].apply(get_bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "train_question_embeddings_standardized = scaler.fit_transform(np.array(train_question_embeddings.tolist()))\n",
    "train_answer_embeddings_standardized = scaler.transform(np.array(train_answer_embeddings.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(df_questions, df_answers):\n",
    "    \"\"\"\n",
    "    Function that finds the best answer to each question according to their similarity.\n",
    "    \"\"\"\n",
    "    predicted_answers = []\n",
    "    predicted_answer_ids = []\n",
    "\n",
    "    for index, row in df_questions.iterrows():\n",
    "        question_text = row['question']\n",
    "        candidate_answer_ids = [int(answer_id) for answer_id in row['candidate_answers'].split(\",\")]\n",
    "\n",
    "        # Getting questions embeddings\n",
    "        question_embedding = get_bert_embeddings(question_text)\n",
    "        question_embedding_standardized = scaler.transform(question_embedding.reshape(1, -1))\n",
    "\n",
    "        # Getting answer candidate embeddings\n",
    "        candidate_answers_df = df_answers[df_answers['answer_id'].isin(candidate_answer_ids)]\n",
    "        candidate_answer_embeddings = candidate_answers_df['answer'].apply(get_bert_embeddings)\n",
    "        candidate_answer_embeddings_standardized = scaler.transform(np.array(candidate_answer_embeddings.tolist()))\n",
    "        standardized = scaler.transform(np.array(candidate_answer_embeddings.tolist()))\n",
    "\n",
    "        # Calculating similarity between question and answers embeddings\n",
    "        similarities = cosine_similarity(question_embedding_standardized, candidate_answer_embeddings_standardized).flatten()\n",
    "\n",
    "        # Taking index of the best answer candidate\n",
    "        best_answer_index = similarities.argmax()\n",
    "\n",
    "        predicted_answer = candidate_answers_df.iloc[best_answer_index]['answer']\n",
    "        predicted_answer_id = candidate_answers_df.iloc[best_answer_index]['answer_id']\n",
    "        predicted_answers.append(predicted_answer)\n",
    "        predicted_answer_ids.append(predicted_answer_id)\n",
    "        \n",
    "    return predicted_answer_ids, predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m train_predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m: train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m],     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate_answers\u001b[39m\u001b[38;5;124m'\u001b[39m: train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate_answers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_id\u001b[39m\u001b[38;5;124m'\u001b[39m: train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m })\n\u001b[0;32m      8\u001b[0m train_predictions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_answer_id\u001b[39m\u001b[38;5;124m'\u001b[39m], train_predictions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_answer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m train_predictions_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[121], line 18\u001b[0m, in \u001b[0;36mget_predictions\u001b[1;34m(df_questions, df_answers)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Getting answer candidate embeddings\u001b[39;00m\n\u001b[0;32m     17\u001b[0m candidate_answers_df \u001b[38;5;241m=\u001b[39m df_answers[df_answers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(candidate_answer_ids)]\n\u001b[1;32m---> 18\u001b[0m candidate_answer_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcandidate_answers_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_bert_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m candidate_answer_embeddings_standardized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39marray(candidate_answer_embeddings\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[0;32m     20\u001b[0m standardized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39marray(candidate_answer_embeddings\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[108], line 6\u001b[0m, in \u001b[0;36mget_bert_embeddings\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embeddings\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Function for getting text embeddings using BERT\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Returns one embedding as an average of the words embeddings of the text \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2906\u001b[0m     )\n\u001b[0;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2979\u001b[0m )\n\u001b[1;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    717\u001b[0m     )\n\u001b[1;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    723\u001b[0m     first_ids,\n\u001b[0;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    739\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[0;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:423\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[1;32m--> 423\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qa\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:525\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m    524\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[1;32m--> 525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_is_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_predictions_df = pd.DataFrame({\n",
    "    'question_id': train['question_id'],     \n",
    "    'question': train['question'],\n",
    "    'candidate_answers': train['candidate_answers'],\n",
    "    'answer_id': train['answer_id'],\n",
    "})\n",
    "\n",
    "train_predictions_df['predicted_answer_id'], train_predictions_df['predicted_answer'] = \\\n",
    "    get_predictions(train_q, train_a)\n",
    "\n",
    "train_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_df.head()\n",
    "\n",
    "# Accuracy calculation\n",
    "accuracy = (train_predictions_df['predicted_answer'] == train_answers_df['answer']).mean()\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Now we can use the same approach to get answers for the test part.\n",
    "test_questions_df = test_questions_df.drop_duplicates(subset='question_id')\n",
    "test_questions_df.shape\n",
    "\n",
    "# Creating the dataframe for the test part\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    'question_id': test_questions_df['question_id'], \n",
    "})\n",
    "\n",
    "test_predictions_df['predicted_answer_id'], test_predictions_df['predicted_answer'] = \\\n",
    "    get_predictions(test_questions_df, test_answers_df)\n",
    "\n",
    "test_predictions_df.head()\n",
    "\n",
    "test_predictions_df[['question_id', 'predicted_answer_id']].to_csv('BERT_sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
