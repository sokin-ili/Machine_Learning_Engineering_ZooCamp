answer_id,answer,course,year,attachments_files
156400,"Alexey
Should we use something non-standard there or can we just go with the usual things we learned in the course?
Hamed
You just need to test different strategies. Something I noticed – if you have so many parse subclasses in your categorical [inaudible], you should be careful about using one-hot encoding. You might say you can use ordinal encoding, if your data in nature had some order. It will be useful. In my particular data, I couldn't have domain knowledge. I didn't know what the subclasses were, so I couldn't decide which strategy I should choose. But if you have the domain knowledge, that’s the key here, I think.",Machine Learning Zoomcamp,2021,
634887,"No, I don't think there is anything you cannot use. I just want to ask you to document it, if there is something you use that is not a part of the course. For somebody who will be reviewing your project, it will be new information. So just give them some context and explain, “I used GridSearchCV because this is easier and this is what you can do.” Then you describe it, and you give your peers a chance to learn from what you’re doing. This way, they will not be lost and will be able to grade your homework. If you do that, you can use whatever you want. 
From the materials in the course, like if you use logistic regression, or rich regression, or XGBoost, or random forest, then you don't need to really go into details when documenting because I would expect that people understand that without much explanation. Maybe it's also a good idea if somebody, let's say, wants to use not XGBoost, but LightGBM, or CatBoost, which are different implementations of gradient boosting – you're free to use them in your project as well. You can use FastAPI instead of Flask, for example. You can use Poetry instead of Pipenv. You can use Conda instead of Pipenv. You're basically free to explore to use what you want and play with different tools, just be sure to document that.",Machine Learning Zoomcamp,2021,
954016,"Alexey
Yes, you will be. You can submit the project. As we promised at the very beginning, we will give certificates based on project completion, not based on the homework. So if you're catching up right now, and you didn't do all the homework, it's fine. If you are caught up and you know what to do, you can take part in a project.",Data Engineering Zoomcamp,2022,
3699,"Alexey
I think the question refers to the homework form where we asked you to submit your code. Here, you can just create a repo in GitHub, create a folder there, homework 1, put your SQL files there, and just leave a link to this file when you submit. It can be anything else. If you are more of a GitLab kind of person, you can create a report in GitLab. But it should be something publicly accessible. Put your SQL queries there and submit it with the form.",Data Engineering Zoomcamp,2022,
858915,"Dmitry
It's fine, because this is the showcase purpose. We said to optimize the number of books, for example, the validation steps. Also, the architecture can be a bit changed. For sure, we can create a much better model.
Alexey
For this model, I guess, if we wanted to build a cat vs dog classifier that works well, we would use a pre-trained neural network and fine tune it, right?
Dmitry
Yeah. That’s one of the options.
Alexey
Or just get a lot of different pictures of cats and dogs and train from scratch. Are there any other options?
Dmitry
Other options would be  to try to tweak the parameters and all those things.
Alexey
So you think you can train a model from scratch – the one we had – without using a pre-trained neural network and then have a decent accuracy?
Dmitry
Yeah, but it's also the question of what “decent” is. 
Alexey
At least 80%, for example.
Dmitry
Around 80, I think yes. But we need to remember that the pretrained will always have the benefit.
Alexey
If we think about this – right now, this model that we have, has an accuracy of 65% on validation, which is just slightly better than a random guess. Well, it's not slightly better, but it's 10-15% better. There are chances that, you pick a few random images and in both of these cases, it will be incorrect just because it's 65%. Right? 
Dmitry
Yeah, but we didn't do any…
Alexey
Yeah, I'm just saying why this can happen. This could be the reason – 65% is not the best accuracy in the world.",Machine Learning Zoomcamp,2021,
830918,"Alexey
I guess the question is asking about the difference between what we are doing here in the course and the actual work of a data engineer.
Victoria
More problems. [chuckles] More troubleshooting, probably.
Ankush
I think the course is made in a way to set you up for becoming a data engineer or tackling all the problems of a data engineer. Mostly it will revolve around the technologies that we are talking about.
Alexey
So what are the main differences, except for more troubleshooting? I imagined that there are analysts who come to you or other people with ad hoc queries, saying, “Hey, where is this data? Where is this table?” And then you have to help them. What are the other main differences?
Ankush
I would say complexity. Definitely.
Alexey
Yeah, we have a relatively simple case, right? We only need to do one join. Well, two joins. We have a bunch of tables, but we don't need to join these big tables with each other. We only do a join with the location table, which is pretty small. But in practice, we often have two big tables that we need to join.
Victoria
Yeah, we don't have complex business logic. Plus, you don't really do the setup all the time. You set up BigQuery once and then you maintain it, but you probably won’t be dealing with the service account that much.
Alexey
Ideally, there may be a team who deals with that. For example, at our work, we have a team who manages Airflow, so I don't need to worry about this. All I do is just write DAGs, commit them, and that's it. I never had to set up Airflow locally and worry about these things because it's managed. I think it’s the same with the data warehouse and other things.
Ankush
I think one more thing that everybody with data does is basically communicate outwards what the meaning of different columns is or what this ratio is versus that ratio. All that. I have to personally do a lot of that. What's your experience like?
Victoria
I would say the same, especially for business stakeholders. But DBT also has this data catalog part. So in the data team that helps quite a lot. Also we look at and work on the selection of an actual data catalog. So hopefully, there’s not too many questions.
Alexey
In our case, for these questions we will have a catalog team. We have an internal catalog tool and they get all these questions, not data engineers.",Data Engineering Zoomcamp,2022,
243026,"Victoria
I think there's a lot. It's a little bit hard to get a data engineer, and that's also why I would assume there are a lot of openings, and you will have a lot of options. How can someone land a job? If you go to meetups or have contacts, that's probably a good way. Or just apply.
Sejal
I agree. Actually, there are plenty of data engineering openings. When I was a data engineer in my previous role, I used to get invitations for job interviews almost every week from recruiters. There's an abundance for this role in the market. Definitely try it out.
Alexey
Since you changed your title to ML engineer, do you now get fewer or more invitations? You don't get anything? 
Sejal
Oddly. [chuckles] 
Alexey
[chuckles] Okay, so we see that being a data engineer is actually better, in terms of market demand.
Sejal
Yeah, I think engineering skills are way more in demand than data science and machine learning skills are.
Alexey
Yeah, that's interesting. What happened to “the sexiest job of the 21st century”? Nobody wants to hire them now. [chuckles] We had an interesting discussion in one of the podcasts here with Ellen. Ellen was a data scientist and she became a data engineer. She is from Berlin and I think she also talks about how to get a job as a data engineer in Berlin specifically. I think the recommendation was to talk to consulting companies. 
They have some sort of programs for coaching juniors, because the business model is to hire as many juniors as possible because they're cheap, and then you sell them to their client for a lot of money. So you need to have training programs so that the juniors are effective. Her recommendation was to try to find such a consultancy company and learn there. I hope it's an accurate summary of the episodes. Maybe I'm misinterpreting something.
Victoria
Yeah, I think if you come from a non-data background, and you're starting with the course and all that, just bear in mind that you'll have to apply a lot and there will probably be a lot of rejections. At least that's how it was for me at the very beginning when I moved to Berlin. Because everything was kind of new and all that, I didn't have a job at the time, because I just moved to Berlin and then looked for one. 
Try to get as many interviews as possible, and then do the challenges and all that. That will train you a lot and will also help you know where the benchmark is, kind of. But yeah, there are a lot of offers, so just start applying. Start applying, start talking to other people, probably to other data engineers, as well. That's also good. About the consultancy, I don't know. I've never used anything like that.
Alexey
I worked at an outsourcing company, but it was before I moved to Berlin. But I can confirm that in the company where I worked, they did have quite a good training process. They would have a coach that was assigned to me who would help me with everything. There was also some sort of bootcamp program. Then after that, they would try to sell me to a client and they would coach me on how to pass an interview with a client. Once a client likes me, I just start working. This is quite common, I think, in outsourcing and consulting companies. For me, that was a very long time ago, so I don't know how that actually works now. [chuckles]
Sejal
That's actually an interesting model. It's good that companies are actually putting that much effort into coaching the employees to be honed to certain projects. That's nice.
Alexey
Yeah, but the thing is, the business model is buying somebody for cheap and selling them for a lot of money and then taking the margin. So, unfortunately, sometimes (at least this was in my case) the salary was pretty low. I remember that they didn't have much left after paying for my flat. So be careful. [chuckles] But at least that was a good experience. After one year, I could sell this experience for “market standards,” let's say. So depending on how young you are, how much you want to get this experience, I think sometimes it's worth actually agreeing to a lower salary in exchange for experience. Then you get this experience, and then your value on the market becomes higher. 
Another shameless plug, but this is something we actually talked about with Juan Pablo. He suggested looking for small gigs that don't pay a lot, but this is experience you can sell afterwards. This is somewhat similar to what we mentioned, even though he's more into analytics than data engineering, but I think the tips that he shared when he was looking for a job – I think they're pretty universal. There was also a funny story that he was actually driving an Uber to be able to survive while he was trying to study things in order to switch. That's a cool one as well.",Data Engineering Zoomcamp,2022,
347048,"Let's say we have a two dimensional thing (2D) and we want to turn it into 1D. For this case, we need to use Pooling2D. I'm not sure right now and I want to quickly check it. This is the reason. Remember, when I was showing you how I usually go about defining the model? I build it sort of layer by layer and every time I add one more layer, I do a model that predicts to see what the output is. And then based on that, I see what kind of pooling layer, for example, I need. Let's say if we want to turn 2D into 1D, I think we need to use 2D pooling. I'm not exactly sure whether it's 2D or 1D. I think 1D pooling is needed when we have something one dimensional and we want to turn it into just one value, then we use 1D pooling. And then 3D pooling is when we have a three dimensional thing and we want to turn it into a one-dimensional thing. [image 2] These things always confuse me, to be honest. 
That's why I follow this step by step and then I just try different poolings. Then I want to make sure that, if I convert an image into a vector presentation, I have something one-dimensional. Usually the size is the number of images times something. So it's more like a 2D array. For each image, I have a one-dimensional vector. Then based on that, I try different poolings. Sometimes, you can also just flatten. Flatten takes whatever D – let's say you have KD – and you want to turn it into 1D, you use flatten. There are many different options. I think it's clear what the difference is. I might not remember exactly when to use 1D, 2D, and so on, but the difference is what kind of input they take and what kind of output they produce. If it's just a cube, then it's 2D, if it's a hypercube with three dimensions, that it's something else.",Machine Learning Zoomcamp,2021,85248_qa9_ml_zoomcamp_office_hours__week_11_pic2.jpg
772478,"Kind of. Actually, pickle expects that you have the code that, let's say, if your pickle and object are of a particular class. Let's say this class could be part of the package Scikit Learn linear (the class is logistic regression). When pickle loads an object, it expects that this class is present in your Python. It loads all these methods from that code – from that module. It expects that the code will be there and it just basically loads the data and the behavior is stored in the code. So you have to have the code.",Machine Learning Zoomcamp,2021,
934691,"I think in one of the videos, I talked about the three different subtypes of supervised learning, integration, classification, and tracking. About ranking – [image for reference] for regression, Let’s say you have a car and then we extract your feature matrix from the car. You apply the formula (the g function that you trained) and you get a prediction, like price. You do this for one object. You get one car and you do a prediction for one car. 
When it comes to ranking, you don't have one object. Of course, you can have multiple cars and apply this to multiple cars. But the core difference with ranking is – in ranking, let's say you have some results from Google. So you have some query and you have some results from Google. There could be, let's say, results from 0 to 99. And you need to apply a function to each of the elements. This is a group. And you need to apply this model that you have (the model g) to each element of this group. Then it produces a ranked list. Let's say, you apply g(x0), and you apply g(x99) this x99 is the row here – the results of the query. Then what you do is rerank the output – you rerank all these results using this function. 
What I'm trying to say is, here, you look at the group and you try to see how good the ranking is within the group. While in the case of simple regression, you have more standalone objects, sort of. But with ranking, it always has to be a group. Of course, when it comes to ranking, this g can also be a regression or it can also be classification. But then you always need to think about the other elements in the group. I hope that answers the question. 
This is not something we will go into detail about – not in this course, for sure. This is just for you to know that it's a little bit different. Maybe this is something you want to do for your project or explore for the article. This is totally fine.",Machine Learning Zoomcamp,2021,966609_qa6_01.jpg
513382,"Zero. You don't actually have to submit homework to get a certificate. For the certificate, you need to finish two projects – the midterm project and the capstone project. This is what you need to do to get the certificate. The rest is optional. It will give you some points. If something happens, and you don't find time to complete one homework assignment, the world won’t stop. You can just catch up and perhaps do another homework assignment. You will just not get the virtual points. But for the certificate, all you need to do is finish two projects.",Machine Learning Zoomcamp,2021,
654080,"Victoria
For example, there are some weeks that kind of overlap one another. Maybe not overlap, but you could use DBT or you could use Spark. This is what we mean by one of two approaches. You can choose what to use out of everything you’ve learned.
Alexey
Also for the pipeline, indeed, batch versus stream is also an option. How exactly your data ends up in your data warehouse before you do… Somehow you need to put the data in the data warehouse and you can do it with batch or with steam. After the data is there, you can do some other stuff on top of that to transform the data. For example, you can use DBT for that or something else. Indeed, there are many ways you can juggle it. Or maybe you can have both – that is also an option. Probably you should just go with one.
Victoria
I mean, there are some things where there's not much of an option. For example, with BigQuery you need to have Google Cloud Storage or things like that, unless you're using the local version, I guess.",Data Engineering Zoomcamp,2022,
939924,"Alexey
What do you think, Dmitry?
Dmitry
Thinking about that, I think it's referring to one of the questions before. Usually, when you have a real task, you need to do everything together. This should be a combined process and you shouldn't go step by step. Basically, one of the purposes of the EDA step is to give you the bridge to the feature selection process – thus helping you to understand what feature you will use or not use in your model. After you decide that, you need to decide whether it's okay or not okay to go with regularization. So basically, step by step.
Alexey
It also doesn't hurt to try regularization as well, even though it seems like there is no redundancy in your features – but, all of a sudden, regularization helps. So you just need to test it.
Dmitry
Or it doesn’t help.
Alexey
Yes. So you don't know. There is no way to find out except to try it.",Machine Learning Zoomcamp,2021,
88031,This is correct. Yes.,Machine Learning Zoomcamp,2021,
337445,"Victoria
I would say yes, this is the go-to, and not just DBT. Yes, DBT enforces this. If you go and read their viewpoint and all of this, they'll explain that they also do the models with this kind of structure. But as I mentioned in the modern concepts, this is a concept that comes from the 80s that Kimball defined. With the kitchen analogy, I find that very, very useful. This is normally how you go. Of course, the more complex it gets, you may need more steps, and that's okay. But always try to have this separated – What is the presentation? What is the source? You will need to do some typecasting like we do, for example, maybe do some duplication. And then in the middle, you may have more models that will go in that data pipeline, let’s say, in that flow. But this is more or less how it would look like. 
Alexey
What is the kitchen analogy? 
Victoria
The kitchen analogy is that the way you model your data is similar to how restaurants model their food. You have a warehouse where you just have raw food. This is where you have your source data. Not everyone is allowed to do that. In fact, it's actually dangerous. That's why there are security measures about who can actually go to the warehouse, what can be stored, and how it can be stored and things like that. Then you have the kitchen. Only the people that can cook this raw food and make it into food that they're going to serve are allowed in there. This would be the data engineers and the data analysts, maybe analytics engineers. This is what you would be doing in the data warehouse, trying to process that raw data. And then at the end, you have the part of the restaurant where people eat – the dining hall. This is the presentation layer of your data warehouse. This is how fact tables will look like, or your data marts, if you're building data marts. These are the business stakeholders. Everyone is allowed to get that foot, let’s say. There are no restrictions, but it's only already presented. You're not presenting them raw.
Alexey
I think we have this tableau self-service tool, where we can go and do all this Cubes and all that. This is the presentation layer, right?
Victoria
Yeah. Technically, in BI tools that are self-serve, you could have everything. You could expose everything. Right now, we have the external tables, then we create our own tables in the [inaudible] and then we create the section model and then we create the factories. I could have everything in there. I could even have the ones that my development exposed to the self-service, the BI tool, like Tableau or Looker, whatever. But what this analogy says is that you should use the presentation layer only, and not the other parts. Because if you present the raw data, then people wouldn't know how to use it. It's going to be like, “Oh, I didn't find these phones, or these other ones. Maybe it's because there was no transformation,” and things like that.
Alexey
Nice analogy.",Data Engineering Zoomcamp,2022,
717763,"Victoria
I would say it’s probably similar to what happened with data analytics engineering. Suddenly the whole stack became more complex and you couldn't just make do with a few scripts and [unintelligible] and that's probably where it started, I would assume. But I don't know the whole story of the data engineer role.
Alexey
Maybe I will give you a very, maybe not correct answer, but one from a data scientist’s perspective. Data science started more than 10 years ago. Maybe people expected too much from data science. They thought they would just hire a bunch of people with PhDs in mathematics and they would just do some magic and then the data would just be clean and the models would be perfect and the companies would earn a lot of money. 
They did this, things didn't work, and they thought, “Okay, what is the problem?” It turned out that it required a lot of engineering – all these data pipelines need to be there. I think this was around the time when data science was getting some traction, people realized that it's also important, and I think it was also together with analytics – maybe in the beginning of 2010s (somewhere around there).
Victoria
Do you want the right answer? 2011. In Airbnb and Facebook, apparently, they started to do it and it's indeed related to all of the things that we were talking about – the complexity of the ETL process, then they suddenly needed a role that was more oriented to just building these pipelines to transport and transform that data.
Alexey
I imagine that a company hires an analyst and the analyst says, “Okay, I don't know what to do with this. Somebody needs to come and set up this infra.” Then, because analysts couldn't do this, data scientists couldn't do this – somebody needed to do this. So ETL developers kind of rebranded into data engineers, I guess. Nice history Lesson.",Data Engineering Zoomcamp,2022,
851050,"Alexey
In week 3, for yellow taxi data we use 2018-2020. And for frequently/high use, I think it was 2019. For zones, it's just one file. Victoria, for week 4, we use just one month, right?
Victoria
Whatever you load, basically, in week 3. So it would be as I understand the one for 2019-2020, right? Since we're going to be building the model, it would be independent – we'll just use whatever is in the table. If they didn't get to do the homework or something like that, if at least they have one month, it should be enough for them to run the models.
Alexey
I'll try to update the description so it's less confusing. For yellow taxis, it’s 2019-2020 and for for-hire vehicles, it should be 2019. We don't use green taxis for week 3.",Data Engineering Zoomcamp,2022,
132688,"Alexey
It is possible, but only if you have a Gmail account. In Google Forms, this option does exist, but you have to have a Gmail account. Many people who take part in this course have iCloud, or Yandex, or Yahoo, or some other email provider that’s not Gmail. That's why it seems it's not really possible to do that because it requires you to be logged in to your Google and then it automatically captures your email and then sends the result. This is what I know. Maybe I'm wrong. If you know how to make it in a way that everyone, not just Gmail users, can get a confirmation and it doesn't force others to use Gmail for that, let me know.",Machine Learning Zoomcamp,2021,
278395,"Let's go to the notes. We have this course repo here. Here is the course, let's take, say, linear regression. We have the notes here – and this is where it says “Add notes from video (PRs are welcome)”. What I mean here is that – let's say you watch a video here and you took some notes. And you want to share these links with these notes with your fellow learners. You can do that by taking these notes, putting them somewhere, such as on Notion, or Medium, or GitHub, or whatever you prefer – some online page. You can put them there and just create a pull request with a link to your notes. Then somebody who maybe doesn't like watching videos can just go through your notes and read them. 
That's the idea. Even better, instead of adding links, you can add notes directly here, so people do not need to go to some external site – they have everything here in the repo. That’s the idea. I don't have time to make notes myself. That's why I added these notes. If you have time, if you want to contribute, and if you have some notes, you can contribute your notes and help everyone with that.",Machine Learning Zoomcamp,2021,
403081,"Alexey
Yes, you're free to choose any dataset. A few people already reached out to me asking if what they had was a good dataset. And I think to everyone who asked me that, I said, “Yes, this is a good dataset.” There was one thread that was a question about images – whether it's a good idea to use images with this project. I don't think it's a good idea to use images, because there will be quite a lot of things you will need to do in addition to what we cover in the course, like image processing, and trying to extract some information from images. That is outside of the scope of the course and if this is the first time you’re working with images, it could be quite difficult. I don't think it's realistic that you'll be able to do a data engineering project and then this thing on top of that in two weeks. You might end up spending too much time on that. So maybe try to select a less ambitious dataset. If it's a table, a CSV file, or parquet file, then good. If it's images, it is probably better not to use it. If it's text – with text, you also need to do quite some work to process it. Maybe if it's just natural language, it’s also not a good idea to use it. For example, this common crawl dataset, there is a lot of just text, which probably could be not a great dataset, depending on what exactly you want to do. If you're not sure, maybe share your idea – share what you want to do – and then we can tell you if it's doable within a couple of weeks or not. I also want to share a DataTalks.Club’s Slack dump. There is also quite a lot of structured information that could be used. It's also grouped by days – for each day, there is a JSON file. You can build a nice pipeline for that. I'll probably make a dump and share it in the channel and you can see if it's something you want to use or not.",Data Engineering Zoomcamp,2022,
998091,"Victoria
No, it's not bad. Normally, I wouldn't say you will have both when you're working. It will depend a lot on the use cases and all that. Actually, we thought through the project in a way that you could use the technology that you want to use. Is it okay? It's okay if you just use DBT. Also it would have been okay if you would have only used Spark and not DBT. In the future, if you want to, you can always do the Spark part. It's kind of like the equivalent to the transformation that you've done in DBT. You can even use a Spark with DBT as well, if you want to get into that. I think that's okay.
Alexey
I think it's quite common that you do some pre-processing while the data is still in the data lake. The raw data you pre-process with Spark, then you put this back to data lake again, but in a more processed/prepared format, and then you load this to a data warehouse and then in the data warehouse, you do find the final grouping, aggregations, and all these things in order to prepare the data for reports, dashboards, and so on – you do that with DBT. This is my understanding of how companies often do that.
Victoria
Yes. Or I would say that they maybe use some for some kind of data Spark, and then some kind of data DBT or the combination of both and then some data goes directly to DBT and some just directly to Spark. So that would always depend on, as always, on the use case or the company.
Alexey
This reminds me of a talk we had recently with Rahul. In this video with Rahul, he talks about modern data warehouse design. He mentions EMR – they use Spark for processing streaming things. They consume from Kafka and then here, they have DBT. Here, they have these generic ETLs. In principle, you can have something like Spark here, then he puts this to staging data, then you have DBT that gets data from the staging area to the domain area. So check the video if you haven't. Judging from the number of views, most of you probably already watched it.
Victoria
[chuckles] I haven't watched it yet. 
Alexey
Yeah, this is a good one. 
Victoria
Yeah, I wanted to. I downloaded it for my client.
Alexey
[chuckles] Cool. Rahul shared this link on his LinkedIn and a lot of people saw it. This post was quite popular, so that's why a lot of people saw this video. It's a cool video. Check it out.",Data Engineering Zoomcamp,2022,
890166,No reason. I just think Flask is the most popular framework for doing this. It's relatively simple. That's the main reason.,Machine Learning Zoomcamp,2021,
764907,"So far, for week two and for week three, we will follow the simple approach. So the simple approach is what I described. We just take an entire dataset, and we split it into three parts: train, validation, and test. So this is what we'll use for week two and week three. In week four, we will talk about cross validation – I will not explain what that is right now. But what I want to say is that this approach to validation is sufficient for many cases. I think it's pretty safe to say that in many, many, many applications, just doing this split once is enough. 
Cross validation, of course, is nicer and we will learn more about this in week four. Right now, I will not go into detail and try to explain it. Wait for the video. So “how to select an approach for cross validation?” I would say that there are multiple options: k-fold… I'm not sure many people will understand that because we haven't covered that yet. But basically, if your dataset is big, then you don't need k-fold. If your dataset is smaller, then use k-fold. For “leave p out” – I've never used it personally.",Machine Learning Zoomcamp,2021,
373352,"To be honest, no. I haven't used Uvicorn. I have used Gunicorn. I don't know if it matters. I haven't actually checked benchmarks. We use Gunicorn. We also use others – I don't remember which ones. There is no reason, basically.",Machine Learning Zoomcamp,2021,
163271,"Ankush
I think dedicating around two to three hours would be enough. Going through the videos will be less, but you might want to do the homework. And you might want to experiment on your own a bit with the technologies that you are not familiar with. That might take a bit more time. But on average, two to three hours per week should be enough to finish the course.",Data Engineering Zoomcamp,2022,
181614,"Alexey
Yeah, you can learn Scala for Spark. It will be useful. I think we've had a discussion about this and Ankush said it’s definitely going to be useful. I remember one case when, in my previous company, we needed to rewrite something from Python to Scala and it improved the speed of this step significantly – just rewriting it. So knowing Scala is helpful, but I think you can get away with just Python for most of the stuff. It depends on how deep you want to go into Spark, and then maybe you can learn this. My personal opinion is that there are better ways to learn than Scala. [chuckles] Tastes differ. Victoria, do you have any suggestions or ideas? Should they learn Scala or something else?
Victoria
No, I think that's more of a data engineering focus, definitely. But I'm with you – if you learn Python and stuff. I also think if you come from a computer science background, it just depends, right? If you already know how to program and all of that, don't be scared if you had to pick up Scala, because you'll probably be able to do it anyway. Of course, like everything, you'll have to learn something, but don't be scared that you will not be able to know it or something or that people won't hire you because of that.
Alexey
Yeah. To get hired, Python is sufficient. Also, talking about Scala – as I understood, for Kafka, packages in the Java world in JVM, for both Java, Scala and other JVM languages – are more advanced than Python. For some things, for example, for streaming, there are also libraries like Flink – they have Python integration, but still the Java API and the Scala API are better. For some use cases, you might end up learning Scala or Java. I would say – don't learn it until you need it. But if you are into programming languages, you can pick it up.
Victoria
I think it's always better to know a few things well – like Python and SQL and stuff like that – and then pick the other things up. As opposed to knowing a bunch of things but just not very well. That's my strategy, I would say.",Data Engineering Zoomcamp,2022,
971586,"Alexey
Do you have any ideas, Sejal?
Sejal
Yeah, I think we updated... Maybe you should move that section to the main readme page. You know that part where there are some additional things that we could do in our setup, such as writing unit tests, and the CI/CD framework, and so on and so forth? I think it's in the “project” section, but maybe we should move it to the parent branch. But those are some of the things that you can cover a bit further, I would say, to make a complete packaged environment for yourself, including the test cases and automatic deployment pipelines.
Alexey
Yeah, I think it's a good idea to move it out of the project. Like “next steps,” right?
Sejal
Exactly. This is just something, based on that question, see how many things can be automated out of this. There might be some cases where you can run scheduled pipelines in Airflow (or whichever tool you're using) in order to generate more frequent reports (maybe weekly reports) for results that you need on a daily basis. Let's say you have a forecasting pipeline where you're estimating the stock prices a week from now, maybe you can have a scheduled cron job based on that. That's something that, with a dashboard, can reflect those predictions or any real-time stock market prices also on a weekly basis or on a daily basis for you. So that is also an app that you can create for yourself. Just an idea.
Alexey
I'm thinking that there are a few things we didn't cover. For example, we didn't cover serverless, which is quite an interesting concept also for stream processing. Let's say you have a stream, and then you have a lambda function (or whatever, I don't remember what they are called in Google). So you have a function that you use for consuming from a stream, and then doing something, and then putting it to another stream. You can build quite complex pipelines from these streams without writing the consumer code yourself. You just write a lambda function that is applied to a stream and does something, and then internally, it handles everything you need to handle (to consume) and so on. This is quite a cool concept – serverless. Another thing we didn't cover that could be useful for data engineers is Kubernetes. Or a similar thing, for example, using AWS Batch on Amazon or using Kubernetes Jobs, or something like that. That also could be quite useful.
Sejal 
Yeah, +1 for Kubernetes. I think most companies are using it, preferring to use Kubernetes as the choice of cloud cluster instead of AWS. Having settled hosted solutions on Kubernetes is definitely a plus, so I definitely recommend learning it.
Alexey
There is another thing I would like to recommend learning about, which is something we didn't cover at all. With serverless, the thing I mentioned – we kind of covered that a little bit in the streaming week. With Kubernetes, we kind of covered it a little bit in the batch week. We at least mentioned that you can use a Kubernetes job for batch jobs, or serverless for streaming. What we didn't cover at all is the concept of data monitoring, data quality checks, and things like this. There are a few nice tools that do that. For example, tomorrow, in DataTalks.Club, will have a webinar (a workshop) about data monitoring with whylogs. Whylogs is a tool for doing data monitoring. There are other nice tools, for example, maybe you've heard about Great Expectations, or Soda SQL – there are quite a few of them. This is something we didn't cover at all. Well, maybe a little bit in the DBT Week. We covered testing a bit. But I think this deserves more attention. This is something that many companies are looking into, so definitely worth checking out. Victoria, maybe you also have some ideas?
Victoria
Definitely data quality, I agree on that one. With DBT, you can do a lot. There are a bunch of packages as well. But, of course, you have to have an implementation. Other than that, observability. I'm not sure if you mentioned that. I guess you did as well. [Alexey mentions monitoring and observability] Also very good tools. 
Alexey
Something we also didn't cover specifically, but mentioned a few times, are tools like Fivetran and so on. I think they're gaining popularity. Or Airbyte, which is an open source alternative for Fivetran. This is also worth checking out. But as you see, there are many, many, many things that didn't make it into the course that look quite useful. Then you might think, “Okay, now I need to learn five, six more topics.” Maybe you shouldn't just try to learn as much as possible. If your goal is to, let's say, work as a data engineer, you can already start applying to data engineering positions and then see what they ask you – what kind of things do they need from you that you don't know yet? There are thousands of things that you can potentially learn, and it's very difficult to select what the next one should be. So maybe you base your decision on whatever you hear from potential employers.",Data Engineering Zoomcamp,2022,
204569,"Alexey
I don't think I really understand the question. 
Ankush
If you're in the same VPC, does it matter?
Alexey
But the cluster – is it a Spark cluster or which cluster? A Kafka cluster?
Ankush
If they're all running in the same VPC, they should be able to access each other's IP, right? 
Alexey
Yeah. Oh, okay, externally IP. I guess if you're running inside the same network, then you can use just an internal IP. Or if it's inside Kubernetes, then you can just refer to them by names of deployments. I don't know. 
Ankush
I also don't know. It's really specific to that particular use case. You need to give us more. Maybe on Slack you can explain it a bit more – Where are you running this? What kind of machines? And what exactly are  you exposing in terms of external IP?
Alexey
But usually, you have a Kafka cluster (some Kafka machines) and then you have a Spark cluster. And then the Spark cluster connects to Kafka and reads data from that. 
Ankush
It just depends which VPC you are running in. If you're running in different VPCs, then you need to expose it separately. Then you need to expose the IP and all that. But if you are running in the same VPC, it should not matter. You should be able to access it internally. But I might be wrong. I'm no no DevOps. [chuckles]
Alexey
Usually, I take these things for granted, and maybe this is not a good thing. They work and I am very grateful to the data team that makes these tools work, but I often don't ask myself, “Okay, how is it actually configured?” Because it just works until it doesn't – when it doesn't, then I can go to the support channel and ask, “Hey, can you please fix it?” 
Ankush
I think that's also not the data team, that's also maybe a DevOps team embedded inside the data team. Because you're working at OLX, which is a pretty big company. Maybe you have many smaller teams inside big teams? But I guess it's about a DevOps topic.",Data Engineering Zoomcamp,2022,
83369,"Yes, I will. I haven't written the script for that yet. What I actually want to do is – many people asked me how they can see the results and I made a mistake at the beginning when I was letting people register. I didn't add a checkbox saying that it's okay to use some of the data (like first name and last name) in public. So I cannot really have a public leaderboard, where everyone can just go and check their score, simply because I didn't ask for permission to do that. What I want to do instead is to write a web service where there’ll be an email field. So you go there, you put your email, and you get the scores. This is only an idea. I hope I'll be able to implement this. 
Hopefully, it shouldn't be too difficult and you will be able to get your scores this way. But yes, I will grade the homework. I will store the scores of this course, because at the end, if you remember from the first video I talked about the public leaderboard at the end. Of course, for the 100 people on the leaderboard, I will ask them for their permission to publish their names. But before that, I'll need to figure out how exactly to communicate the results of each separate homework. For me, grading is actually not difficult. I want to write a script for doing that.",Machine Learning Zoomcamp,2021,
655484,"Alexey
I am not sure what this is. Maybe this is an answer to the thing we just talked about regarding filters. Well, you can use matplotlib for doing that. You take a filter and then…
Dmitry
I think PIL is a more interesting option. It's more to the images, I guess.
Alexey
Yeah, but for PIL, it's more like if you want to save it as a JPEG, for example. But if you just want to plot it, to see how it looks, matplotlib is probably sufficient. In the homework, you also use matplotlib. You didn't use PIL for that. 
48:08 [0 upvotes]
Can we use dogs vs cats as the main capstone project with better performance?
Dmitry
I guess, maybe the other datasets?
Alexey
Yeah. Are there other datasets?
Dmitry
I mean, there are a lot of open datasets, I guess.
Alexey
With cats and dogs. Yeah.
Dmitry
I mean, I think cats and dogs another time wouldn't be very interesting.
Alexey
Yeah. But if you add more data there, that should be interesting.
Dmitry
Maybe cats versus dogs versus something else?
Alexey
Who did you do? Wombats? Cats versus wombats. [chuckles] I think in the image net, there must be wombats, right? 
Dmitry
Yeah, there should be.
Alexey
So you can just take a part of the image net and get wombats from there? 
Dmitry
Have fun.
Alexey
[laughs] Or maybe wombats versus… What is similar to wombats? Because wombats and cats are quite different, right? They have different environments. So it's an easy task. 
Dmitry
Maybe wildcats.
Alexey
Yeah, I saw a wildcat once in a zoo. It looked like a normal cat. So I suspect that maybe they just put a normal cat there and said it's a wildcat. [chuckles]
Dmitry
Better marketing. [chuckles]",Machine Learning Zoomcamp,2021,
204684,"Alexey
This is a question in the lecture. I use exception and the output of exception, when we don't include top, is this 5x5x2048. This is a long, long, long tube – a three-dimensional thing. Dmitry, do you know why it's 5x5? I think this is because they have these convolutional layers 1x1 – quite a few of them. At the end, they have 2000-something filters, and this is the result of applying these filters. So we still have a lot of feature maps. But I'm not completely sure. Do you know what's going on here, Dmitry? What happens in exception on the last layer?
Dmitry
No, I think it's because of the filters.
Alexey
Yeah, so this is just a bunch of…
Dmitry
Convolutional nature.
Alexey
Yeah. This is not something we specified. This is coming from the pretrained network.",Machine Learning Zoomcamp,2021,
255748,"Yeah, I think there are other options. I think I shared a link in Slack, you can check there. There are options. You can, for example, save your model as a JSON file, you can save it as the PMML standard (predictive model markup language) which is a different way of exporting models. I think it also works with Scikit Learn. There is a library for converting Scikit Learn models into this format. I find it easiest to just do pickle or you can also use joblib, which is essentially almost the same thing as just pickle. 
There are many advantages – the model becomes smaller. There are, of course, disadvantages that you become dependent on a particular version of the code. This is why Scikit Learn gives you warnings when you try to load a model that was saved with one version and you want to load it from a different version. That gives you warnings. Also, if you think about logistic regression, it’s just a bunch of coefficients and then the bias term. So what you can do is just export it yourself into a JSON file and then load it yourself. You can just write some code for that.",Machine Learning Zoomcamp,2021,
243124,"Alexey
I guess this way, if there is a question that many people want to see answered, we'll cover it first. To me, it makes more sense.
Victoria
Yeah, that makes sense. But I guess some of the questions were questions relating to topics that we’re answering and they get to the last row because you get to see them. That's maybe where it comes from. But it makes sense to do it the popular way because they get to vote.",Data Engineering Zoomcamp,2022,
419359,"As a matter of fact, I do. Soon I will be editing three or four right now. I don't remember. I think four. So this is what I use for editing videos. Frankly, I didn't find anything else on Linux, but it works surprisingly well.",Machine Learning Zoomcamp,2021,
243114,"That's an interesting question. The tricky part here is – in machine learning, you have this formula that you saw many times g(x)≈y. First of all, we need to have this thing here y. This is our target. If we can already think “What is the target we're predicting?” Let's say we want to understand what is in a picture. We have a picture and we want to build a model for classifying dogs and cats. In this case, our target would be dogs and cats – which is present in an image? So we need to think about this target y. This target is very important. If we know what we want to predict, then potentially it can be solved with machine learning. 
Basically, the answer to this question is that you need to think about the answer “What is the target variable that I'm trying to predict?” And “What features do I have?” (the latter is represented by x in g(x)≈y). You need to have both. You need to have a target y – this is what you want to predict and you need to have features x. If you can express your problem in these terms, as a feature matrix and as a target, then it can be solved with machine learning. If you cannot, then you’ll probably need to think more about this.",Machine Learning Zoomcamp,2021,
673096,"So the correct strategy would be not to look at the testing dataset at all? Usually, let's say when I do EDA at work, say I use a SQL query to get the data on my computer, to do some initial analysis. In this SQL query, I only select the rows that I need for the training. I don't look at test at all. It's generally a good idea not to look at test at all because when you do this it’s called data snooping. This is bad because it might influence your decision and you don't want to do this. You don't want to influence your decision. Because maybe you look at it and think, “Okay, if I do these things, it will become better.” But maybe this is not the real pattern and when you see a new dataset, this pattern that you accidentally observed yourself is not true. So try to avoid snooping as much as possible. Use testing only to test your models. Avoid looking at tests at all costs, basically.",Machine Learning Zoomcamp,2021,
545208,"Sejal
My answer would be opinionated, but I have generally learned not via formal education, but just by trying different projects out, trying open source projects out on GitHub, or trying to do little things I can automate at work. Even if you're a data analyst or a data scientist, you can take on some automation, just opportunities wherever you see where you can engineer some pipelines. Then just proceed from there and see how you can help your current use cases out in the company or in self-learning as well. Just try things out on your own based on whatever tech stack, for example, that you want to learn or a particular core area that you want to learn. For example, a recommendations engine, or data engineering pipelines within the recommendations domain, or a search engine and so on, so forth. Just some thoughts on from the top of my head but maybe Ankush and Alexey you can add to that.
Ankush
From what I can say, it basically depends on your end goal. With a formal education, you get the opportunity to meet people, you get the opportunity to learn from PhDs or from professors who are really proficient in their domain. You can learn the theory behind the practical. Right now, we are more focused on, let's say, a particular technology. Even if you're talking about theory, we are talking about that theory of that technology. We are not going deep into the theory of computers and the maths behind it. For example, a distributed system would have mathematics all behind it before actually even starting. So if you want to learn those things, then you have to go for a formal education and learn that from a good university. If your aim is to socialize, or to do a PhD, or to do a Master's afterwards, then formal education is also the right way because you build relationships there. But if your aim is to learn, especially learn in the domain where you want to work on specific technologies, or become a data engineer, let's say, then there is nothing better than self-taught. You will never learn the same things that you will use in work with formal education anyway. In formal education, the things you will learn are a bit different than what you will learn if you do self learning and focus on a particular subject.
Alexey
I remember my education – my Master’s. I don't think most of the things I learned were useful, by which I mean directly applicable at work. They were useful. For example, we studied database internals. We actually needed to implement a database. That was fun. I'm not sure how useful that was for my work. It's also two years. Maybe after half a year, you realize that data engineering is not for you, so what do you do then? [chuckles] With self-learning, it's kind of safer, right? You can just say, “Okay, this is not for me. Let me try something else.” But if you're halfway through your Master’s, then you have this, “Okay, should I finish it? Should I not finish it? It's just one year. It becomes difficult. Then there’s this thing called Master’s thesis. [sighs] I remember how much trouble I had. I actually wanted to do a PhD. Before doing Master’s, I thought, “Okay, I'll do a Master’s and then I’ll do PhD.” And then when doing the Master's thesis, I realized that I hate writing papers. This is what basically PhD students do all the time. It was fun, but not something super useful for work.
Ankush
And now you're thanking that Master’s is only for two years, right?
Alexey
Yeah, exactly. In Germany, it can take longer. Easily.",Data Engineering Zoomcamp,2022,
546791,"Alexey
We don't have it in our curriculum. We only have a project at the end.
Victoria
But people can write Medium articles. 
Alexey
Of course, yeah. Maybe the question comes from the fact that in the other course, Machine Learning Zoomcamp, we had an article as one of the activities. But I think towards the end of the course people felt a bit tired of doing this. There were also three projects, not just one in that course. I think by the time you finish this, you will be pretty exhausted and you will not want to write articles. But if you do want to write, please write. 
You can write, of course. We can talk about this. If a lot of you want to do this, let us know. We can then think about whether we should incorporate this into the syllabus or not.",Data Engineering Zoomcamp,2022,
88453,"I would say so. It's not a must – I would say that it's a big plus to get exposure into all these areas. I also think being able to deploy your models is a really crucial part here. That's why there is so much emphasis in this course on the deployment part, because you should be able to do this. As a data scientist, especially as a machine learning engineer, I'm sure you will probably have to work with Kubernetes at your job – or something similar to Kubernetes. So knowing how to do this is quite beneficial. You should learn that and if you have this experience, companies will get in a queue to hire you. I think that's very useful experience.",Machine Learning Zoomcamp,2021,
641330,"44. Two people submitted twice. So actually, it's 42 unique submissions. I hope with the extension, many more people will be able to work on the project. I know that maybe many of you were putting things off until the last day and then something didn't work. It happens to me all the time as well. But if you now focus on putting things in Flask and then putting them in Docker, you should be good. If you’re training a model right now, stop it and go to the deployment part. Then you'll be good.",Machine Learning Zoomcamp,2021,
604632,"Alexey
I don't know what the ETLT is but maybe I can summarize because this is something we discussed in Slack recently. I would say if you can write something in SQL and you already have things in your data warehouse and you're not afraid of costs – I think this is what Ankush wants to mention and he will probably talk more about that – but if you can do it with SQL and with a data warehouse or something like Hive or Presto, something managed like Athena in AWS, so that you don't need to worry about having this Spark cluster. It needs maintenance. You can use this managed Spark cluster from AWS or something like that. It gives you the opportunity to just write the SQL query and then put this SQL query to your Airflow or DBT and it just runs the things. But sometimes you need things that are very difficult to express with SQL. Sometimes you need more control over things. Then you go with Spark. 
Ankush
Sometimes you just want to unit test your code so you write that in Spark. I think DBT is completely different from Spark. It's not a one to one comparison. DBT is something sequential. It's sequential SQL queries and it has many features that Spark would not offer out of the box. You can write, let's say, 20 steps in DBT and it's understandable using DBT Graph / DBT cloud. I think if you do the same thing in Spark, it would get hard to debug over time. So think about your use case. I think having a one to one comparison is not fair for either DBT or Spark, in this case.
Alexey
In the end, both are used for batch processing, right? There are some similarities.
Sejal
I think it also depends upon the larger use case. DBT is particularly a tool used by analytics engineers, and more from the standpoint of creating data marts or transformations for that could generate views for your analytical dashboards. Meanwhile Spark is more multi-purpose technology that can be used in ML as well, for distributed processing. Namely, its core value is in distributed processing. Just like Ankush said, it's not really comparable.
Alexey
You also have this flexibility of using whatever you want from Python or from Java or whatever environment you use, which you do not have with DBT. In DBT, you can only use SQL. In BigQuery, you can have some user-defined functions, but they're pretty limited and hard to test and then it will turn out a nightmare at the end. Right?
Ankush
Absolutely. I think maybe we can also see it as something like this. If your company is using something like DBT and Airflow – go with that solution. If your company is only focused on Spark and you are very familiar with Spark, then feel free to explore more Spark, SQL and that area. It will depend upon the maturity of the team and the company as well – basically what you are doing at that moment.
Alexey
A use case, when you don't yet have a Spark cluster, maybe it's a good idea to try to avoid having to maintain a Spark cluster. If you can express things with SQL and use serverless things like BigQuery or Athena or something like this, where you do not have to manage a Spark cluster, or YARN or whatever, I think it's better in the beginning. But then eventually, you will probably need to look into this direction and get a Spark cluster. As Ankush said, it depends on maturity. In the beginning, you probably don't want to deal with all that.
Ankush
Yeah. It also depends upon what your clients are. If you have more analytical engineers in your company who are not familiar with Python and not familiar with Spark, then you also don't want to debug each and every use case they have or every issue they have. Maybe having DBT that can run SQL will be much better for you maintenance-wise. You might also want to compare things in that respect. And obviously, cost.",Data Engineering Zoomcamp,2022,
131536,"Glory. That's all. There is no other prize. Of course, if you're one of these 100 and you want to put your name there, I will create a page on the course page with your name. Yeah, that's it. A name and any link you want, like LinkedIn or Twitter, or whatever. That's the prize. Everyone who took two projects will get a certificate. It's not just the first 100, but anyone who completed the projects will get a certificate.",Machine Learning Zoomcamp,2021,
136229,"Alexey
Well, it's not really anonymous because you will be asked to submit a link to your GitHub. I don't know if there was a need to ask questions in the previous course. I think you can ask for contact information in the project description. I don't know.
Ankush
I think we should not encourage that because it might be unfair for different people. First of all, maybe you're not available to answer all the questions. Let's put it like this, if you are grading then maybe it's unfair, but if you want to learn, then it's definitely helpful. So I think the grading should be done based upon what is submitted, because it's also really important to document all the steps. 
Alexey
Yeah, I guess if you need to contact the author, then it means that you cannot really give four points for reproducibility. 
Ankush
In that case, maybe instructions should be clear in the report itself, not via contact. 
Alexey
Yeah, exactly. But I don't remember this being a problem in the previous course. Maybe you can also create an issue in GitHub. I don't know.",Data Engineering Zoomcamp,2022,
739537,"The idea there is that Flask comes with a web service, but this service is very simple. If you send it multiple requests, I don't think it will be able to handle multiple requests at the same time. It will not be able to do multi-processing (set up multiple processes to handle multiple requests) and things like this. It's very limited in this sense. You can only use it for testing when you run things on your machine. If you want to deploy something to a production environment then you need to use Gunicorn, which is optimized for real production traffic. There are multiple requests coming in at the same time – 50 requests per second or 5000 requests per second. Flask will not be able to handle that. This is my understanding. To be honest, I didn't really look into this much.",Machine Learning Zoomcamp,2021,
861567,"Alexey
Ankush, do you know anyone who did a shift like that?
Ankush
Not really, no. I don't know anyone like that. Do you?
Alexey
I actually, I'm thinking now. People who shifted to data engineering are mostly software engineers – Java developers, for example. I also know a Python developer who because became a data engineering. I know a PHP developer who became a data engineer. Mostly, they come from software engineering. For DBAs, I usually see them become platform engineers or DevOps engineers, with more focus on the Ops part rather than the engineering part. Perhaps maybe for you, this will be a more natural transition. Maybe you first get into the infra side of things, and then go to data engineering. Maybe that.
Ankush
As I always say, “data engineering” is a very broad term or job profile. You can have DataOps in it, which basically would be a good fit for a DBA. Then you have analytics, which is more focused towards analysis, and there you can write SQL scripts. So if you are already doing SQL as a DBA, then this shift would also be pretty natural for you with respect to at least working with SQL and writing pipelines in SQL. Then once you get more comfortable with it, start learning more Python. I think from there, you can just pick up different other technologies, which you think are appropriate for you or are interesting for you.",Data Engineering Zoomcamp,2022,
762237,"It's suspicious. AUC looks quite good, but F1 score is below, compared to AUC. In general, an F1 score of 63 is good. Don't forget that AUC evaluates your model on all thresholds, but F1 score is just one threshold. Maybe the threshold you used for evaluating F1 is not the best one? If you play a bit with thresholds, you will see that actually your F1 is better. Actually, these numbers look good to me, so I cannot say they are bad. Also, when AUC is around 90% or slightly less, it can sometimes be a bit suspicious – but yeah, it's good. Maybe it's a bit too good. I think it's fine.",Machine Learning Zoomcamp,2021,
996926,"Ankush
The reason it’s called this way, because the data itself is lying externally in another storage, like Google Cloud Storage, in our use case. Once the data is imported inside BigQuery itself and BigQuery saves it in its own format, whatever the format is, that will basically be an internal table to BigQuery. That will normally just be called a table. But when the data is lying outside BigQuery, like in CSV format, in this case, they are calling external tables because the data source is external.
Alexey
External tables are slower, and it's also more expensive to query them. In general, we would use these external tables only as a temporary place where we would keep our parquet files before they go to our data warehouse. Right?
Ankush
Exactly. Generally, that's definitely one of the use cases. You can also use it to load it into your internal tables. You can also use it to do some quick analysis, to see if your data is correct or not and things like this. Also, you can build external tables with partitioning and clustering. That can be a big problem in the future.
Victoria
So in this case, the bucket is ours and all that, but you could technically have a bucket that someone shares or something like that and if they drop it, you lose it. You lost that table entirely because it was not in your storage. You didn't own it, technically. [Ankush agrees] I think that would also be something to consider.
Ankush
Yeah. External tables are just metadata information. The metadata information would still exist if you create it. But if you try to query the table again – basically select star from this particular table that no longer exists, because the data source is deleted – the query will run into issues.
Alexey
I ran into these issues before. It says “source does not exist” or something like this and it just refuses to execute the query. 
Ankush
Exactly.",Data Engineering Zoomcamp,2022,
828550,"Yes, randomly. I think that's the easiest way.",Machine Learning Zoomcamp,2021,
580366,"I don't think so. It really is problem-dependent. Sometimes adding a lot of features can hurt your model. You don't want to add too many features and just hope everything will work out. You need to be careful. Sometimes adding a feature can result in model degradation. But there is no standard. It really is problem-dependent. For some problems, you need to have a few features and for other problems, you need to have a lot of features. For example, we talked about online advertisements. For these models, they use millions of features, like the device type – they use one-hot encoding. And when you use one-hot encoding, you get millions of features. It's really problem-dependent.",Machine Learning Zoomcamp,2021,
628477,"Alexey
You can just take a look at the thread with datasets that we had in Slack and I’m pretty sure you will find something interesting there. Then just go from there. I would start from a dataset. First, I would find the dataset and then try to think what exactly I can do with this dataset.
Ankush
I can say that if you finish one project where you are basically using more batch processing, then maybe the next project can be more real-time processing. Similarly if you're doing less Airflow, then do more Airflow. Something like that. Try to explore different areas to broaden your knowledge.
Sejal
Nothing else to add from my end, really. Basically just what you guys said. I think it really depends on the needs are. The situation is based on what you feel like learning, what you think is important for you to learn in order to implement, with respect to what you're currently working on. I think you'll figure it out. In addition to what you're learning in this course, maybe you could also try learning some standard software engineering skills on your own, which can be applied in data engineering as well, such as orchestrating CI/CD pipelines, workflows, GitOps, automated test cases, especially unit tests, maybe with pytest. You can use pytest in combination with Airflow. Make your code more production-friendly, basically. We haven't really covered that. I'm not sure if we will be covering these modules in the next iterations. If we find the time, then definitely we will add it as bonus lectures. This is also something you can try out.",Data Engineering Zoomcamp,2022,
846462,"Most likely, no, because I think it will complicate the logistics. I also want to see how many people actually manage to submit by the deadline and then make a decision. But I am a bit hesitant to extend the deadline. The reason for that is, remember that we have a third project. If, for some reason, you do not have time, you can still do this project but just a part of the third project, for example. To be able to get a certificate, you need to do two projects out of three. The reason we have a third project is exactly for this – maybe somebody is behind and still wants to take part. That's why, if you don't have time now, you can do this as part of the third project.",Machine Learning Zoomcamp,2021,
832418,"Alexey
Yeah, so you don't need to complete all the homeworks to get a certificate. The homework is for you to make sure that you understand what is going and to get feedback. In the homework, we try to ask questions to make sure that you understand the lesson. For example, one of the questions for this homework was to run Terraform in it. We want to make sure that you have set the environment to be able to continue in the following week. For example, if you already know some things but you want to maybe revise others, you don't have to do that homework. You just need to focus on what you want to learn. Then you will do a project and then you will get a certificate. This way, we want to emphasize the importance of doing projects, because this is how you actually learn. You might have the question, “But why do I need to bother with homework?” For homework, we give a little bit of incentive in terms of scores. That's basically it.",Data Engineering Zoomcamp,2022,
786074,"Alexey
To me, it's not about certificates. It's not about whether you know Hadoop or not. It's more about how you can lead a project, be a technical leader in the project, make decisions there, and drive these decisions. When you can do this, then you’re a senior. When you can say “Okay, we want to use this technology, not this technology, because XYZ reasons.” Then you can call yourself a senior. Of course, it varies from company to company. In some companies like Amazon, a senior is already a leadership role. In other smaller companies, maybe a senior is less of a technical leader, but still. To answer your question about having a scale – you can just Google for “public career progression matrix”. I think Dropbox has a public one. There are some other companies that have it public and then you can check there. This is how I would answer that question.
Ankush 
Yeah, absolutely. It really depends, if you are talking in terms of the companies, then it really depends upon the company. But if you are talking in terms of personal consideration, then you can set up a scale based upon what you think specifically.
Alexey
As for certificates, I personally don't care much when I’m hiring, for example.
Ankush
I have zero certificates.",Data Engineering Zoomcamp,2022,
780569,"Alexey
I'll start with “when”. When? Next week, because we extended the deadline by one week. This means that many of you are still working on the project. The deadline for that is next Monday, April 4th, 2022. That's the deadline. After that, the review starts. The review will start next Monday and on the Monday after that. 
When it comes to “how,” we have the guidelines (criteria) that you will need to look at. There will be a Google form. You will need to enter the ID of the project that you're reviewing. There will be a section in the forum that will say “For problem description, is it 0.1 point or 2 points?” and you will have something like this for each of the criteria. In the previous course, for problem description, you would need to select one of the points and for each dimension, you would need to select the option given. 
At the end, we have a script that takes some sort of average from all these points. This is how we get your final score. To see what projects you were assigned to review, we will have a table, where there is a hash of your email and the hash of the project that you need to review.",Data Engineering Zoomcamp,2022,
157429,"While Ninad was presenting, I did a quick sketch of what is still left to do. Today, we're starting with the capstone project, and then there will be peer reviewing. For the capstone project, we will have two weeks. The capstone project starts today and we will have two weeks. Then we will have one week to review the project. I am not sure this time that we will be able to extend it because in many countries, there will be Christmas break, which starts on the week when we should finish the peer reviewing. I'm not sure how flexible we can be here with the capstone project. So please try to finish before the deadline because I'm not sure we'll be able to extend it. The reason I shuffled things around a bit is because of that. I know that many people will have vacation and holidays on Christmas break. That's why we should focus on the capstone project right now. There is that. By now you probably have an idea of what is expected, because we already did the midterm project. I hope you got inspired by the two presentations we had right now. 
In terms of other things, Kubernetes and Kubeflow will still be there. For Kubernetes, I will try to record everything this week and make it available. If somebody wants to use Kubernetes in their capstone project, you should be able to do that hopefully. But the homework for Kubernetes, I'll publish the videos now. But you will have a lot of time to finish it. Actually, Let's say if your Christmas break is at the end of December, then the first week of January, you can spend on solving the Kubernetes homework, or however you prefer. I know in some countries, the break actually starts on the 31st of December and then finishes like on the 10th of January. In Russia, for example, it’s like that. You're free to choose how exactly you want to spend your time to do Kubernetes homework. 
For Kubeflow – setting up Kubeflow is really difficult. If you're really motivated, you can follow the videos and do that, but I will not have you do the homework. There will be no required homework – only optional, if you want to practice Kubeflow. There will be something, but no graded homework. Also, I promised a third project. For those who were a bit behind with the midterm projects and didn't have time to finish it for various reasons, we will have a third project. If you just like projects, you can also do that. It's not like you have to do only two out of three, you can do three as well. For the third project, officially, we will start it on the 10th of January. Of course you can start it earlier because it will be exactly the same as the midterm or capstone project, just a different dataset. We'll have one week for peer-reviewing it. We should finish the entire course by the 31st of January. 
The last thing – articles. For the article, you just take a topic that we haven't covered during the course and you cover it – write a short article about it with code and so on. For that, I haven't really figured out the logistics yet, but this is something I want to prepare now. You have all this time – December and January – to work on the article. If you want to do this, this is of course, optional. You don't have to do this – only if you want to. You’ll have two months for that, basically. 
Just to remind you – to get the certificate for the course, you need to finish two out of three projects. That's the main requirement and the article is optional – you will get some extra points for that as well. That's roughly the plan.",Machine Learning Zoomcamp,2021,
831512,"Ankush
Vic has experience with being a lead right now, so feel free to jump in. I think one of the most important things about being a data engineer is to know a lot of things and you need to know different stuff. You might want to be an expert in Spark, but you still would need some knowledge of Kafka. If you want to be an expert in analytical engineering and go deep into DBT, SQL, but you still need knowledge of data warehousing. To go towards a senior data engineer, I do feel that you need at least some knowledge of the other areas, but you also need a core competency in at least one or two of the areas. For me, when I was a mid-level, and I wanted to go to a senior, I realized that my company really depended upon Kafka. Luckily, that was also an interesting area for me. I took a lot of interest and developed a competency into that. But at the same time, I was also interested in data lakes in Spark and that gave me the opportunity to work on projects which could then lead to me being a senior data engineer, and then also leading a team later on.
Alexey
I would also add that you should check out the DataTalks podcast, where we cover things like that. Maybe not all of them are specific to data engineers, but I think you can find some of the talks interesting. Maybe this should be another podcast episode, because I don't think we can cover that in detail in 10 minutes.",Data Engineering Zoomcamp,2022,
302381,"In this case, you take the dataset that is available on Kaggle and you pretend that this is the only data you have. Then you do the split into the usual thing – test, validation, and train – or just test and full train. And then you decide how you want to validate your models using cross-validation, just holding out one part of the dataset. Just the usual thing. You don't have to rely on the test dataset. If you want to submit your model and then just show (maybe take a screenshot) your final performance, that's also fine. But it's not required.",Machine Learning Zoomcamp,2021,
684306,You just need to practice. I don't think there is a better way to do it.,Machine Learning Zoomcamp,2021,
54998,"Victoria
Yes.",Data Engineering Zoomcamp,2022,
198661,"Alexey
It's not over for you, because we only want to look at your project. If you're behind right now, don't worry. Just take your time. We will decide whether to give a certificate or not only based on projects, not on homework. So it's not over for you – you can catch up. 
Week 3 should be lighter. Week 1, you can maybe fast-forward directly to Docker Compose and then run it there. Week 2, depending on how much time you have, maybe you can go through the solution that will be published today and then it will give you everything you need for week 3, or you can use a transfer service. The video is already there. While watching the video, you will probably learn a lot already, like the homework solution.",Data Engineering Zoomcamp,2022,
786913,"I don't think there is any difference, to be honest.",Machine Learning Zoomcamp,2021,
568519,"Yes, I think writing an article on GitHub is good. You can basically have an article there. I would suggest trying Medium. It's not as difficult as it seems. On Medium, they can also start recommending your article, so more people will see it. But it's up to you. GitHub is fine. If you want, you can actually turn any GitHub repo into a website. It's called GitHub Pages. You can just take a look at that. Maybe instead of just putting your article in a Git repo, it's possible to turn this Git page into a blog page. You will need to follow this instruction. For me, it's not difficult. If you don’t not come from an engineering background or you haven't done this before and you're not very comfortable using the command line, it will take more time for you to actually figure this out. But I think it's worth investing some time into seeing how it works because you can easily have your own website for free – entirely for free. You don't need to do anything else. Actually, I have a secret. It's not a secret, actually. The DataTalks.Club site is actually a page that is run on GitHub Pages. Everything you see on the website is on GitHub. I use GitHub Pages to host it and it's free. All I need to pay is for the domain – DataTalks.Club – and the rest GitHub provides for free. I hope I convinced you to try it",Machine Learning Zoomcamp,2021,
300376,"Use cross-validation to check that. If on validation, you get better scores – if you drop one, drop one of them and then go for this. Sometimes it makes sense to drop even if there is a decrease in your score simply because maybe one feature is more complex to compute than another one.",Machine Learning Zoomcamp,2021,
617661,"Alexey
Well, it depends on how much time you have. Ideally, yes, you should try to run it. You should try to learn as much as possible from this code. I'm pretty sure if you try to run it, you will learn a lot from the other projects. But I know if you do that for all three projects, then you might not have time to actually, you know – have a life. So it's up to you. You can just check the documentation and see that things make sense there, and that you don't see any big mistakes that just jump out at you. You can do that, or you can run everything. It's up to you. 
But if you have time, please run it, and please try to learn as much as possible from your peers. Based on what I heard from students from the Machine Learning Zoomcamp course, they learned a lot from trying to run the projects. So please do that. By the way, I saw that one of the projects was run on the Yandex Cloud. I don't think it would be possible for you to actually try to reproduce it and run it. In some cases, you will not even be able to do that. If that's the case for you, then the only thing you can do is just review the documentation.",Data Engineering Zoomcamp,2022,
184650,Maybe. Maybe not. We will see. But all the content is available.,Data Engineering Zoomcamp,2022,
34583,"Ankush
Can you append to a parquet file?
Sejal
I think what they probably meant was – I think Alexey and I were also talking about this – for incremental loads, since the end result of week 2 has a BigQuery external table, I think by default, it's not in append-only mode. Right, Alexey?
Alexey
When I was executing it, I saw… I executed a bunch of DAGs runs and then when I went to BigQuery to see the results, it was the same table that we created after the first DAG run. Basically, it didn't append to that table for some reason. I don't know why. But then I think I saw in the week 3 videos that the way we create the statement when we create a table, it needs to use some kind of sort of wildcard. 
So we say that, “It's 2019, then minus 10*” and then BigQuery picks up all the files with this pattern. This is probably how you typically do this. I don't know – I'm not a BigQuery expert. But at least for week 2, you will need to upload data to Google Cloud Storage. You will not need to create these BigQuery tables. Then we will create a BigQuery table in week 3, and Ankush will show you – I saw this in the video. There is a SQL statement for creating the table.
Ankush
In BigQuery, yes, you need to use wildcards to pick up multiple files. If that's the aim of doing it, I would still not append to a parquet file, because parquet files are not meant for that purpose. They're not append-only kinds of file, they're more like columnar storage. Basically, if you append, you need to rewrite the whole thing. In that case, I would not suggest that. Basically, keep it open-ended. Close the file after you're done writing to it and then probably let BigQuery do the read. 
On the same side, if you are writing to a BigQuery table internally, and I think that's also covered later on in the videos, what BigQuery does internally is figures out after a couple of writes (let's say an hour or so) it figures out that there is an imbalance between your internal file structure, and it reclusters (automatically reshuffles) the data and creates a new file for you. So you don't really have to worry about them. There is no cost on your side for it. But that's how internally BigQuery does that, if that was the question.
Alexey
Maybe I can add a few words to that. The way I see it usually happens is, you have a folder in a data lake for a specific date, let's say. First, the structure is the name of whatever table, then year, month, day. And then within this folder, you have a bunch of parquet files. Then if you want to add more information, you just add another parquet file to that folder. This way, you get more data to this partition.",Data Engineering Zoomcamp,2022,
132467,"Yeah. If you want to experiment with Linux, you can try Mint. I'm using Ubuntu, as you can see. Mint is probably good as well.",Machine Learning Zoomcamp,2021,
682464,"Ankush
I think in today's time, if you're doing any sort of software engineering or data engineering or back end engineering, you need some knowledge of Docker. Because the world has moved around us in a way that you need to build your application, you need to deploy your application, and you need to maintain your application. And the easiest way to do this is to use Docker. I would say it is definitely important. Would you install Hadoop or Spark on Docker, maybe not? Maybe you will use dedicated hardware to do this, and you will not do it on Docker. But learning Docker, there is definitely no way around it. I think you need to have at least basic knowledge of Docker. I myself am not an expert in Docker, but I do feel that knowing the basics, knowing how to get around it, and knowing basic commands, will really help you to grow in your career.
Alexey
Maybe I could add a point of view of a data scientist. In data science, we often need to use libraries (things like XGBoost) that require some sort of native dependencies. Native dependencies are these things that you install with AptGet install in Linux and Ubuntu (like OpenMP and things like this). Let's say I want to use XGBoost in Spark, then I have a lot of trouble like, “How do I prepare my package with all the dependencies in such a way that I can submit to the Spark cluster and it doesn't fail?” When I found out that in Spark, we can actually submit Docker containers – we can prepare and package all the dependencies in Docker, and then submit it to Spark and then Spark would run it, so Spark would use all the dependencies specified with Docker – then my life became a lot easier. Then I didn't even need to worry about how exactly I prepared the ZIP archive. For those who don't know Spark, maybe what I'm saying doesn't make much sense for you. But believe me, my life became easier because I can just prepare a Docker file instead of figuring out how exactly I should run AptGet on Spark nodes. That was quite helpful for me, personally.
Sejal
What to do after that? I'm sure you must have read about the advantages of Docker and why it is useful in every stage of software engineering, be it backend or frontend or even infrastructure. It has a lot of advantages in terms of usability and portability. Also, it really downsizes your effort in terms of preparing deployment-based code. Very low maintenance and you can mock your production environment onto your Docker version and test things out without having to deploy it to production. This is why I would say it’s important.",Data Engineering Zoomcamp,2022,
469860,"Alexey
For homework, we will use automatic grading. You will not need to review each other’s homework assignments. The project is something that we’re planning for week 7 to week 10, so basically the end of the course. Nobody's late there yet because it hasn't happened yet. So don't worry about that.",Data Engineering Zoomcamp,2022,
242833,"Always after. Let's take this case. [image 1] Here, you split the data, so you have your test dataset. Then you split the data again – you have the validation dataset. Let me just do that here, just to show it. This would be a data frame. And now we'll split this data frame. Use 20% and 25% here, then validation. Okay, and now have data frame validation. [image 2] It’s still the same. Now all this SMOTE, downsampling, upsampling, and so on – you do only on this data frame train. You do not touch data frame validation at all. So you experiment with this. You need to make sure that in validation, your distribution of the target variable stays the same, because this is the distribution of the target variable you will see in real-world data. You don't want to change this, because if you change this, then you will not be able to evaluate your model against a real world scenario. It's very important that you do not change validation here and you only change train. Here, you can do downsampling, upsampling, SMOTE, and so on. You do this, you train your model, and then you test it on the validation dataset set. Then you check whatever target metric you want and when you train a full model, you do the same. You keep your test dataset intact – you don't touch it, you don't apply any downsampling there. You only apply all your techniques to the data frame train full.",Machine Learning Zoomcamp,2021,"892123_qa7_ml_zoomcamp_office_hours__week_9_pic2.jpg,892123_qa7_ml_zoomcamp_office_hours__week_9_pic1.jpg"
314135,"This is what mostly I see – usually companies prefer to go with Kubernetes, especially bigger companies, where they have people who can support the Kubernetes cluster. This is quite a popular approach. This is why I decided on covering when I was thinking whether I should cover Kubernetes or not because this is a fairly advanced topic and it's very difficult to cover Kubernetes in just one lecture. Similar to neural networks, it was not very easy. I think it's quite important to get some understanding of how Kubernetes works and to not be afraid of using kubectl. 
I would say, yes, it is a preferred industry option. Other options are also quite popular. For example, at OLX, where I work, we use Lambda. We don't use Elastic Beanstalk, for some reason, but we do use Lambda. We do use Kubernetes a lot, and we usually deploy things either through FastAPI or Flask. Mostly Flask, but we have a couple of projects that use FastAPI. Go with Kubernetes, although if you're doing a project alone and you don't have people who know Kubernetes quite well, I wouldn't advise using Kubernetes. Use something simpler, like maybe Lambda. That would be a better option.",Machine Learning Zoomcamp,2021,
554368,"More than one? In this course, we covered linear models and we covered tree-based models. In linear models we saw the C parameter, or if it's rich regression, I think it's called alpha – you should tune that. And then you should try decision tree, you should try random forest and XGBoost, and then different parameters, and then select the best model. Basically, just going through what we learned so far, and trying different models, and then different parameters for these models. You don't have to use XGBoost, for example – you can just use random forest. I suggest using XGBoost, or some gradient-boosting tree implementation. Just try, let's say, at least three different models and then for each model, try to tune the parameters. That should be sufficient.",Machine Learning Zoomcamp,2021,
276086,"I think around 200? 204, I think. Something like this.",Machine Learning Zoomcamp,2021,
126767,"For example, if you go on Kaggle and you see the number of notebooks available for the particular competition, it could be a good indicator that it's a good dataset. Other than that, we didn't really talk about multi-class classification. I think it's okay if you want to do multi-class classification, but the first dataset should probably be either a binary classification (if you see that the target is clearly zero or one) then I think you can go with this because you already know how to deal with numerical variables, you know how to deal with categorical variables, you have some idea of how to deal with missing data. So if you see that this dataset has these things and it's similar to what we did before, this dataset is a good one. If you're not sure, just ask in Slack and we will help you.",Machine Learning Zoomcamp,2021,
127707,"Ankush
Avro and Parquet are totally different things. Avro is row-based and Parquet is columnar, so if you are doing something like batch processing or data-analytical kind of work, Parquet is a perfect solution to use with Spark or something because it's super fast, to calculate something like a sum or account from a Parquet file or read a particular column rather than the whole column or the whole row itself. Avro, on the other hand, is very strong at providing flexibility in terms of backward and forward compatibility. So if you're using Avro with Kafka, or using something like a Protobuf with Kafka, that would be really useful. So think of them like different technologies altogether and use it for different use cases.
Alexey
And Avro, I guess, the use case would be streaming and Parquet is for batch jobs?
Ankush
Yeah, I think definitely, in the beginning, that would be the basic use cases. Yes.
Alexey
There is also something called OCR. 
Ankush
That's also columnar storage. 
Alexey
Oh, that's a different ORC. [chuckles] I remember watching a presentation on Berlin Buzzwords about this format. I remember seeing how cool it was in the presentation, but I never actually tried it.
Ankush
Neither did I.
Alexey
So it's an Optimized Row Columnar. Interesting. I never knew what it stood for. But is it widely used or most people just go with Parquet? 
Ankush
I have seen only Parquet until now. But I guess there will be some companies which will definitely be using it. It's not a bad technology at all. I think it's just that Parquet is famous and there are more compatible solutions for Parquet so people just tend to use that more often.",Data Engineering Zoomcamp,2022,
467959,"I think it is required. It depends on the cloud. If you want to deploy a Docker image to the cloud, then you need to put Gunicorn or Waitress inside Docker because as you saw this warning, Flask says do not use it for production. So you do not use it for production because the Flask developers will cry if you do this.",Machine Learning Zoomcamp,2021,
920089,"Yes, of course. That's what I'll do today.",Machine Learning Zoomcamp,2021,
72889,"Sejal 
In my previous setup, like I said, we had a bunch of Lambda functions where the execution was coordinated by Step Functions. This is how it would work in the case of Airflow.
Ankush
I have not used step functions at all. And honestly, I've also not dug deep into Cloud Composite of Google Cloud Platform. But there are certain criteria you need to choose and one of the biggest criteria is price. If you think that your price would be super less in comparison to the price you will pay for maintenance, for developing this, for deploying this, and most probably, scaling it in the future, then go ahead and use AWS Step Function. 
But if the price is just too high for your requirements or your use case, then maybe digging deeper into your own deployment of Airflow would be better. That would really depend upon your use case and how you calculate price.
Alexey
I think AWS Step Functions are relatively cheap, but they do not have as convenient a UI as Airflow. Things are more complex there. This was my impression of not using them personally, but seeing a demo of AWS Step Functions at work. We tried one project and we decided not to use them, because it wasn't as easy to configure these workflows. But I guess we’re also more used to Airflow.",Data Engineering Zoomcamp,2022,
287399,"Alexey
It depends on what exactly you want to do with the Reddit data, right?
Ankush
Yeah, exactly. I would focus on the final goal. Let's say, “I want to calculate for Reddit data.” You might want to say “What are the most keywords being used?” Or something like that. In that case, your transformations will directly depend on that.
Alexey
Or something like, “Active number of posts per day.” 
Ankush
Exactly, or, “Per hour distribution of posts.” I'm pretty sure that in the US, it goes up during the daytime and it goes down at night. So what are you trying to answer? That's the first question I would focus on. Once you know the answer to that, then I will know which transformations to apply.
Alexey
Yeah. That's why the end goal of the project is a dashboard, so “What exactly do you want to put on the dashboard?” and then go from there. “For this dashboard and for the data you use, what kind of transformations do you need to do in order to go from this data to the dashboard?” Then it will become clear that you need to do this group-by, that you need to join with this or that table, and so on.",Data Engineering Zoomcamp,2022,
724668,"Victoria
You just, again, set up a warehouse and then at the end, you define the connection. The only difference is that you would have to use the adapter, but DBT has several adapters. What I would do first, if you want to use DBT, is go check the adapters and make sure that it's supported. There are several that are official. For example, Redshift you could use. There are some that are community-supported and that where Azure is.
Alexey
Yeah, this is AWS, so Athena. Azure and Athena are here. Redshift and Presto here as well. For example, we have a Presto cluster, so we use AWS – and within AWS, we have our own Presto cluster. Potentially, if you have a similar setup, then you can connect DBT with Presto.
Victoria
In general, to add a little bit more to the answer. Let's say you have the project right now, and you have it in BigQuery. What you would do is change the profile and that's it. Then you set up the connection. You may have to make some changes if there's something in SQL that supports BigQuery or not the other way around. But other than that, you will be able to run the project in a few minutes.",Data Engineering Zoomcamp,2022,
686577,"Sejal
Maybe someone can help me answer this because I am not really an expert on the Kubernetes side. But Terraform lets you build static code templates for your infrastructure in an IEC-style manner. So it would work in cases where you want to have a static-based infrastructure or in cases where a set of infrastructure resources are destroyed at any point in time and you want to restore it from a certain image or a certain state – Terraform is useful here. In the case of Kubernetes, it's actually more like a spin-up cluster. You can deploy your services onto a Kubernetes cluster. The use cases are really different in this case. Maybe Ankush or Alexey can add more on Kubernetes.
Ankush
Yeah, definitely. I think Kubernetes and Terraform are very different. When you think about Kubernetes, you are thinking about deploying microservices or some sort of applications. That can be done by Terraform using AWS Fargate or AWS ECS solutions. But Terraform does very much more. Let’s assume that you do not have Kubernetes, then you will use Terraform to set up your Kubernetes cluster. But you will also use Terraform to set up, let's say, S3 buckets. In next week’s course, you will see that we use Terraform to set up a transfer service from Google Cloud Platform. All of this is not possible in Kubernetes. Yes, you can argue that there are certain services like Spark and Flink that are coming up with these new solutions, which can be run on Kubernetes. But still. For example, say you want to run Kafka clusters. In those cases, Kubernetes is not a good solution. You would like to have a more stable solution or a stable infrastructure in that case. That's where Terraform would definitely help you to set these kinds of clusters up.
Alexey
As a person who is quite far from infrastructure, I would add that usually we have people dedicated to that in our company. Terraform helps you not go to the web UI and click things. Instead of going there and clicking, you just have a file where you say that “I have this bunch of resources that I set up in our cloud.” Then, let's say, if you need to move from one account to another for whatever reason – this has happened multiple times at my work, when we needed to migrate to a different account, so we just do Terraform destroy in one account. Actually, we would destroy later, but we will do Terraform “plan and apply” in one account, and then go back to the old account and do Terraform destroy. Of course, it still takes some time, but you have everything in your code and you know exactly what kind of services you use, for example, what kind of buckets there are. We use AWS so for us it would be what kind of lambda functions there are. Basically, all the resources that are there. Kubernetes could be just one resource in this Terraform file. There is a comment that says, “Terraform is infrastructure as code and Kubernetes is infrastructure.” I think it's quite a concise way of summarizing it.",Data Engineering Zoomcamp,2022,
349959,"Alexey
Well, I think now we finished Kafka week – I think we should have called them modules, not weeks, because they didn't really correspond one-to-one to physical calendar weeks. Now, when we finish Kafka week, then we'll start the project. I think three weeks is reasonable, because we don't want you to spend too much on working and working on projects. Instead of being too ambitious, maybe you should think, “Okay, what can I actually do in two weeks?” The third week is for reviewing your peers. Let's say right now – today is the deadline for Spark. And then for Kafka, let's say the deadline is next week, so we will have until March 27 to finish the project, and then one last week of March to do peer reviewing. That's roughly the plan. We will, of course, see how it goes and maybe adjust. But for now, that's the plan.",Data Engineering Zoomcamp,2022,
578115,"Alexey
You mean how much time do you get? The deadline would be in two weeks, so it will be 28th of March, 2022. Then you will have one week for the peer reviewing.",Data Engineering Zoomcamp,2022,
597692,"I don't think I understand the question. Some validation needs to be separated through, I don't know what you mean so maybe just ask in Slack. I think if you're not sure, just do EDA on the training dataset – on the first train dataset, not on the full. But if you do it on the full train dataset, nothing bad will happen.",Machine Learning Zoomcamp,2021,
928855,"Yes, there is a hashtag. The hashtag is #mlzoomcamp. You can use that – put that in Twitter, in LinkedIn and I think you'll find some posts there. And of course, you can use this hashtag for your own social posts as well.",Machine Learning Zoomcamp,2021,
311710,"As I said, take your test data and put it away – don't look at this at all. For your validation, you can look at this, but do not use it for encoding. Do not use it for anything. You only want to apply what you learned on the training dataset. Let me draw it. [image 7] This one “test,” you take it and you hide it. You forget about it. Then “validation,” you only do transformations here. Here, you will do things like fit and so on. And here, you only do transform. Here, you never fit anything (on the validation dataset) you only fit things on “train.” You fit your model, you fit your transformers (like dictionary vectorizer is a transformer because it has this transform method). You only transform the validation dataset. Of course, for test, you also transform. You don't train on this, but you hide it away. You put it away, you don't look at this.",Machine Learning Zoomcamp,2021,420835_qa2_ml_zoomcamp_office_hours__week_5_pic7.jpg
296080,"Ankush
What we use is basically unit testing. We rigorously unit test other small components and then there can be an end-to-end test in order to test the whole pipeline, basically. So that's the literature. [chuckles]
Alexey
I think there is a thing called Data Kitchen. They have a book, I think. They have this DataOps Cookbook, and I think it goes a bit into data quality topics. I think they talk a bit about that. But I don't remember how detailed it is. Maybe this is something you can check. Have you seen this one?
Ankush
No, I'm just checking it out right now. So what's the point of Data Kitchen?
Alexey
I think they're consultants. “Stop band-aiding in your data pipeline.” I think they consult on how to do DataOps properly. Here, of course, you need to provide your email so they can try to get you as the client. [chuckles] But multiple people told me that this is a useful book.",Data Engineering Zoomcamp,2022,
568911,"Victoria
Yeah, I was at one in software that uses Spark. I don't use Spark, but I still got the job. I didn't take it. Going back to what you said before – no one is going to reject you if you don't use exactly what they use. What's more important, I guess, is that you can learn it. Regarding Kafka, it's important that you understand what's going on because you'll build a pipeline on top of Kafka as an analytics engineer. It depends on how much your data engineers will work, but you'll still have to consume that data. As an analytics engineer, you are going to be the bridge between those data engineers and the analysts, so it's very important that maybe not that you're an expert in Kafka, but that you understand what Kafka is. That would be the least you would have to know. I wouldn't recommend anyone to go and learn Spark if they are mostly focused on analytics engineering. As I said before, you need to understand the concept. It's more important that you understand what you have to model, how to do it, and all of these things.",Data Engineering Zoomcamp,2022,
525898,"It’s two weeks from now (October 18, 2021) meaning the first of November, 2022",Machine Learning Zoomcamp,2021,
971755,"Alexey
I'm very glad you asked that, because I think we actually did a few videos about that in week one. So check it out.",Data Engineering Zoomcamp,2022,
599256,"I don't think there's any rule of thumb. If your notebook is doing multiple things, then you deploy… Basically, each Python file should ideally be doing one single thing. There should be one Python file for training, for example, one Python file for testing the model – or something like this. I don't think there is any particular rule of thumb. Sometimes it makes sense to have larger files, sometimes smaller files.",Machine Learning Zoomcamp,2021,
409133,"Sejal
We were initially planning to, but we decided to move it out. We think it will be better to have a separate course on this. Alexey will be sharing more details on this. We do not have anything specifically planned on this, but maybe Alexey has something to add.
Alexey
Well, after this course finishes, we'll have another one that is called (as you might guess) MLOps Zoomcamp. Maybe some of you were wondering, “Why Zoomcamp?” We’re actually using Zoom right now to show the four of us, while the rest of you watch this on YouTube. But it's just a funny name. You can think of this as Zooming into some of the topics. That also works.
Ankush
I think it started from “boot camp” to “zoom camp” meaning it’s going online.
Alexey
With Zoom there is this limit of 100 people per call on the cheap plans. That's why for Machine Learning Zoomcamp, we decided to go with YouTube and kind of stayed in YouTube. Now we have more than 600 people. We wouldn't be able to fit into one Zoom call. But the name, I think, kind of got stuck. So it's funny.
Ankush
If you are interested in MLOps, take part in the DataTalks Slack group. In a couple of months, we will release some information about that – with more details and maybe a structure.",Data Engineering Zoomcamp,2022,
920712,"Ankush
This is part of your next week’s (week 3) course. If you look into the first video, you will find the answer there.",Data Engineering Zoomcamp,2022,
377708,"Alexey
The projects will be individual. Each one of you will need to come up with your own project and do this end to end. That's why it's not really a problem. For the next round, the concept will be the same – you will need to find a project on your own and solve it. We will help you find datasets for that, but it will be something that you will have to do end to end. That's why it's not a problem if your projects stay in GitHub. Maybe your question is also about homework –whether the homework will stay the same or not. This is something up to us to figure out. Right now, if every one of you posts the solution to the homework in GitHub, then theoretically, it's possible to find the answers and then use them. But then I would really question why you would do that? Why would you find answers online and put this in the form? What does it actually give you? Are you learning or you're just doing this to get this course certificate? I hope you're doing this for learning, not for the scores.
Ankush
Do we have a plan to do the next Zoomcamp?
Alexey
Let's see how overwhelmed we are by the end of this one. [chuckles] If we have any energy left. 
Ankush
The next one will at least be easier since we don't have to create videos again. [chuckles]
Alexey
That's the plan with Machine Learning Zoomcamp. I want to restart it in September with the same videos. I just need to figure out what exactly we're going to do with the homework – whether it should be new ones. There was a question in Slack about whether it's a problem that people are starting to share the answers in GitHub to this homework because somebody can reuse it. Just don't reuse it. Just pinky swear that you will not do this. And if you do this, then I don't know, you will get on Santa Claus’ naughty list and you will not receive a gift. [chuckles] That's the best that we can suggest here. Doing other things will complicate the process.
Ankush
There was a message on the live chat that, “The homework points are only for the leaderboard. There is no real benefit.” And I think that's true. Plus, if you have seen, we use hashes, so it's next to impossible to know whose email id it is. So there is actually no fame in it.
Alexey
Actually, what I wanted to do is create a page in GitHub for the ML Zoomcamp with the top 100 people there. So there will be some amount of fame at the end for people who agree to be there. We'll have to ask for explicit permission like, “Do you allow me to put your name there?” But again, I would question your motivation if you really want to get on that leaderboard without completing the homework. I'm not talking about anyone specific, but why do you even ask that question? I remember for Zoomcamp, the question was, “What if I cheat? Will I get caught?” [chuckles] Cheat. It's not my problem, right? It's your problem if you do.",Data Engineering Zoomcamp,2022,
510783,"I don't remember the last time I needed to use unsupervised learning in my daily work. I did need to use it for my Master's thesis. I think I used it for some Kaggle competitions as well. At work, I did something similar to clustering, which is called “locality-sensitive hashing,” which allows you to group similar items together. You can think of this as clustering. At OLX, we actually have a project that needs clustering, I’m just not the one who does this. But these projects don’t tend to happen very often. 
That's actually one of the reasons why I decided not to include unsupervised learning in the course, because I don't think it's as widely used as supervised learning. It is more difficult than traditional supervised machine learning in the sense that it's not easy to actually evaluate the performance of your clustering results. Let's say you group your clients into five groups, but how do you know if these groups are good or not? This is difficult to know. That's probably the main challenge of unsupervised learning – how do we evaluate the quality?",Machine Learning Zoomcamp,2021,
229264,"Yes, everyone who submitted their homework for their midterm project will have to review others. I believe I answered that elsewhere, so if something is not clear, we can talk about it more in Slack.",Machine Learning Zoomcamp,2021,
642829,"Alexey
One thing that maybe I wasn't clear enough on – please always include the code to your solution. Maybe some people didn't watch the first, where I said that if you don't submit the code for your homework, you will get zero points. I saw that in some of the submissions, some people put a dot there or empty or put some link to some Towards Data Science article, for example. Please don't do this. Please put your own code. If you don't, then you'll get zero points for the entire week. I'll make it explicit in the form for week three as well. So please don't forget to put your code there.",Machine Learning Zoomcamp,2021,
615971,"Alexey
If this is about week 2, then yes – after we upload the video with a solution. I will also update the leaderboard. It will probably take some time, depending on how clean the data is – if I need to do a lot of data cleaning, it will take a lot more time. So please don't put things that don't look like emails to the email field. When you fill it in, please be careful, because it takes time to clean this later.",Data Engineering Zoomcamp,2022,
766817,"Alexey
I don't think we will actually have time to summarize everything. Of course, I got inspired by Sejal – she prepares some notes and she's recording a video based on those notes. So we will share these notes. But you can also take notes and share your notes with your fellow students. This way you will learn in public – you will sort of regurgitate this content, you will produce something new – it will be useful for you and it will be useful for others. We do not promise summaries, but we encourage you to take notes and share them with others. Jupyter books? I think this is something that we will publish if in that particular week we have a Jupyter notebook or some code snippets. Of course, we will share that.
Ankush
We will definitely share code snippets and all the code that we use in the particular week. That will definitely be shared on GitHub",Data Engineering Zoomcamp,2022,
694057,"Actually, most of the time, I just go with functional because it's a habit. The reason for that is that I found many tutorials. For example, in transfer learning they use. They use functional style. More complex models also use functional style. That's why I'm kind of used to functional style more and that's why I thought it would be a good idea to cover functional style. From functional style, you can easily go to sequential, but when you go from sequential, I think it's more difficult to understand functional style.",Machine Learning Zoomcamp,2021,
244430,"Dmitry
Usually just testing different approaches. I don't know whether a golden standard or something exists.
Alexey
There is no science to it. Or maybe there is a science in the sense that you have your validation dataset and to treat the augmentation is a hyperparameter. You try it with this augmentation, without it, and then see what works better. This is annoying, because in neural nets you already have a lot of parameters and then on top of that, you also have parameters for augmentations. I don't know if I understand the second part – the ratio of generated to original images. Maybe it comes to the way we trained the model in this homework? First, we trained it on original images and then on generated ones. In this case, the ratio was kind of 50/50. It was an arbitrary choice, right? Was there any science to it? [Dmitry says “No”] Okay.",Machine Learning Zoomcamp,2021,
95645,"Alexey
I am not sure how to answer that question. Actually, here I have something, but I think this is a spoiler to the homework solution. [chuckles] But using Airflow helps. You can use Airflow to do that. That's what we've been doing this whole week (week 2) is using Airflow to first download the data from the NY taxi website, then parquetize it and upload to Google Cloud Storage. It wasn't S3. 
Oh! Now I see where this question is coming from. I guess. If we go to the NY taxi data website – we have a transfer service there – one of the videos was about transfer service. There, the transfer service was moving data from S3 to Google Cloud Storage. Now I understand the question. This data is already in S3 – if you look at the URLs, you can see that the URL contains “S3, Amazon AWS”. This is the name of the packet. 
This data is already in S3. We didn't need to do this. That was probably a long answer to what should have been short, but the data is already in S3, so we didn't need to do any of that ourselves.",Data Engineering Zoomcamp,2022,
754651,"Alexey
I think the question is asking if the model building features in BigQuery are good. I think they are good. I haven't done this myself. From what I understand, it’s pretty basic. “Ab initio” is something Latin, right?
Victoria
“Starting from or based on first principles.” It sounds very like Spanish because it's Latin, but I just wanted to check. [chuckles] It's usually used in legal terminology.
Alexey
Maybe it's coming from a lawyer who wants to become a data engineer. Anyway, the model building features are pretty basic, but it's a good start. You can try that and then switch to Jupyter Notebook for more advanced things, let’s say – when you need more flexibility. This is what I think. I haven't really had experience doing this in BigQuery. 
In AWS, it's a little bit different. I am not aware of a similar method. In AWS, you have Athena, which is similar to BigQuery. I don't know if you can create a model in Athena. Maybe you can. But in AWS, I would use Sagemaker and Sagemaker has Jupyter Notebooks. If it works for you, then it's good, and then when you need more complexity, you can switch to Jupyter Notebooks.",Data Engineering Zoomcamp,2022,
434596,"Again, the answer is the same – use cross-validation to find this out. Try with different weights and see what makes sense for your particular dataset.",Machine Learning Zoomcamp,2021,
838013,"Victoria
It's in the buzzwords, right?
Alexey
Yeah, it is. If you go to the repo, you can see that there. We will not have like, we will not have a practical week about this. It will be more like a five to 10 minute video explaining what it is and when to use it. But that's what we planned for this. No more.
Ankush
Maybe, we can spend a couple of minutes on it today as well. Data mesh is basically a concept. The idea is – Let's go back and say “How are data pipelines and the data engineering teams built in different companies right now?” You have people who are generating the data, there are people who are consuming the data, and then there is this one big team in the middle, called the data engineering team, DataOps team, or whatever you want to call it. Different companies have different names. These teams are responsible for ingesting the data, transforming it, putting it out there and making it in such a way that it's consumable in the right way, in the right fashion, and all that. So what data mesh wants to do is basically decentralize the particular role of this big team and wants to give the power or, let's say, the responsibility to the team who are generating the data and the teams who are consuming the data. Therefore there’s no layer in between. So instead of the team taking the whole role and doing this, it's kind of a better idea to build up services, and then give the responsibility, or give those services as a software, to these teams who are generating the data and consuming them and basically going on a higher level with respect to this. In this case, what happens is the team that’s generating the data is responsible for the quality of the data, for answering different questions about the data, and all those things. That's really useful because when you have multiple teams handling the data, the idea of the data, or the knowledge of the data, is lost. The best person, or the best team, to answer such questions, or the best team to have the knowledge of the data, is the team that's generating it. So if they are also responsible for putting the data into the data lake or data warehouse and then also responsible for the queries, then it attaches this whole thing together. That's kind of a very rough idea of data mesh.
Alexey
It’s a very abstract concept. Actually, if we go to the YouTube channel, there is a longer explanation of data mesh. By “longer” I mean one hour long – an hour and six minutes. [chuckles] So if you're into this kind of stuff… By the way, this DataOps 101 is also quite a nice one. You can check this out as well. And this one, Modern Data Stack for Analytics Engineering, is also good. Basically, you can check out these three – they're good. But if you're interested in data mesh specifically, the first one goes into a lot of detail, the second one – in a bit of detail.
Ankush
There's also a very nice blog post.
Alexey
Zhamak is actually writing a book about this right now. I think it's in early release. She's written five or six chapters. I don't remember. But it's in progress. It's going to be quite a big book right.
Ankush
Now, this blog post itself is like today's date. I read it in a couple of days, it was impossible to read it in one go. It's huge.
Victoria
I think you can also download the first two chapters or something of the book. And they give a quick overview as well.
Ankush
There's also another book by O'Reilly called Data Mesh in Practice by Max Schultze.
Alexey
This one is free, right?
Ankush
I think it is free. It's free.",Data Engineering Zoomcamp,2022,
624888,"Ankush
Is it always a good practice? No, obviously not. Because there's never just one way of doing things. But in some cases, this definitely makes sense. For example, let's say you're using Snowflake as a data warehouse. That's a very expensive data warehouse, honestly. In those cases, you might want to keep a data lake layer for machine learning solutions. That will really help you cut down your costs and even maybe remove some data from the data warehouse if it's getting too expensive or something. There are definitely use cases in which I would definitely recommend everybody to have a data lake before a data warehouse, but not in all use cases. If you are a small company or you have a very small dataset, and you want to be quick, then putting in an extra layer of the data lake might just have a negative impact on the overall performance and speed.
Alexey
I am thinking of any data we have in our company that goes directly to a data warehouse, and I don't think we have anything. Everything first goes to the data lake and then some of this data might end up in a data warehouse. But this is, let's say, only 10 to 20%. The rest stays in the lake.
Ankush
Yep, I think that's because your company is very mature in using data. Your company is also very mature in saving this data. Generally, in a lot of companies, the maturity level is not that great. And in those cases, having a data lake might just create a data swamp. [chuckles]",Data Engineering Zoomcamp,2022,
940532,"I have no idea what you mean. What does it mean “It just switches me to root in /app”? Maybe ask in Slack and put how your Docker file looks like and what command you used for building the image. I hope you watched the video about Docker that we had a couple of weeks ago. If you haven't, or it was a long time ago, maybe you should rewatch it and maybe it will clarify some things. Again, please share the Docker file, the commands you used for building the Docker image, and let's try to figure out what's wrong.",Machine Learning Zoomcamp,2021,
843322,"Alexey
I'm not sure about the last part –how can Docker be provided in Terraform?
Ankush
Can you run stuff from Terraform? I think that has always been just for... You can set up an Airflow cluster with Terraform, but I don't think you can execute a job there.
Alexey
So it's something like, you use Terraform to prepare the environment (to prepare the thing where you will run it) and then you will need to actually run something. There will be at least two separate commands. First, “Terraform apply”, and then “Python run” or something like this. 
50:02 [1 upvotes]
Are we allowed to contact the instructors directly to discuss projects? There are things I am unsure of regarding my project as I'm using some new tools. 
Alexey
I think it's best if you ask these things in the Slack channel because A) others will be also interested in learning the answer, so it scales better instead of everyone writing to us directly – we just answer once and then everyone sees the answer. And then B) Others can answer this. If we are not available right now, somebody who knows the tool that maybe we don't, for example, Pulsar, one of the students might know it and then they will be able to help. So it's best to use the channel for that, and not contact us directly.",Data Engineering Zoomcamp,2022,
574914,"Alexey
I think this question was already asked last time. I'm not sure about doing that. It makes sense in some cases, when you have a follow up question to a question, maybe. But I think voting makes more sense, because maybe if a question is more interesting for 3, 4, 5 people, then to me, voting is more natural. I don't know. Maybe?
Ankush
I think voting is more beneficial for the bigger group. But if some question is not answered, please ping that in Slack, and I'm pretty sure somebody will pick it up.",Data Engineering Zoomcamp,2022,
903950,"I think I showed this multiple times in the lessons. You use the validation dataset, you compare your different models on the validation dataset, and the one that has the best performance should be the final model. This is a good rule of thumb. It's not always the end of the story. Remember, when we talked about cross-validation. Let's say if you do cross-validation and you see that your model has very good performance, but also has very high standard deviation, it's not a great model. You probably want to have a model that has lower standard deviation, even if the performance is slightly worse. Right now, this might be just too much information for you, so just stick to this rule that whatever is the best performance on the validation dataset wins. Later, when you work with this, you will develop this intuition of how to actually pick the best model.",Machine Learning Zoomcamp,2021,
604487,"No, you do not. Building locally is fine.",Machine Learning Zoomcamp,2021,
790919,"There are multiple ways. The first way is to not. Just train a model as is. You just need to be careful not to use accuracy as your evaluation metric. You use precision, recall, F1 score, AUC, and that's it. The second option is… Actually, there is a very good talk in our YouTube channel about that – Machine Learning Design Patterns, where Sara talks about one of the patterns, which is about handling imbalanced datasets. I think this is the rebalancing data pattern. 
You can just take a look at this and Sara will explain how exactly you can deal with unbalanced datasets. One thing to note here is that you do this downsampling and upsampling – you do this only for your training dataset. For the validation dataset and for training dataset, you leave it as is, because you want to evaluate your model. Let me just draw it, I think it's better. 
Let's say you have your dataset, and you split it into the usual 60/20/20. Then you set the training data aside. What you have is this training dataset and this validation dataset. Then you can do all sorts of tricks for training datasets – you can do oversampling and you can do undersampling. And you can check out Sara’s talk for details on how to do this. There are techniques like SMOTE for generating datasets. You do all that, but only on the training dataset, and the validation dataset you leave alone. You leave it untouched. You do not touch the validation dataset at all. You do all sorts of things there and then you evaluate your model on the validation dataset. 
It's important to use precision, recall, F1 score – you don't use accuracy here. Here, you can also experiment with different things like, “Okay, if I oversample the minority class, or if I undersample the majority class, which way of dealing with this works better?” And then you just apply to the validation dataset and see which one works better. [image 1] This is the idea. Then, of course, after you find the best approach, you can test it on the test dataset, and you also leave the test dataset alone – you don't modify it at all. You do all this oversampling/undersampling on the full train dataset.",Machine Learning Zoomcamp,2021,46632_qa6_ml_zoomcamp_office_hours__week_8_pic1.jpg
150648,"Yes, of course. You train it in exactly the same way as with ‘sparse=False’. You do that and think the code shouldn't change at all. From the example, you can see it was even faster. This is how we train on sparse matrices. There are some advantages and disadvantages. I just didn't want to go into sparse matrices for this course, but they are useful. There was a question, “What if we have 100 categorical variables?” This is the case when you want to use sparse matrices – when you have a lot of categorical variables. If you use dense – let me maybe draw it here. This is dense, and let's say, you have zero here and the rest are zeros. This is column number 0, 1, 2, 3, 4 and this is row number 0 and 1. A sparse representation would be – for row number 0, you have 4 as 1 and that's it. Then for row number 1, you have 3 as 1. So you don't write zeros here. Since four out of five values here are zeros, you just don't write them. This is when you use sparse matrices. [image 3] I just came up with this, but I think it's a good rule of thumb. Let's say 50% of these values are zeros, then you can just go with a sparse matrix. But, actually, it was a bit faster. So the use case is – when you have a lot of categorical variables.",Machine Learning Zoomcamp,2021,997256_qa2_ml_zoomcamp_office_hours__week_5_pic3.jpg
478083,"Victoria
I'm not sure what to answer. I mean, yes, we can do it, definitely. I am not adding that as part of the project, at least not in the workshop. The reason for that is because we want people to learn and we have to also consider that people are entirely new to this concept. I thought it would be too much overhead to try to cover things that are already quite complex day-to-day. Even though I work daily with DBT, I work with slow-changing dimensions on a daily basis, for example. So that's also why I think it's something that you will look up at the moment you use it. But you can definitely do that with DBT, especially deletions after implementing a load step, this can be done with hooks. I think there's a section I did with advanced knowledge and I link hooks. Like incremental models that I mentioned before. As for changing dimensions, I didn't link it, but I could link it. This is something that you could use with snapshots, which I think I added as a concept. For updates, I guess you could also do it in a pre-hook and a post-hook. But this is also something that you would do with an incremental model. In an incremental model, when it loads that new chunk of data, you can use a merge. Or you can do, depending on the workers I use, you use it could be an insert and an update. So that's something that would work on a unique key. For this project and also for the final project, it's up to you. The more you understand, the more complex you can go. There's no limit. I hope that answers that.",Data Engineering Zoomcamp,2022,
596763,"Ankush
If you're using Java or Scala, you might be somehow using object-oriented programming under the hood. Let's say, for Scala, if you're using Spark, you would be creating some user classes – if you're not using Dataframes, if you're using RDD. And if you create that, that's already object-oriented programming in some ways. Obviously, it will not be as extensive as it might be for some libraries, if you are working on some libraries, but you can definitely do that. However, I would still stick with data frames and maybe other stuff, rather than using core object-oriented programming technology for data engineering. What do you say, Alexey? 
Alexey
I think for streaming, like producers, consumers – they are objects, usually. Right?
Ankush
No, they are just bytes. You can convert them in Python. You can just convert them to a dictionary. And  in Scala, you can also just convert it to a dictionary if you want... [cross-talk]
Alexey
I think that consumes – let's say you have this Kafka consumer... Usually... I'm trying to... I remember that in Kinesis we kind of used our own thing, so that was a class. And then in this class, we will have a method like “consume,” that we define. We extend from the base class and then we say, “For this consumer, the run method (the consume method) is this one.” And then we just define the logic for processing the message and the logic for pulling the message or committing lives elsewhere.
Ankush
Yeah, but that's just a style of programming. Regarding object-oriented programming, I understand it more like interfaces, inheritance, polymorphism, and all that. That's why I say – Don't overcomplicate it, because it's just data.
Alexey
I think we did over complicate it in that example.
Ankush
[laughs ]See, you can use OOP, but you might just overcomplicate things.
Alexey
But yeah, I think it was inheritance – so we had a base object that can pull the message, that can commit, and then we extend it, and then we define this consume method. It's useful. Maybe it would be easier if we just had a function “consume” that would do the same thing, right? I think in some cases, you can have a better modularity (modular, nicely-documented code) if you use OOP (object-oriented programming). In some cases, it's just a mess and you don't know what's going on.
Ankush
Exactly. For simple stuff, you generally won't need it.",Data Engineering Zoomcamp,2022,
739571,"This is why you need a validation set. You use a validation set exactly for this. You use a validation set to try different ways, like let's say you want to try different C's or you want to try different alphas – you use a validation set for that. Or you want to get rid of some of the features and try to see if the score improves or not – you use a validation set for that. Also, this week, we will talk more about evaluating models (binary classification models in particular) and we will talk about cross validation, K fold cross-validation that are also useful for that. But in general, just use your validation set for that.",Machine Learning Zoomcamp,2021,
199659,"Alexey
This is something I didn't show how to do in week five. Ideally, yes, because it is one of the steps in your workflow. So yeah, you should use this. You definitely can and you should.
Ankush
Yeah, absolutely. And if you decide not to, for some reason, please document it – how do we reproduce this? Or rather, how does the person evaluating it reproduce it?
Alexey
But there are Spark Operators. Here, you can see how to do this. It's just an operator.
Ankush
I think the SparkSubmitOperator one would be the important one for us. I think this one would be the one you have to submit your Spark job, right? So maybe this one might be helpful.
Alexey
To be honest, I don't really know how to use this because at work I use a wrapper around Airflow. I just write a YAML file saying, “I want to execute this job on this cluster.” Our data engineers made it very simple for us to use.",Data Engineering Zoomcamp,2022,
623076,"That's a very broad question and really depends on the background you have. Let's say, if you're a data analyst, then you probably learn things differently from a software engineer. Or if you don’t come from an IT background, you probably want to take a different approach. So it really depends on your background. For me, what was helpful is the course on Coursera by Andrew Ng. It’s very old – from around 2012. This was one of the first courses I took. 
Another thing that helped me was Kaggle. Of course, the current course is based on my book Machine Learning Bookcamp. I hope that at some point, somebody will say that this is a must-have resource for a beginner in machine learning and AI. I'm a bit biased here, of course. I might say that it is, but I don't know if it's necessarily true. I'll leave this up to you to decide whether that's the case or not.",Machine Learning Zoomcamp,2021,
169051,"Flask is – let me draw it. Let's say you want to create a web service. [image 5] The web service gets some requests and then the web service responds with something. It could be predictions, it could be something else – it doesn't have to be related to machine learning. To implement such a web service, you can use Flask, you can use FastAPI, you can use any other framework for creating web services. Here, I use Flask. You can use something else – up to you. But Flask just allows you to do that. 
When to use POST? For example, let's say for “get,” you usually parse your parameters of a query. Let's say if we go to Google and put in “test,” you see that this q=test (https://www.google.com/search?q=test&oq=test&aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1&sourceid=chrome&ie=UTF-8). [image 6] This is a parameter that we pass through a “get” request (this is a “get” request). But sometimes, let's say if you want to score your customer, then you don't want to put here that gender=female, contract=one year, and so on in the string. Then you send JSON in POST. So in POST, you can also add some body into your request. This is when you generally use post. 
General use case of Flask is to be able to create a web service.",Machine Learning Zoomcamp,2021,"567302_qa2_ml_zoomcamp_office_hours__week_5_pic5.jpg,567302_qa2_ml_zoomcamp_office_hours__week_5_pic6.jpg"
145810,"Ankush
No, I don't think that's the focus of the course. I think the focus of the course is to learn data engineering basics, and to learn how to develop software in Kafka and Spark. We might cover some test cases, yes. But we will not be an extensive writing good unit test cases course",Data Engineering Zoomcamp,2022,
449040,"Sejal
I wanted to address this based on some feedback we've received on the course that we have scoped around. Terraform is particularly DevOps-based technology. Because data engineering has a close overlap with DevOps in terms of building the infrastructure stuff, this is why we also introduced Terraform more as an extra, in order to use it. This is how we designed our course. We did not scope it in a way where we are covering the basics of Terraform. That was not the intention. It was more like a part of the week 1 prerequisites, where we are using Terraform in order to build our infrastructure so that we can focus on the more core parts of data engineering. To answer this question, I would say that for a junior DE, maybe it is not as important, in my opinion. Maybe a Terraform Associate Certificate is something that you can ask whether your current job or your company is requiring you to do it – if you're heavily focused on infrastructure work. Otherwise, I would just say to just learn the basics of Terraform if Terraform is really required at your job in order to build the infrastructure stuff.",Data Engineering Zoomcamp,2022,
193489,"Alexey
Well, I guess if you want to do a dashboard, then you will have some aggregates. I have a hard time trying to imagine a dataset that you can just take in raw form and put it into a dashboard. Maybe if it's already grouped (if it's already aggregated), but then it's probably a very small dataset and that's not suitable for this course. Then I think you should maybe select a reasonably big dataset that does require transformation. But if you're in doubt, just ask in Slack. I really don't know what kind of dataset you can just put on the dashboard without any transformations. 
Ankush
Yeah, some minimum transformations would be required. 
Alexey
Some counts, some group-bys, maybe joins.
Ankush
Exactly. I think it's better to choose a dataset that will allow you to do all that.",Data Engineering Zoomcamp,2022,
800512,"I will tell you that you're really missing out if you're not doing the deployment step, because this is one of the most important steps in a machine learning project. If you train a model, and you finish with this –you have your notebook with a pickle file – this model is useless. Nobody can use this model or you have to rely on somebody else – you will need to give this model to somebody else and say, “Hey, can you please deploy this model?” And they will reply “Yeah, but maybe next year. This year, I'm busy with my stuff. Sorry.” So I think the ability to be able to deploy a model is one of the most important skills you can have. I do suggest, maybe not for this midterm project, but in general – if you're serious about data science, I think you should learn Docker, you should learn Flask, you should learn some of these tools. They will be very useful for you. So please do this. 
To actually answer your question – you couldn't finish this, maybe you didn't have enough time. That's understandable because this is complex. I understand that. Especially if you're doing it for the first time and all this Docker stuff is new to you. Will it be sufficient to pass the midterm project? I do not know yet because I want to first look at the distribution of scores. Let's say a lot of people, for example, 90% of people submitted a midterm project and didn't have this step, then I'm not sure if it's okay to say it's not adequate. Basically, I want to decide on the threshold for passing the project after seeing the scores. Probably it will be better if I do it then. 
That said – I strongly encourage you to do this. You still have a couple of days. There is a good tutorial from Ninat in Slack, where he shows how you can use Google Cloud Shell for doing Docker stuff. You don't even need to install Docker on your computer. You can use your Gmail account, your Google account, to do this stuff. Take some time, go through these tutorials, and you still have two days. I think you'll be able to do this. If you're still training the model, stop doing this and try to do deployment, please. You will be grateful when you learn how to do this.",Machine Learning Zoomcamp,2021,
270018,"Sejal
The official Airflow setup, like I mentioned, the documentation is quite overwhelming, because they have decided to provide at the default level all the services that are required for a multi-node setup. Now, the workshop that we have is using a single machine. A multi-node setup which is more compatible with scalable clusters, for example, is not suitable for a single-machine configuration, especially if you are just working on something like an 8GB RAM. 
This is also why I made the recent announcements and created the no frills version, where we removed unnecessary services such as the Redis queue, which enables a multi-node setup, also the triggerer, workers etc. Only the main things that are required are actually the scheduler, the executor which is not a threaded executor or multi-node executor, but instead a single-node executor, which is a local executor. Along with that, the web server if you want to use a GUI-based platform for Airflow. So this is why your RAM and CPU usage is conserved and this is why it worked for you.
Alexey
When it comes to this smaller version of Airflow, if you have problems with it, like I do, maybe it's worth trying to give it a try and run it on a cloud virtual machine. This is where I don't have any problems with this. I see other questions about Airflow being a memory-intensive resource. The best way I solved it, for me personally, is using a cloud virtual machine. On my Windows computer, with my Chrome open at the same time, it's not ideal. You have to choose either Chrome or Docker Compose with Airflow. Chrome is quite useful so that's why I opted to do it on a virtual machine. So maybe it'll work out for you as well.
Ankush
Looks like it's time to buy a new machine.
Alexey
Yeah, I just did. I think I bought this computer something like a couple of months ago and it's already like this. But I blame Chrome. I don't know, no matter how good your computer is, the moment you open a couple of tabs in Chrome, it just says, “Okay, there is some RAM. Let me take it.” And then there is not much left for the rest of the processes.",Data Engineering Zoomcamp,2022,
742537,"Alexey
For the homework, you need “for hire vehicles data” only for 2020, but for the lectures (to follow along with the material) Ankush uses 2019 and 2020 “yellow taxi” data. So to be able to follow along with the videos. To execute the same queries, you need “yellow taxi” data for these two years, but for the homework it’s only 2020. You wanted to mention “green taxi” data, Victoria?
Victoria
Yeah. Do you think they'll need 2019 and 2020? As long as they have some green tax data, their project will work.
Alexey
We're now talking about week 4 – about analytics engineering – because we also use some green data there. In the homework solution, I show you how to upload “green taxi” data as well. This is quite a small change, so if you already know how to upload “yellow taxi” data, you will just need to change a few lines of code. Basically, you just find/replace “yellow” to “green” and that's pretty much it, because the URLs have the same pattern. But this is for week 4.
Victoria
To summarize, by week four, you should have the “yellow taxi” data for 2019 and 2020 that you use for the videos to follow Ankush. 
Alexey
For “for hire vehicles” there are two options – one is high volume, the other is low volume. So we need “for hire vehicles” not “high volume”.
Victoria
“For hire vehicles” = 2019. And then you can use the same to get green taxes data for 2019 and 2020 to be able to do week 4.",Data Engineering Zoomcamp,2022,
196917,"Maybe I will. I don't know. Actually, I started to like Windows, because editing videos on Linux is terrible. On Windows, it's much easier. And a bunch of other stuff. When it comes to development, I don't think there's anything better than Linux. But when it comes to other simple day-to-day stuff, like editing a video or editing an image, then I don't think Linux is the best environment for that.",Machine Learning Zoomcamp,2021,
322444,"I think people have good intentions. I don't think somebody will intentionally give you a bad score. If somebody wants to intentionally give bad scores, please don't. Because why would you do this? Seriously, please don't. We're here to learn. If somebody's giving their best and you see that, we will have a matrix that you can use for evaluating. Just give the score the person deserves. I don't have a link with me, but Coursera actually did some research on peer reviewing and they found that peer reviews actually work quite well. People don't give bad scores just for the sake of it. Another reason why we have three people reviewing one project is exactly to avoid that. If there is one person who is giving everyone bad scores, then we'll take a median to avoid that. That's why we have three scores. Also, think of this as your opportunity to get feedback. People will write something saying, “Hey, I had problems running this.” It's good when somebody tries to actually run your project. So I don't have any answer here apart from that. Please have a bit more trust in people. I think they have good intentions.",Machine Learning Zoomcamp,2021,
717437,"What can I say? If you want to work as a machine learning engineer, you need to be able to set up environments. Practice makes perfect. You just need to train more and then eventually you will be able to do this.",Machine Learning Zoomcamp,2021,
951625,"Victoria
I don’t think you need to. I've been in Berlin for three years. I do speak German, but not for work. It will depend. If you go for a startup, of course, you're going to be safe. Just try to make sure to ask that before and all of that. It can happen to you that you end up in a team where there are a lot of Germans, in my experience, and then you will feel like you need to speak German in order to integrate. 
That's the thing I would recommend, though. This is outside of the learning part – if you live in Germany, try to learn German so you can actually integrate. At least in my case, I feel like it has helped me a lot to feel more in the country and be able to go to a doctor and or receive calls and stuff like that. I speak at least a bit of German, I don't speak super-fluidly. But it has been quite helpful. But not to work.
Alexey
Maybe it's also helpful to separate Berlin and the rest of Germany here. In Berlin, it's very international. In many companies in Berlin, if you go and start speaking German with people there, they will just look at you and say “Sorry, I don't understand you.” But if you do this in Munich or any other German city, they will speak to you in German. My German is quite bad. I’ve lived like five and a half or six, seven years (I lost count). It’s just been too long. Now I start feeling ashamed that I still don't speak German. [chuckles]
Victoria
Everything I said before, disclaimer – in Berlin. But it's true. You can even sometimes go to coffee shops and they reply to you in English because the person doesn't speak German, which is quite crazy for the German capital. 
Alexey
Most of the couriers who deliver things also don't speak German. I can hear that from the way they speak that they don't speak German. And that's good for me, because I don't speak it. [chuckles] We just switch to English.
Victoria
Yeah. And it's true that you can get everything in English. There was also one small part of the question “do you need a visa” and all that. It will depend. I have Italian citizenship so I don't need a visa, for example. But then every other country, I would say yes if it's not European.
Alexey
You don't need German for a visa. If you already have a work contract, then you just use this contract and that's enough to get the blue card visa. “Blue card” is a special program. But if you want to get a job seeker visa, then I think you need to show some level of proficiency in German. I don't remember if it’s A2 or B1 one? You don't need to spend a lot of learning to get to that level.
Victoria
For a job seeker as well? For someone like a data engineer? Because I think for IT, the requirements are less strict.
Alexey  
I think so. Maybe not anymore. Five years ago, I think it was a requirement to get at least A1 level.
Victoria
Not anymore. Because I know a lot of people there have job seeker one and they definitely don't speak German.
Alexey
Okay. Then ignore what they said and look it up.
Victoria
Yeah, definitely look it up. [chuckles] It also will depend on your country and if you have some kind of degree or something like that, then it may also change things.
Alexey
Right. For the Blue Card, you need to have a degree. Without a degree, you cannot get into this program. Check the internet.",Data Engineering Zoomcamp,2022,
878140,"Yes. You can submit them anywhere you want. Did I say 20 pt? I don't remember. But yeah, they do work. But it should be a good article. “Good” meaning, it's not you copy/pasting the code and nothing else. You actually need to explain what's going on there. Also, about the article – closer to the time when we will be working on the article, I will have some guidelines for that as well.",Machine Learning Zoomcamp,2021,
240572,"Yeah, indeed, that's the case. When you move your threshold, like when we looked at the precision recall curve. This is recall, this is precision. When our precision is high (this is a very high threshold) this could be 0.7 or 0.8. The precision is very high and recall is low. The distribution would be – a very tiny amount of customers will be predicted as default and the rest will be okay – no default. [image 4] Of course, again, use your best judgment. If for this model, it's really important to be precise but you don't care about identifying all defaulting or churning users, then you can set a higher threshold. But of course this changes the distribution.",Machine Learning Zoomcamp,2021,610225_qa2_ml_zoomcamp_office_hours__week_5_pic4.jpg
159178,"Ankush
We have all the operating systems among the four of us, I guess. I'm using Mac. I guess, Alexey, you're using Linux.
Alexey
I have Windows and Linux.
Ankush
As you can see, we all have different operating systems, so it's pretty open to all operating systems. But generally, I think the most supported ones would be Mac, Windows, and Linux. 
Victoria
We have a lot of things in the cloud anyways. A big part wouldn't even be dependent on the OS. That's also nice to know.
Sejal
There will be instructions available for any alternative OS you're using. For example, some part of the GCP setup may not work in a standardized fashion if you're using Windows, but Alexey has prepared some special instructions for that as well. There will be updated materials specific to the kind of OS you'd be using. But if possible restrict to the top three. [chuckles]
Alexey
Yeah, I decided to actually do this course on Windows and prepare all these materials on Windows, because for the previous course, most of the students had Windows and I had trouble trying to figure out why some things don't work there. That's why for this one, I decided to do it on Windows. This way I already know the answers to some of the questions why something doesn't work, hopefully.",Data Engineering Zoomcamp,2022,
744169,"Alexey
I think in the lectures we used five. Actually, in the next lecture – this week – we will cover classification. One of the lessons there is about one hot encoding. There we will see how to actually use all the values, not just top 10, or top 5.
Dmitry
I think the question is regarding what we discussed with you yesterday.
Alexey
Oh, okay. If you have a variable with 20 options, I think one hot encoding is still good. But let's say if you have a variable with 100 or 1000 options – what do you think, Dmitry? Is one hot encoding still a good option? 
Dmitry
Usually, if we're talking about a linear model, then no, it’s not. For sure. I mean, it can work out in the ensembles of the trees, for example, random forest. But if we were talking about linear models, it cannot be good in this situation and you can use, for example, label encoder. Basically, instead of creating n numbers of columns with, let's say, zero and one, you can use one column and specify the numbers there. So if you have, let's say, 1000 different values, that will be 1000 different numbers. This is how the label encoder works. 
Alexey
Well, I think you said that for linear models, it's… maybe I got confused. But I think for tree-based models, this way of encoding (label encoder is good) but for linears, it's not.
Dmitry
I think it also kind of depends on the curves of dimensionality or how big your feature set is in the end.",Machine Learning Zoomcamp,2021,
443865,"I don't think I'm the best person to do that. I'm a Docker user, not more. I don't know exactly how it works. I think it uses something like cgroups. I'm not sure – don't quote me on that. Perhaps if you just look up “How Docker works.” There are articles about this that can be found on Google. Basically, the short answer is – I don't know how it works internally, but I know how to use it.",Machine Learning Zoomcamp,2021,
157200,"I don't know much about different Flask modules. For machine learning, you probably don't need to know them. You just need to play around in Flask. Most of what you need as a machine learning engineer is in that course. There are not so many, but that should be sufficient, in my opinion. Maybe I'm wrong. I'm happy to talk about this and we can do this in Slack.",Machine Learning Zoomcamp,2021,
564293,"Yeah, we actually have some good tutorials on GitHub. If you use AWS, you can just follow them. The materials we have – deploying with AWS. If you don't use AWS, we have some articles from the community in the “Deployment tutorials” section. How to use PythonAnywhere, how to use Heroku – there are some tutorials here from the community that are quite good. Let's say if you want to use PythonAnywhere, from what I see from this instruction, it's relatively simple. And it's free. You can just start with that and use that. Other than just going ahead and using this, I don't have any other suggestions.",Machine Learning Zoomcamp,2021,
40420,"If two people want to work on the same dataset, but individually, please feel free. I imagine there is no way to control this. If you go to Kaggle and you both select the same competition, even if it’s just accidentally, it can happen. As long as it's not exactly the same code – as long as you don't copy from each other – that’s fine. Yeah, please don't copy your code.",Machine Learning Zoomcamp,2021,
298345,"Alexey
I think it's okay. If you have any particular problems, maybe ask in Slack. But if this is the first exposure for you to all these tools, it might be overwhelming. There is a lot of information and it's understandable that some things aren't clear. When I actually rewatch some of the videos I did, (when editing, for example) I realize “There is no way people will understand what I meant here.” It is fine that some things are unclear. But if you have a particular doubt, for a particular line that you don't understand, and you want to understand it, you can just ask on Slack. Anything you can do to supplement? I would just say to go with projects. Then, when you get stuck, Google, or ask for help and this is how you will learn and fully understand things.",Data Engineering Zoomcamp,2022,
545395,"Victoria
In the course, specifically, I think we cover Postgres mainly because it's easier and because many people couldn't set up a Google Cloud account. Then it's a nice alternative because at the end, you still have a database and you can query the data and you can load the data. 
You still get the concept and get to practice that. In real life, I've only seen it in smaller companies. Since it's also very easy to set up, it's an easy way to start. Locally, you'd have it probably in an instance or something like that, so there’s a little bit more of a setup. But I would say, at least from what I've seen, it would be under that scenario.
Alexey
I would add that Postgres was originally created and is still intended to be used as a transactional database, not as an analytical database, which means that this is for – let's say you have an ecommerce shop and you fill your basket with all your orders. It is for these kinds of queries, like a block or something like this – so when you have some transactions and then it has transactions, consistency, and all that. You would use different databases for analytical workloads, when you want to analyze a lot of data. 
Some of these databases are actually based on Postgres, for example, Redshift, or Greenplum. There are quite a few that are based on Postgres. You don't usually use (you can, but you usually don’t use) plain Postgres for these kinds of things. But for a local setup that we have right now, this is just the easiest way and it can work reasonably well up to quite a lot of databases – quite large databases. I don't think we will actually hit the performance bottleneck of Postgres in this course, because you need to have a lot of data to notice it.",Data Engineering Zoomcamp,2022,
736818,"For soft skills, resources, and career questions, I would recommend you actually go to the DataTalks.Club Slack. There is a channel called Career Questions. Go there and ask your questions there and use the Slido link for the course. This way we can separate the two and during these sessions I can talk about the course and not career, while in Slack, we can talk about careers.",Machine Learning Zoomcamp,2021,
685530,"Yes, I do. There is a script. I will not share it with you now. Maybe after the course finishes.",Machine Learning Zoomcamp,2021,
55591,"Alexey
One thing is that you should not use images unless you really know what you're doing. It probably shouldn't be a super small dataset. It should be more than one megabyte, ideally.
Victoria
Probably more than one file. In the taxis, we have up to four. At least two. I think also, it's important to understand that the course is more focused on the concepts and everything that they're going to use is more focused on building the pipeline and processing that data than the actual presentation. Maybe it's super cool to do an analysis on images, as you were saying before, but in reality, what you need to practice is more on processing that data. That's why it's easier to use something else.
Alexey
Maybe if you come across a dataset and you're not sure if it's a good dataset or not, just ask in Slack and we'll help. I think the list I gave is quite good. It's a good start. Then, of course, if we talk about Kaggle, there are tons of different datasets. Some of them contain images, some of them are tweets, for example. By the way, you can also use tweets. There is some structured information that you can extract from tweets, as well. Maybe this is actually good if, let's say, you want to practice streaming, because you can just get a stream of tweets coming from Twitter and then you can process them on the fly. I think this could also be a good project. Or sometimes you can also parse something. For example, there are websites, like eBay for selling and buying things. You can build a parser for getting the data. This thing could go every day, take the data, do something with this data (put this to CSV, for example) and then put this to a data warehouse or do something like this. But don't spend too much time on scraping. I think there are already scrapers available. Maybe if you just Google “eBay scraper,” you will find some code, so don't spend a lot of time on trying to build the scraper yourself.
Victoria
Also something like Twitter, as you suggested, and Instagram as well – they have APIs, which makes that part super easy. If you want something like that and you can find something that already has an API, then that's probably the easiest option. You can also get to practice that, which is also very good.",Data Engineering Zoomcamp,2022,
195145,"Alexey
We have 33 submissions. I think the decision to extend the deadline was a good one. Hopefully, by the end of this week, we will see a higher number. I understand that doing a project is much more difficult than just watching the course and following along. These 33 people – you probably put a lot of effort in, so thanks for doing that. I saw one person write that he put 100 hours already into the project. This is outstanding. Sorry for being so cruel with you. 100 hours is a lot.",Data Engineering Zoomcamp,2022,
857258,"Yes. When you run here [image 3], I always forget whether the first one is a host machine or container. Let me open the Docker file. [image 4] Here we specify the port on the container, in the Docker file. Then you need to map this port to your host machine and this is where you do this. This is where the port changes would be reflected. It's up to you to decide which port you want to use.",Machine Learning Zoomcamp,2021,"772791_qa4_ml_zoomcamp_office_hours__week_6_image_4.png,772791_qa4_ml_zoomcamp_office_hours__week_6_image_3.png"
119630,"It is not because it still uses the target information. When we fit our KNN, we take the entire dataset that we have and put it inside our model – and it contains labels. So it is supervised, because when we look up the price for a house, we see what houses are around and then we look at their prices. In this sense, this is supervised learning because we look at similar objects to make our decision on what the price should be.",Machine Learning Zoomcamp,2021,
560772,"Yes, use linear models. They’re usually quite good for large datasets – better than, let's say, tree-based models. Another tip is using this SGD classifier (stochastic gradient descent classifier) which is also for linear models. It is faster than the usual logistic regression because it can look at a part of data when training – it doesn't need to have the entire dataset in memory. For large datasets, you can use this or you can just go to Amazon and rent a bigger machine and just train XGBoost there. This will probably be better than trying to train a linear model on your computer. Also, you can do sampling. Let's say if you have a pretty large dataset, you can take a sample of 10,000 rows and it should work reasonably well. Again, it's case-dependent.",Machine Learning Zoomcamp,2021,
365812,"Alexey
We want to keep it two weeks in order to... if we give you more, then you can get too ambitious. Then you might decide, “Okay, I will use Pulsar. I will use Prefect. I will use Azure,” and you might go crazy with it all and it might not work out well. So we don't want you to be too ambitious. We want you to select something simple, and then have an end-to-end workflow – something that you can do in two weeks. That's why we purposely made it only two weeks and not more, because if we will give you more time... so, just come up with something simple.
Ankush
The idea is not to have a full-fledged production-ready system. Keep it simple, but also use all the technologies and see how the grades are being given, so you can then develop maybe a bit more sophisticated solution. But I think it should be reasonable in two weeks.
Alexey
Yeah. And, of course, if there are any problems, or if you're struggling with something, we have Slack. You can ask your questions there and people are very helpful in our Slack. Usually, when I check the channel, I see that most of the questions are already answered. This is really amazing. Thanks for your guys' help there. I'm pretty sure if you get stuck and if you ask a question there, people will help you, and we'll also try to help you, of course. So don't be afraid to ask for help and don't get too ambitious. Just try to do something simple. Keep it simple and you will do fine in two weeks.",Data Engineering Zoomcamp,2022,
505463,"Ankush
Absolutely. I think this course is meant to give you all the knowledge required to be on the job. We are going to talk about analytical engineering. We are going to talk about streaming. We're going to talk about Spark. Maybe not all of it will be useful for your job when you start it, but it will eventually be very important as you progress in your career. It will definitely help you land your first job and it will definitely also help you progress in your career if you are currently already an engineer.",Data Engineering Zoomcamp,2022,
782118,"Yes. It's very related to ridge regression. There is a mathematical proof that shows that it's exactly the same. It's a bit hairy. There are a lot of derivations that you need to go through to show that they are equivalent. But yeah, they're the same. Lasso is a bit different. I don't know if I should go into detail now. It's a different way of regularizing. One of the key features of lasso is that it forces some of the weights for your model to be zero. If a feature is not important, it makes it zero. But for ridge regression, it just makes it very small. In short, they are very related.",Machine Learning Zoomcamp,2021,
110700,"Alexey
For that, you probably haven't watched the one hour and twenty minute video, but spoiler – this question is covered there. I do not remember the exact time in the video. It’s somewhere at the beginning. Maybe you can skip around until you see it. You can take a quick look and then you don't have to watch the entire video. I know it's quite long and probably daunting when you see that the video is an hour and twenty minutes long. [chuckles] Sorry about that.
Sejal
I would just like to add to that – not specifically to execution_date, but in terms of the announcement on the best practices videos. Alexey and I just discussed that I think rather than creating a separate video for best practices, it would be best if we merge some things together. I prepared more of a text-based version with the time codes – Airflow/Postgres video that Alexey has prepared. 
These would provide reference links to what concepts Alexey has explained including [unintelligible] and also using Airflow configuration variables, such as execution dates, and also backfilling and so on and so forth. I'll be sharing that by today or tomorrow. Stay tuned for that.",Data Engineering Zoomcamp,2022,
564469,"When you deploy your model – let's say this model is deployed as a web service, meaning that if we have a phone (this is an old iPhone with a button) So you have some details about the listing of a car, like title, model, make and so on, and you want to predict what the price of a car, right? This phone sends a request to some web service and then the idea is that, eventually, this request ends up in a model. And that model replies. It gets all the information about the car, like model, make, age – all these things – and then it replies back with predictions, “this car costs that much”. This is what we recommend to the user. Let's say $50K. This model – we say that it's deployed to production, because the client can talk to this model (can communicate with this model) using a web service. And they, let's say, use HTTPS to communicate with this model. This is the deployment. [image for reference] 
Monitoring the quality means – let's say we deployed the model, and then one year after that, a car that one year ago cost $50K now probably costs $40K. Cars change, new cars appear, cars that were expensive some time ago become less expensive, maybe some cars become more expensive – so things change. This is what we need to be able to detect – these changes. This is what we call model monitoring. We need to see if things that we call changes in distributions or “drifts in distributions”. If these things change, we need to be able to detect it, and let's say, retrain a model. 
Scalability is – let's say, that now you don’t have just one phone, but you have millions of them. One simple web service cannot handle that much load. So what you need to do is deploy not just one instance of a web service, but let's say, 100. 100 together can deal with 1 million requests at the same time, but one single one cannot. This is what I mean by scalability. We can scale our web service, add more instances of our model there to be able to process all this traffic. Maintainability here means – let's say, there is a bug in this model and want to fix this bug. How easy is it for us to fix this bug? What do we need to do to debug the model? And so on. This is maintainability. How easy it is for us to move around the code base of this web service. Usually, if you follow best engineering practices, then it's easier. This is not something we will cover in this course. For that you probably need a course on software engineering practices. 
Actually, I talked about model monitoring, but there is also simple monitoring, like “How many requests per second is this model getting?” Things like this. For that, we also need to set up monitoring.",Machine Learning Zoomcamp,2021,539917_qa6_05.jpg
853241,"Alexey
Victoria, do you know any good streaming datasets? 
Victoria
I've only used Twitter. 
Alexey
I think you can get some data from stock exchanges in real time. But I don't know whether it's free or if you have to pay for this. Maybe there are some exchanges that can give you this data for free.
Victoria
Yeah, they are a few that give you the data for free, now that you mention it. I think there’s also one from the US that has sports news. I couldn't remember the name now, but they also have an API. That could also be a good streaming source. But it's news. It might have something structured, but…
Alexey
Yeah, I think if you just Google some public data streaming datasets, that could look promising. Coinbase, Twitter. Here’s a good list.",Data Engineering Zoomcamp,2022,
775298,"Victoria
Same as we've already said, you don't need to know everything. No one knows everything. There's a point. At the beginning, everything is a bit difficult. Everyone has a lot of problems with Docker and BigQuery and stuff like that. But you also get to a point where learning everything starts getting a bit easier. For me, for example, I worked several years with Microsoft SQL Server and then I changed jobs, and they used Redshift. I learned it very quickly because I knew the basics. Then we migrated to Snowflake and, again, it was very easy. The basics are always the same – a data warehouse is a data warehouse, SQL is SQL, Python is Python, and ETL processes are ETL processes. So don't focus that much on the tool. 
As long as you know one well and you understand the concept behind it and how everything works and things like that, you'll be able to work there. If you're able to integrate, again, if you know how to do an ETL process, for example, then you will also be okay to integrate, regardless of the tool that you'll use at the top. I also don't believe, or at least I wouldn't do it – if I were hiring someone and the person doesn't happen to know the exact data warehouse (for example, we use Snowflake) I won't ask very specific things for Snowflake. And if they do know Snowflake and they've been certified or whatever in Snowflake – that's a very big plus, but that's not something where I would reject someone because they don't have it. If you understand the data warehousing concepts, you're good. 
Alexey
Yeah, it's like any other technology. Let's say we use Airflow, but somebody hasn't used Airflow before, and they used Luigi instead, comes to us – it's not like we're going to reject them just because they didn't have experience with Airflow. These concepts, I think, are transferable. The same with cloud. Let's say somebody has experience with GCP, but we use AWS. Again, if the person has experience with cloud, it's enough because I think many things are transferable from one cloud to another. 
Of course, there are specifics – there are some particular things that are specific to each cloud, but most of the concepts are pretty transferable. Take any technology that we covered in the course, there are many alternatives. If you know one of these alternatives, you are good. Ideally, as long as you can build a data pipeline and you know all these concepts that we covered and you know at least one technology from all of these concepts, that's already quite good.",Data Engineering Zoomcamp,2022,
481851,"Yes, from time to time, I do use my software engineering skills. Mostly I now spend my day in meetings. I don't do a lot of hands-on work. A few years ago, I would just code all day long, but now I do a lot of meetings as well. These days, I don't leverage my past software engineering experience as much as I did before. But yeah, there are always cases. Let's say I need to deploy something. So this is very useful, because in my opinion, data science is still an engineering thing – you need to write a lot of code. Having a background in software engineering and in programming is very helpful.",Machine Learning Zoomcamp,2021,
171982,"Ankush
I just use QuickTime on my Mac, and then I use iMovie to edit it. That's why you don't see my face. [chuckles] But I like your tool more, actually. 
Alexey
What I use is called Loom. I can just press a button right now. And now you can see that it starts recording. Now, what I am saying is being recorded. Then I press another button and then it just creates this thing that I can share. That's really cool. This is very convenient. I really love this tool. 
Ankush
How do you edit it?
Alexey
There's a download button in Loom. I download it and I use Kdenlive. I actually finished editing a video for a different course – for ML Zoomcamp. There was one video that I kept putting off. So now it's done and I used Kdenlive. The KDE in the name comes from Linux, but it works surprisingly well on Windows as well. I use this on Linux and when I moved to Windows, it also works here. It actually works even better than on Linux. I find this surprising. Of course, it's free and open source.
Ankush
Oh, really? That's impressive.
Alexey
Loom is not free. It costs like $5 per month or something like this. I don't only use it for the courses, but for other things as well. There's a free tool.
Ankush
What other things?
Alexey
Well... for recording documentation, let's say. [chuckles]
Ankush
[chuckles] Oh, so you're recording documentation and putting it in OLX?
Alexey
Not for OLX. For DataTalks.Club. For documenting processes. The other tool I use for recording, which is also free, is OBS. There is a virtual camera, so you can start that and, for example, do that. Here I can use this to just start recording, and then it can capture the screen and then save it to a file. Victoria also uses OBS for recording videos.
Ankush
I think the best thing I like about Loom is that you have feedback, right? In the video, you are talking and you have a face there. 
Alexey
Yeah, the face. And it's cool that it's shareable immediately. So you can just record and then, “Okay, here's the homework solution.” And then the next day, after sleeping, I have to download, upload it to YouTube. So I don't need to do that. I can just share this file. Well, of course it's unedited. So then I can also edit this and then upload to YouTube, but it's shareable immediately. This is pretty cool. I don't know what Sejal used. Do you know what she uses for recording? I know that for a couple of videos we recorded in Zoom. We just had a call and recorded it, but I don't know what she used for other videos.",Data Engineering Zoomcamp,2022,
64848,"Ankush
Yes. Yes, you can. But I think this is a transfer service, right? So yes, you can. But the only question is that the data which we are using is on AWS. If you want to run and test around with some other data source, then feel free to do this. Also leave a comment, maybe, so we even know how it goes with Azure blob storage.",Data Engineering Zoomcamp,2022,
289652,"Alexey
Yes, I think so. I don't think I can draw anywhere. Let's say you have one machine with Spark, and you have another machine with your Airflow. The machine with Airflow needs to be able to send the request to Spark, to do Spark submit. They need to live in the same network. If this is the case, you can do a Sparks submit and then save the URL or IP address of that Spark computer, that's enough. One thing that you might need to do though is  that you will probably need to install Spark to Airflow in order to be able to do this. 
Maybe Sejal will correct me, but I think you will need to do this because you will need to have Java, you will need to have this Spark submit script in order to be able to actually send the jar over the network to the Spark master. So you will need to modify your Airflow container – if you used the Docker images from the course you will need to modify them, you will need to install Java, and you will need to install Spark. But when you do Spark submit, you just say “--master <master url>” and you specify the master of that virtual machine with Spark. This is how I would do it. Actually, instead of running Spark on the virtual machine somewhere, I would use Dataproc. It is relatively simple to run. You just need to click a few buttons and wait for five minutes when it creates a cluster, and then you have a cluster. So it's pretty convenient.",Data Engineering Zoomcamp,2022,
406643,"Alexey
I can try. First, there is a transfer service, which you can use if you have an AWS account. The reason you need an AWS account is because it asks you to enter your secret access keys. Basically, even though the data is publicly available, you still need to do this. There is also an alternative, which I think is actually something that Sejal shared. This is the script that you can use. You just run it. Preferably you run it in a cloud virtual machine, then it will be quite fast. You just uncomment what you need. So let's say if you need to have green data for 2019, you run it. And if you need yellow data, uncomment it. What this script does is, it downloads the file, saves it to CSV, and then it also saves it to parquet. And then I think what it's doing is uploaded in the parquet file. If you need to upload the CSV, then you can probably replace it somehow. Yeah, probably comment in the parquet part. So, this is how you can do it without Airflow. Probably the easiest is transfer service.
Victoria
I also did something similar, a script from Sejal. And then I chose maybe not the best way, but I just drag and drop all of the CSV to the backup. And then from a backup, you just do this like Ankush explained on the first video out of the three videos. Also have it.
Alexey
So you download the data with a script and then you just select all, drag and drop to Google Cloud Storage and that's it. That's smart.
Victoria
Not the best way. But I didn't have a data engineer. So I gotta get that thing up somehow. [laughs] But yeah, I guess the question is very much targeted to what we're doing. But in general, there are so many tools that you could use as well. I believe I mentioned Fivetran and Stitch. But, in general, I guess any other tool like that could be used.
Alexey
But without air flow is what you show in the videos, right Ankush? Insert into something and then you do select to start from an external table? And then this inserts data.
Ankush
There is also a way to directly upload it to BigQuery. But I just chose the external table because that's easy.",Data Engineering Zoomcamp,2022,
993839,"If you have an issue like this – if you see that the code is copied. I didn't explicitly say that it's not allowed, because I assumed it's kind of clear that it's not okay to just copy code and say it's your own. But if you see something like this, and you see that only a part of the project is copied. For example, the part that is about training the model, but not productionizing it, then you just give zero points to the part that is copied – the model training, for example. But for the rest, you use the criteria from the table. For the next project, I will have it explicitly saying that any copying will not be tolerated and will result in zero points. So if you see that somebody copied their work, then you just put zero everywhere. Not for this one – for the next project. Also, speaking of the next project, I think it's actually a good idea to start thinking about it right now so that later you already know how projects look and what is expected. The capstone project will be very similar to this one. You can already start thinking about what you can do for the capstone project as well.",Machine Learning Zoomcamp,2021,
964992,"Alexey
246, which is 150 less than last week. Last week, it was 404. I don’t know if this homework was maybe a bit more difficult. But the next one is also fun. I think if you liked the homework we prepared for this week, the next one will also be fun. Right, Dmitry? 
Dmitry
Yeah, for sure.",Machine Learning Zoomcamp,2021,
761141,"Alexey
Please don't feel dumb. Do not feel left behind or dumb. You can always ask questions in the chat and in the channel. You can also look things up. I don't know what the state of our list of frequently asked questions is. I think it was abundant. Maybe we can put some life there. If you have a question, we still have 10K history in our Slack. So if you have a question, look it up in Slack, and then if you find the answer, please put it into frequently asked questions. This way it will be preserved. Of course, please don't feel dumb because everyone has a different amount of time that they can dedicate to the course. Everyone has different backgrounds. For somebody, maybe it's the first time they’re using Docker. And for others, they've been using Docker for the last five years. Of course, for the second group of people it's easy, but it doesn't mean that it should be easy for you.",Data Engineering Zoomcamp,2022,
439850,"Yes, there will be. As I said, we will provide a matrix that will have multiple dimensions, like parameter tuning, data prep, etc. and for each of these dimensions, you will have some criteria saying, for example, “0 = no parameter tuning whatsoever,” then “1 = model tuned,” and “2 = a person tuned multiple models and selected the best one.” Then for parameter tuning, if you tried multiple models and selected the best parameter, you get a score of 2. Whoever is reviewing will just have three options in the form. It will say “parameter tuning,” and then option one, option two, option three, which will have the relevant points attached. They will just click the correct option and this is how you will get your score.",Machine Learning Zoomcamp,2021,
377027,"No, you should not. You should use the validation dataset for hyperparameter tuning. You tune your model using the validation dataset. Then, at the end, you join the training and validation dataset to train your final model. Do not tune parameters on the test dataset at all. You only use the dataset occasionally, very infrequently – only to double check that you don't accidentally overfit. When you have your final model, then you use it preferably only once. So you try to not look at this at all. For hyperparameter tuning, you use the validation dataset",Machine Learning Zoomcamp,2021,
504339,"Victoria
I think that's ne that someone already shared in Slack. I've seen it in the past and I really like it. But I don't know – I'm not a data engineer. But I think it's very complete. If you go to the website, they actually have a link to all of these resources to learn. I wouldn't be able to talk about all of the content in this roadmap. There's some content that I'm not super familiar with. But I think it's very complete.
Alexey
I think this is a good note here: “Beginners shouldn’t feel overwhelmed by the vast number of tools and frameworks listed here. A typical data engineer would master a subset of these tools throughout several years depending on his/her company and career choices.” When you see a roadmap like this, don't feel overwhelmed. I think learning these things will take a couple of years, right? [Victoria agrees] It feels like a university curriculum here in one document. Computer science fundamentals will take you one year to master that. 
Victoria
That's the thing, they have courses. Maybe this is not the one I saw, but they have several courses. This is also why they have these icons. But I remember having seen that they actually had links.
Alexey
There are actually multiple roadmaps like this. Maybe this is a different one from what you saw. For example, I would go a little bit lean, if I can say that – maybe you don't need all the data structures immediately. You can just maybe pick the most important thing here, which would be basic terminal usage, and then a bit of Linux, then Git as well, then Python here, then SQL here. So maybe you pick one thing from each, you go through the entire thing and then you repeat. 
Victoria
So the green check marks are general things, the hearts are personal, and the cloud icons are cloud-based.
Alexey
It’s a nice roadmap, but there are too many things. I can see why it can be overwhelming. For example, Jenkins – I don't know. I think companies still use Jenkins, right?
Victoria
Yes. I haven't seen it, but I think they do.
Alexey
We use it for legacy, and legacy is such a thing that stays legacy for very long. Somebody needs to support it. If you're into these kinds of things, Jenkins will be useful. We also use GitLab CI/CD for the CI/CD section, which is not here. But I think it's very cool. Terraform is here.
Victoria
For example, CI/CD, I would say it's good to know the concepts. But it's not something that I would focus on in the very first year. It's good to know what it is, but it's something that you would only do once you have a more stable project or something like that. Even in a project, not even in the knowledge. Then you can pick it up.
Alexey  
It looks very nice – the picture. It’s high quality. But to my taste, there's too much stuff.
Victoria 
I'm going to look for the one I was mentioning and I'm going to share it if I find it. It had a bunch of different courses and it had information on how difficult they were and things like that. It’s https://awesomedataengineering.com/. They also have a GitHub repo. This is actually the one I based my repo on – the one I talked to you about for analytics engineering. I think it's more linear and it doesn't have that much. But it has a lot of things. What I find nice is that it defines the coverage, the depth, and also whether it's free, or if you have to pay – so you can focus on only the free resources.
Alexey
And then it tells you not only what you need to study, but also what you need to read to pick the topic, which might be more useful.
Victoria
I find this one more useful. The first one is pretty nice, but it's maybe too much, as you said, and also probably more oriented to their courses.
Alexey
I see that they suggest covering workflow management as one of the last topics, which I am not sure if it’s the right place for that. I would have put it a lot earlier. We actually did that. [chuckles] But yeah, it looks like a good map. Oh, you can even click here and then just get all the free resources. 
Victoria
Or only see the books or only see the courses, if you want to do courses. It’s quite nice. They even have something about the best books – they only have three, but there was Designing Data Intensive Applications and then, the Data Warehouse Toolkit, which is the one that I use for as a refresher or a review of data modeling concepts. Seven Databases in Seven Weeks?
Alexey
Data Warehouse Toolkit is a classic book. It's pretty old. I recognize the Kimball name. This person created a lot of stuff. 
Victoria
It's one of the fathers of the data warehouse along with Umair Imam. I think the first version was in the 80s.",Data Engineering Zoomcamp,2022,
412893,"Alexey
I'm curious to know what exactly your difficulties are. Is it not knowing what to do next or not knowing where to start? But maybe I’ll take a stab at that. First of all, you go to our GitHub repo, and then in GitHub repo, we have things structured in weeks. You click on week 1, for example, if you want to learn the materials from week 1. Week 2 is what we're preparing right now, so it will look different once we finish it. It will look like week 1 – it will be more concise. But to know about the details, you will need to go there. 
Here, we have everything you need, starting from the introduction, then all the code about Docker, and then GCP, Terraform and then setting up the environment. Actually, for the environment setup, if you didn't have any problems installing Docker, installing Terraform, or installing Google Cloud, then you can just ignore this thing. But if for some reason, for example, you have trouble running Postgres in Docker, you can check out this video. It's a step-by-step video that shows how you can set up an environment using a virtual machine on Google Cloud. This video is more like a bonus, and you can watch it at any point. 
You should follow the order that you see on the page. In the playlist, we actually have the Google Cloud intro first. I don't think it actually matters because this introduction to Google Cloud Platform is a very short video. It doesn't matter if you watch it before Docker or after. We will try to follow the same format for the other weeks. We'll have multiple sections in each week. For example, in week two, we have three sections, like data lake and orchestration. First there is one video on data lakes and then there is basics about orchestration and then there will be a bunch of videos about Airflow. 
To answer your question – go to our GitHub repo and follow the approach we suggested there. If you have any difficulties, feel free to go to Slack and ask there. If you have difficulties with something in particular, going through the Slack channels and looking through what kind of problems people had, it's very likely that somebody else had the same problem that you have, and there is a solution in one of the threads. 
Since we're talking about that, it's amazing to see how helpful you are to each other. That's having a big heart. Thanks a lot for doing that. I don't know about you, but for me personally, it's very overwhelming to open the Slack channel and see like 50 new messages. I'm very happy to see that you already helped basically resolve all these issues. So a big, big thank you to you all for doing that. That's very helpful for us. I'm not sure how we would be able to answer all these questions. So thanks a lot.
Sejal
Yeah, plus one to what Alexey said. Thank you so much for helping each other out. If you have any further pressing questions, we will definitely answer and we are definitely looking at the threads. It’s just that we won't be constantly available, but we can and we will answer when possible.
Alexey
I also wanted to say that if you need a refresher on GitHub, maybe you can just Google “GitHub tutorial,” or “getting started with GitHub,” and you'll find a lot of amazing resources. I'm not sure we will be able to do better than these available sources.
Ankush
We will be using Git continuously in our videos. Obviously, we would be using the commands. So I hope that might help you in refreshing your GitHub knowledge. But the best way would be to do a small YouTube search on GitHub and I'm pretty sure there will be really good videos out there.
Alexey
If anyone comes across a very nice GitHub tutorial that was very helpful to you please share with the rest of the students.",Data Engineering Zoomcamp,2022,
481526,"Well, it's up to you, honestly, if you want to use cross-validation or not. I think you should but you can also just use the usual train/validation split. With cross-validation, as a bonus you can see how stable your model is – what the standard deviation is. Usually, it's useful. So I would suggest that you do this, but I will not require you to do this. Cross-validation is useful, but there must be some form of validation because without it, you will not be able to select the best model.",Machine Learning Zoomcamp,2021,
984370,"I don't know. I haven't checked that, to be honest. Where I took it – I took the data preparation step – I didn't see any data preparation step for time. So maybe it is a categorical variable.",Machine Learning Zoomcamp,2021,
171761,"Dmitry
Basically, we apply it on all the images, if I understand this question correctly.
Alexey
We get an image when we iterate over our dataset, we load an image, and then we apply a random mutation to this image. Basically, from one image, we create only one new image. But if we go over our dataset 10 times, then the network sees 10 times a slightly different variation of the original image. I guess this is how we can configure it – by specifying how many epochs (how many times you want to go over our dataset). That could be it, I guess. [Dmitry agrees]",Machine Learning Zoomcamp,2021,
393128,"I use Jupyter extensively – I use Jupyter a lot at work. All these initial experiments, like all this playing with different models, or doing exploratory data analysis, or cleaning the dataset – I use Jupyter for that a lot. The reason for that is because you have this interactive environment. You type something and you immediately get an answer. But if you do this from a script, I write something there, and then I need to run the script and see what changes. I think the feedback loop is a lot longer when I use scripts (when I don't use Jupyter). That's why I really love Jupyter. I come from a software development background, so for me, the idea of writing code in a web browser was alien until I got used to it. Now I cannot imagine my life (my work, at least) without using Jupyter. I think this is super convenient.",Machine Learning Zoomcamp,2021,
151848,"Alexey
Well, by documenting your project. Just make sure you write everything out, that you show how to run the thing, by creating instructions, etc. Maybe one example could be, let's say, the DE Zoomcamp – if we take here, one of the modules here. I don't think my code here is good. No. Let's go with Docker, for example. There is some documentation that actually shows what you need to do to run this thing. So something like this, maybe a little bit more descriptive. First, describe what the project is about, then how to run these things – what are the things that are needed? What are the accounts you need to create? What are the commands that you need to run? So somebody who's looking at this doesn't just see a blank readme.md file, but there is some information. 
Then if they follow along this information in sequence and execute things, it works. This is what I mean by reproducibility. One tool that I like to make it even easier is Make. You can create a bunch of Make files. We do not cover them in this course, but they're pretty useful. Maybe I can show you an example. Instead of typing long commands, you just do “make all” and then it executes this, this, this and this. So it's a cool thing. But at the very minimum, having this documentation that describes what you need to do with code snippets – this is what we mean by reproducibility. Having this requirements.txt file is also important. Ideally, if you pin versions there so when you write, Python_Kafka==version, this is also good. 
Ankush
I would also say that if you are using cloud-based, then you can assume that the person who's reviewing it would also have the same cloud. So Google Cloud Platform would be available with that person, and then you can use Terraform. So if you're creating buckets, then use Terraform. Then maybe in the readme, you can say, “The first step would be to create this bucket. You create this bucket using this Terraform file and that's how you run it.” For that, the person would need to do the Google Cloud authentication. The same way you did Google Cloud authentication, you can also highlight that in the documentation, “The first step would be to do the Google Cloud authentication,” and so on. Maybe you can use the same criteria (the cloud provider is the same) and then you proceed from there.
Alexey
Yeah, please don't forget these steps, like authentication. Because if you assume that they are authenticated when they run, they might not be and there will be an error.",Data Engineering Zoomcamp,2022,
516218,"MAE – we will not talk about this in great detail. MAE and RMSE are both metrics for evaluating the quality of regression models. In the course, we use RMSE, and MAE is an alternative one. For RMSE, the formula is here. For MAE, the formula is a little bit different. Instead of looking at squared error, we look at absolute error. Absolute means that we use bars. This is a formula. So they're a little bit different. RMSE penalizes very big errors. Let's say if our prediction is 0.1, but the actual prediction is 100. RMSE will get a higher penalty because it's 99.9 squared. This will be a huge, huge penalty. While for MAE, it will be just 99.9. There are pros and cons – you can just use both. So you don't have to choose one. You can always report both numbers.",Machine Learning Zoomcamp,2021,
827511,"The answer is – it depends on the dataset. What you can do is – we typically have this train/validation split. First, you try with scaling, then you try without scaling, and then you see what happens. I think this is something we talked about last week. Actually, there is a notebook that I prepared for last week. We can go to classification, and the notebook is this notebook scaling. It shows you how you can scale your features. You can use a standard scaler, you can use Min Max Scaler – you can use these two different scalars and it shows you how you can do this. And for this particular dataset, it turned out to be a little bit better. 
You can just experiment and see if scaling makes sense or not. For linear models, sometimes it's actually a good idea to do this, but you add extra complexity to your pipeline. So instead of just doing this Dictionary Vectorizer and whatnot, now you'll also need to add the next step of scaling. Basically, now you have more steps in your prediction pipeline and it becomes a bit more difficult to maintain. So I would say that you should try to scale if you see that the improvement you get is significant, because maybe it's not worth the extra complexity that you get. But, again, use cross-validation, use your best judgment, and see what makes sense.",Machine Learning Zoomcamp,2021,
116379,"Ankush
I think we will try to keep the project to a minimum. You will definitely need to spend time, but it’s not like there will be a deadline of one week. We would be giving a deadline of two weeks. And if the community wants an extension, there is a possibility of that as well. We do not encourage you to take time off from work just to do the project. But definitely spending time on it would help you to advance your career.
Sejal
To add to that, this is a self paced course, because we are uploading the recordings of the videos that we are taking. It's not a live course. So please take your own time based on your capacity and available resources in terms of time and effort. You don't really have to squeeze all the things in your timeline to make this work.
Alexey
Of course, everyone will have the same deadlines for the project to make it possible to actually do peer reviewing. But if you're behind with videos, you should still do a project and then you can just ask for feedback about this project – you can just post a link in our Slack channel and then say “This is the project. Can somebody please take a look and evaluate this?” And I'm sure somebody will be glad to do this and learn from that.",Data Engineering Zoomcamp,2022,
606126,"It means a published one, but you can publish it on Medium, for example, or your own blog or whatever. LinkedIn posts – I don't think you can put a nice, long article in a LinkedIn post. But I think there are LinkedIn articles. They don't have code – you cannot embed code there. So I wouldn't recommend using LinkedIn articles. Medium is good. If you have your own blog, then use your own blog.",Machine Learning Zoomcamp,2021,
611684,"Alexey
Is the question about the Data Engineering Zoomcamp or a Zoomcamp? Because for a Zoomcamp, we want to do an MLOps Zoomcamp in the near future (in a month or two, probably). But when it comes to a Data Engineering Zoomcamp, we still need to talk about that. Probably next year. If it happens, then next year. 
Sejal
For the Data Engineering Zoomcamp, there's a possibility that we may just revise some of the recordings of the videos. For example, the Airflow setup, which was a little time-consuming and also a little effortful on the compute resources side. Maybe we might just find an alternative to some setup instructions so that it works smoothly. Currently, the plans are just to refactor the existing Zoomcamp so that it makes it easy for the next iterations. We would otherwise just release a project-based iteration. I think the recordings would still remain the same. There could be a new project cycle.
Alexey
For that, for what you're talking about, we finished the project right now, and then there will be another iteration for projects. So if for some reason, you weren't able to work on the project right now, there will be another iteration immediately after that. Let's say we finish reviewing the week after the next, then immediately after that, we'll start another cohort for projects.",Data Engineering Zoomcamp,2022,
744057,"Sejal
I'm surprised that you weren't able to connect the dots on this one, because the infrastructure that we created for GCP – which is the GCS bucket, as well as the BigQuery dataset – was created by Terraform. If you did run the GCS DAG (the Airflow DAG) to upload the data to GCS, and then view the GCS data in the GCS external table, this is where your infrastructure that was created from the previous week is used. I don't know if you followed through the videos and understood the concept behind these things. 
But if you've skipped the Terraform and GCP part and went for the local installation and local setup to use Airflow and Docker with Postgres only, this is also fine (if you're running out of time). These are the two journeys we've prepared. One is a cloud-based journey and the other one is a local setup. For the users who are unable to install GCP, or unable to access GCP, you have the local setup. So if you're going for a Postgres-based operation, that is also completely fine.
Alexey  
I would add that Terraform is a useful thing anyway, regardless of whether we use it for week 2 or not. Knowing it is quite helpful because I am sure that as a data engineer you will have to use it at some point.
Ankush
Exactly.  I think that was my point that we added Terraform on the basis that you will learn it eventually. The idea was that we introduce it to you at the level that you can get comfortable with it. And you can then extend your knowledge on top of that, especially depending upon your role in the company.
Alexey
Will we use Terraform in other weeks? I think so, right? I don't know about Spark – I still need to figure out how exactly it works with Google Cloud. There's probably a managed Spark, like in AWS. That could be created through Terraform as well.
Sejal
In BigQuery, tables can be created by Terraform or by Airflow operators. We will disclose the information on that in the next few days.
Ankush
We are already using it in week 2 in the transfer service. So we are doing something manually, but those manual steps are just to make it easy for you. If you do it in a company, or in production systems, you should always use Terraform to set up something like a transfer service.",Data Engineering Zoomcamp,2022,
249148,"We didn't use the test dataset for this homework. But, actually, you used that when you trained your model. You did all this validation, you found the best parameters, you found the best set of features that you want to use, or you found the best model (you compared multiple models perhaps). Then what you do is combine your train and validation datasets, train one bigger model, and then you check it on the test dataset to make sure that everything worked out. Check the lectures – I think during every week, starting from the first to the third week, we covered that. So just make sure to check that out.",Machine Learning Zoomcamp,2021,
551167,"In the lectures, we used Dictionary Vectorizer. This is also a transformer, like min-max normalization. If you're talking about min-max normalization as a transformer from SciKit Learn, it's called min max scaler. Basically, there is a method to transform. First there is a method called fit, which you use to learn this x min and x max, and so on. Then there is a step called transform, which you invoke in the same way as you invoke Dictionary Vectorizer. If you're not using this scaler, I suggest you use it. It just makes things simpler. You fit the transformer when you fit your model and then you transform the data when you apply your model.",Machine Learning Zoomcamp,2021,
360539,"I will, eventually. I want to do some analysis to see how much time people spent on each homework. I also want to do a breakdown because I know that people who registered – when you sign up for the course, you say if you're a data analyst, or data engineer, or a student. I also want to see this breakdown per role – how much time it took for, let's say, software engineers. I think this week wasn't too difficult for engineers, but it was probably difficult for students, perhaps. Maybe other weeks were the other way around. I want to do these analytics at some point. Now, I still have to finish editing the videos for week six, so I will do that first. Then I'll need to prepare some guidelines regarding the projects. Only then maybe I'll do these analytics.",Machine Learning Zoomcamp,2021,
925030,"It really depends. Actually, I don't think short codes are better than long codes, because long codes are usually more verbose. For example, if you look at what I wrote here [image for reference] – this is terrible code. Don't write like this. Maybe it makes sense to first select the subset, and select a subset of columns, and then drop duplicates – so actually split it into multiple rows. Because here, [image for reference] you can look at this and immediately understand what's going on. While in the previous solution, it was just one line with a lot of stuff happening and it was very difficult to understand, even though it was more concise (the second one is more verbose). 
I personally think that the latter way of writing code is better, simply because you can just look at this and it's easier for you to understand what's happening. I don't think your colleagues will like you if you only produce very concise code. For your question about production code – it’s just code that is running in production. [chuckles] This is a recursive definition. But if the code that you wrote affects customers, then it's production code. If this code is just running on your laptop and nobody can run it, then it's not production code.",Machine Learning Zoomcamp,2021,"68711_qa6_03.jpg,68711_qa6_04.jpg"
646383,"Ankush
I would pick up a big dataset for a project with Spark, just to see how you can scale and how you can work with a 100 nodes in one go. You’ll most likely spend all your credits, but still. You can scale massively with Spark. If you choose any project, I do not have any suggestion directly for a project. But I would suggest you go for at least a minimum of 200-300 GB, maybe even one terabyte of data. Then you will see the power of Spark shining. You can actually do the comparison between this and maybe a normal script, which does it sequentially, you will actually see the power of something that is distributed and can handle these kinds of features.
Alexey
That's a good thing to try to scale. But I'm not sure you can actually have the scale in your project environment that you would have at work. That would be pretty difficult, I think. Maybe you can also start small – by “small” I don't mean the Titanic dataset or Iris dataset. You need something that is preferably larger than one kilobyte – at least a couple of gigs – and then play with that. The taxi dataset we have, I think it's still quite small for Spark. You need something bigger to actually feel it – to see that you need to tune the garbage collector and all these things. But I do have a suggestion. It will not be close to what you will need to deal with at work, but look for ad click datasets. In advertisements, there is usually a lot of data. For example, in Kaggle, there were a couple of click-through rate prediction competitions with pretty large datasets. I don't know if this one is actually large. Yeah, it's not that large. There are larger ones. You can also look for datasets from Criteo – for example, this 1 TB click logs. There are similar ones. Look for ad click datasets and you will find a lot of relatively big datasets. They are also not that difficult to understand. There is some information about a click, about the device from which the click comes, so you can roughly understand what's happening there. Another dataset that probably could be challenging, but it's called common crawl. This is basically a copy of the internet. They crawl through the internet and they save everything to S3. Get started. Even if you just take a couple of parts of this dataset, let's say for January, it will be enough to play with this data. It's difficult because it contains natural text and it contains a lot of pornography, because this is a copy of the internet, so you will need to be very careful with how exactly you process this. Because it's the internet as it is – just a copy. This is also a good dataset to play around with, not necessarily for Spark, but in general, if you want to try processing large amounts of data. Also, there was a good list of suggestions in Slack. Check it out and you'll probably find good datasets. Another maybe complex example – there are the graph datasets SNAP Stanford. You can find good big graphs here that could also be quite interesting to process, especially in Spark. Because in Spark, there is this GraphX thing. I think it only works for Scala, though. I don't know if it works for Python. But if you want to experiment with this library, I haven't found anything better for processing graphs than this library. This also could be a nice project. 
Sejal
I think there is also Spark ML, right?
Alexey
As a data scientist, I would not recommend using it. [chuckles] But maybe for something quick. I think the reason it exists is just for marketing, to say, “Hey, we have this.” And I don't think people put enough effort there.
Ankush  
It's not powerful enough.
Alexey
Models from Spark ML don't really work well. What we usually do in data science is use Spark for preparing the dataset and then we get a beefy machine to just load the data and use a simple Python script for training the model. So we still use Spark, but we just materialize the data that we prepared with Spark, save it to S3 and then the script would load the data and train a model on the prepared dataset.
Ankush
And hopefully by that time, the data is already smaller in size. You just use the important information. 
Alexey
Yeah, it's already prepared. It's in parquet, for example, then it's fast to download and load.
Sejal
Do you also use Spark for training?
Alexey
No, not for training. We use it for preparing data and we use a Python script that does not have Spark. Then for applying the model, we usually do use Spark. We just do map partition. There is an example that I still need to finish. This is the last video. I still have to process two videos, one about RDDS, map filter, and reduce. And then the other one is about map partition. Map partition is a quite a useful operation and I use it quite often for applying machine learning models. Give me some time, and I will finish these videos, and you will be able to see them.
Sejal
Nice. I think it will be helpful for me as well. [chuckles]
Alexey
I can give you a raw unedited version. It's very bad because I get interrupted, or I forget what to say. It needs editing.",Data Engineering Zoomcamp,2022,
162388,"Alexey
Right now what we have in mind is two weeks, but it depends on how it goes. This is the first time we’re running this course and we will just have to see how much time it takes for you. We don't want you to spend more than two weeks on that. We want to get something small. So don't get too ambitious, please. Don't process all common crawl data for your project. Ideally two weeks, but let's see.",Data Engineering Zoomcamp,2022,
989686,"Dmitry
It’s quite a strange question, I think. I can interpret it in a way they're both frameworks. Keras is a high-level framework, meaning that you can create the network in, let's say, 10 rows of code. You just have the blocks, like model app, model compile, and then you can create something a Lego style? It's very easy. In PyTorch, it's more of a low-level framework, meaning that most of the things, for example, the layer definitions, all the functions, you need to write on your own. Right now, as far as I heard and checked PyTorch, they are going into the direction to have more predefined functions that the people don't need to rewrite themselves and can just use with minor tweaks. I think this can be equivalent, if I understand the question correctly.
Alexey
I think there is a library. I didn't really use PyTorch before. Well, I ran some things that had PyTorch in it, but I never trained the model myself. I didn't write code in PyTorch, but I heard that there is a thing called PyTorch Lightning and there is also a thing called Catalyst. They're both higher level abstractions on top of PyTorch, which are probably similar to Keras, in a way
Dmitry
If you think about the high-level abstraction, then yeah, for sure – you can use Catalyst. But they're basically two frameworks. The main goal is the same. For sure, you can add something on PyTorch just like a wrapper, basically.
Alexey
Keras historically was also a separate library, but then it was just a move by TensorFlow to get more users.",Machine Learning Zoomcamp,2021,
360993,"I use Windows and this is a surface, this is like a Windows tablet and it's called Drawboard. If you just put it into your favorite search engine, you will find it. It's also available in Windows Store or Microsoft Store (not sure what it’s called). I'm pretty new to Windows. There is some App Store on windows where you can just put this in. It's actually free. 
You can do a lot of cool stuff here. For example, I use only two colors, mostly. But you can use more colors. It's a cool tool, and I have this special pen for it. It’s all Microsoft. Let me again share a part of the screen. Maybe I don't want people to know that I have windows. That's why I'm trading on the part of the screen. But I also don't want people to see the taskbar, for example. It's not useful.",Machine Learning Zoomcamp,2021,
596854,"I don't think I can because it just takes a lot of time. First, I need to record the videos, prepare homework, and do all that. I cannot imagine adding collaboration with startups to that as well. Maybe sometime in the future. I don't know. But if there is somebody from a startup who is listening right now and you want to hire people who graduate from this Zoomcamp, please reach out and let's arrange something.",Machine Learning Zoomcamp,2021,
345132,"Yes. I would say this course is for machine learning engineers, not for data scientists. If you want to work as a machine learning engineer, I wouldn’t suggest skipping the Kubernetes part. I would actually say that this is quite an important part, so you shouldn't skip it as a machine learning engineer. For a data scientist, it's okay to skip it, but I still think it will be very beneficial. It will help you to stand out as a data scientist. 
For example, at OLX, when we open a new position, we make a job posting and just within a couple of days, we get several hundred applications. It's very difficult to select candidates just because there are so many of them. I think if you want to stand out in this kind of volume – among these hundreds of people, it’s likely that very few of them know how to use Kubernetes. 
Therefore, I wouldn’t suggest skipping it. I would suggest that you still do this. But, of course, feel free to do this – just do what you like more and enjoy the process. If you don't like doing Kubernetes, that's fine. I think you're gonna find a job where you don't need to worry about this.",Machine Learning Zoomcamp,2021,
647484,"Victoria
No, because the homework just counts at the end, so there's no need. Ideally, the transformation will happen during this week, so the analytics engineering – right now we're focusing on loading the data. To be fair, I didn't really put that much effort in cleaning this part in the DBT because I focused on building a project that could give you all of the concepts, rather than the actual writing a SQL query that you are already familiar with. That was one of the prerequisites. This is something that you could add to the DBT project, for example.
Alexey
Maybe this is also what we will see how to do in Spark, because I think this cleaning is a little bit easier than in SQL. But it also depends on the type of problems we want to clean. For example, leading/trailing spaces are relatively simple to handle in SQL.
Victoria
Yeah. I did like more of a simpler query, and then just focused on the markers and stuff like that. Not to expect this to clean after week 4, but that's where I would add it.",Data Engineering Zoomcamp,2022,
717187,"You can start working. You don't have to inform me. Imagine if everyone started informing me, I will take a lot of time to answer and I will not finish answering all your questions by the first of November. So, yeah, you don't have to inform me. Please write this in Slack if you're not sure. If you think this is a good dataset, just go ahead and use it.",Machine Learning Zoomcamp,2021,
538859,"Alexey
You don't have to do this through Airflow. If you understand how it works, if you understand how to create a DAG, how to make dependencies – as you noticed, there is some overhead with Airflow and I experienced this today that sometimes it can just break on your computer and not work as expected, which is really annoying. What you can do is just run these things locally from a script and then upload them to GCP using Google CLI or just a web browser, because you will need some of this data the next week. 
If you want to follow along in week 3 the videos and do those queries, you will need this data. For the homework, which I think is more important – you need to have all these “for hire vehicles” data. There is a shortcut. You can just use a transfer service. It's just one click and then you have this data in your Google Cloud Storage. But you need to have an AWS account for that, which is a little bit more annoying. 
You don't need to redo this, but you need to somehow get data to be able to do week 3. We can figure something out in Slack, if you have trouble running this.",Data Engineering Zoomcamp,2022,
481080,"Dmitry
I think it really depends on the use case and the distribution of the data that you have. Also, it depends on what you would like to receive in the end – What is the purpose of your plots? Because there are different types of plots and for different purposes. 
Alexey
Is there any value you always start with?
Dmitry
I think you can start with 10, usually, and then you can go into the direction that’s needed. 
Alexey
I usually start with 50. I don't know why. Just a habit, I guess. 
Dmitry
For me it’s usually 10 or 15.",Machine Learning Zoomcamp,2021,
196096,"It's hard to say. I’m on Windows right now and I remember that I had problems with that. But eventually I sorted these problems out. I don't remember what I did exactly, but there are some tutorials. I saw some links in Slack that show you how to do this. You can always use this Google Cloud Shell or rent an EC2 machine for Docker. That's maybe also an option. Sorry, I wish I could help but it's just not enough information to help you. For me, Windows is not the main system that I use. I usually use Linux. If you remember the tutorial you followed, please share it and there are people in Slack who did this and who managed to make it work. Maybe they will help.",Machine Learning Zoomcamp,2021,
706021,"No reason. If you like FastAPI, go for it.",Machine Learning Zoomcamp,2021,
986368,"Yes, please submit your incomplete project. Actually, what I didn't mention is, 15 points is the max amount – but to pass paths you will probably need something like 10 points or 7. I will actually have to take a look at how many points we get to see what kind of cutoff makes sense, in order to pass the project. But you don't have to score all twos for every metric to pass the project. For example, if you have some problems running Docker, then you will not get two points for that – you will get 1 and the world will not stop. You will still learn something. And let's say if you finished at that point (you only did the first part and you didn't deploy) you will get 7 points, for example. You will still learn something and you will still get feedback. And then regarding the passing criteria, I'll just need to take a look at the actual scores at the end, because if nobody gets 15 points then nobody would pass. I hope this doesn't happen, but we’ll see. I want to look at this course first.",Machine Learning Zoomcamp,2021,
397291,"Dmitry
Actually, we applied the transformation, and then removed it from the data frame. That’s if I understand the question correctly. 
Alexey
I don't think it matters whether you first remove it and then apply or the other way around. 
Dmitry
Yeah, the most important part is that it will not be in the future set, because in the end, there will be corrupted results.
Alexey
I think somebody wrote in Slack that they forgot to remove the variable and they spent time figuring out why the model is so good. That's probably the reason why it's so good. [chuckles] Right? That's why I try to remove it from the data frame, so that I don’t accidentally use it.",Machine Learning Zoomcamp,2021,
234334,"You can use Google Colab. Yes, definitely. I don't see a problem why you cannot – with the caveat that for deployment sessions, it will not be possible to use Colab, most likely. I don't know how to use this for that. So you will need to have some sort of local environment or something in the cloud where you have access to the command line and you can execute things like Docker, and you can access Python Interpreter there. Windows should work – Linux or Mac OS is probably the best option. 
But if you use Windows 10 – I’m actually on Windows right now and I can show you this. [image for reference] Here I have the Windows Subsystem for Linux. I'm basically running Linux right here, so I can have all the usual Linux commands, like Git and whatnot. I recommend you to try to do that. Otherwise, you can just use plain Windows as well.",Machine Learning Zoomcamp,2021,
523285,"Alexey
I think this is a question related to the homework – why we didn't create any extra features. I don't know. Dmitry, do you have any ideas why we didn't do this?
Dmitry
Actually, I just thought about simplicity, so to say. My perspective is that we should go step by step. Regarding feature engineering, some percentage will be covered this week (week 3) and in the next week. Therefore, I think it sets a good tempo, so that we can go step by step. But for sure, it's a good question. For sure, we usually need to do that.",Machine Learning Zoomcamp,2021,
493731,"Alexey
It will be issued on a single day because this process is quite manual. Of course, I will not have to type your name for every certificate and then save it. It's semi-automated. But I will still need to come up with a CSV file with all the names, emails, and so on. Even before that, I will need to come up with a design, then I have a script that we can just run and then upload this. I actually have a website where things will be published. So your certificate will be there. There is a script for creating a certificate. It will not be automatic. There is still some manual work. That's why it will be on a single day, after a specific deadline. A specific deadline in this case means that when we have the CSV file, when we have a design for the certificate, and when we hit enter in a command line.",Data Engineering Zoomcamp,2022,
548146,"Ideally, as I said, you take your dataset – you leave the test dataset aside and don't touch it at all until you have your final model. So you don't do EDA on your test dataset. You can do this on train, you can do it on full train – up to you. Just don't do this on the training dataset, because you can accidentally see a pattern there and try to use it, and maybe build a feature around this pattern. Meanwhile, this pattern might not be true in the general case. So try not to do this – it's called data snooping. When you look at the data, you might accidentally see something and this something may seem very important, and you just accidentally overfit. Try to avoid looking at the test dataset.",Machine Learning Zoomcamp,2021,
629898,"Ankush
Yes, they are absolutely the same. AWS Redshift would be quite similar to BigQuery – not quite similar, but still similar. You can think of AWS Redshift as a Postgres cluster – a big Postgres cluster. S3 is definitely a one to one comparison to Google Cloud Storage. And I'm not sure why you want to use EC2. If you want to deploy Spark jobs, maybe consider EMR. But if you're using EC2 maybe for just pipelining or Airflow, then EC2 is also pretty good. It's just a machine. 
Alexey
Because, here we used virtual machines for many things. That would be an equivalent, indeed. You need a remote computer that is running in the same data center as the S3 buckets so they are very fast. So if you download something from S3, or upload something to S3, it's usually way faster if you do it from EC2. Another similar concept is Athena. I think it's more of a thing on top of a data lake, but it gives you this “data warehouse feeling”. 
Ankush
It's SQL on top of your data lake.
Alexey
I would say it's something like... to me it feels like something in between BigQuery and Redshift. It's not as fast. But it's also cheaper than Redshift. 
Ankush
Yeah, I think you're maybe comparing it with Presto. It might be a better fit. It's a Presto. Basically, internally and outside it's a Presto cluster. 
Alexey
Yeah, it is Presto. When I need to Google things – like when you need to Google syntax for Athena – I usually add Presto at the end. Then it works.
Ankush
[chuckles] Yeah, exactly. AWS also has a managed workflow for Apache Airflow. So if you're thinking of using Airflow, then also check that out, so you have to deploy less stuff on your own.
Alexey
And I think in Google Cloud, we also have that. In AWS, you can create EMR, as Ankush mentioned already. 
Ankush
EMR for AWS is something like Google Cloud Dataflow.
Alexey
You can create a Spark cluster with just one click. I think I shared the document that explains how you can configure a cluster. Maybe I'll share again after the Office Hours. So if you want to use that, you can do this.",Data Engineering Zoomcamp,2022,
666820,"Alexey
For example, in Spark, if you Google “Spark GCP BigQuery connector” there is a good article that shows you what you need to do. You need a jar and you need to use this jar when you submit your Spark job. When you do, let's say your files are stored in Google Cloud Storage, you do something with these files, then you have multiple options. The first option is to save it back to Google Cloud Storage and then create it as an external table or something like this, like we saw in week 3. Another option would be saving it directly to BigQuery using this. You just say, “write,” and then you say you want to write to BigQuery. Then it uses this connector to connect to BigQuery and write it. I don't know how permissions actually work here – maybe you will also need to specify the key here somehow. But probably, if you're doing this already from Dataproc, then it should already have all the permissions that you need in order to write to BigQuery, or in order to read from Google Cloud Storage. So if you use Dataproc, theoretically, it should be as easy as that – just add an extra jar and write to BigQuery. Then once the data is in a data warehouse, then you can use the materials from the DBT week or analytics engineering week to actually consume the content from the data warehouse and visualize it.",Data Engineering Zoomcamp,2022,
930549,"Victoria
I think there are two parts here. I can talk from the part where the data is already in BigQuery because we can do this with DBT. The other part would be loading the data incrementally. In DBT, there's a third materialization that I briefly mentioned but I didn't go into details while doing the project because it's more advanced. It's called incremental. What it does is exactly this – your models a table, it is materialized as a table, you'll find it as a table in your data warehouse. But it allows you to use an incremental block, where you would define what is new data for you. Every time you run the model, the first time you run it will do the “create table”. “Create table as” “select everything,” so selecting entirely everything from the source. But the next time you run it, it will insert all the new data, depending on this incremental clause. Because you have to tell it what the new data is. Let's say you have a loaded timestamp, then you can do “select everything” where the timeframe is greater than the max timestamp that you already have in that table. That way you load incrementally. Also, it's very useful, because most likely, you don't need to reprocess. Let's say, in our case, we're processing trip data. Let's imagine that we are processing this trip data every day, because we work at the company or the taxi in New York. We don't want to reprocess the trip data for the past years. We just want to process the new data that just came in. So that would make sense to also do it incrementally, instead of applying the whole business all the time (to the whole amount of years) of data that you have. I think that covers that part.
Ankush
So what does restartability mean? Does it mean loading all the data? 
Victoria
Yeah, I understand that. At least. Because, if you do it incrementally, there's a point where you want to refresh maybe. That's what I understand from restartability. Then you would load everything from the source.
Ankush
Yeah, exactly. It would be the same way, basically. You can choose the different time ranges, or you can just say, “No filtering. Just give me all the data.” In that case, the table will be created from there. All the fresh data, basically. And to make it an automatic operation, just remember to create a replace table. Since that's an automatic operation, there would not be any issues if there are queries already running. I don't know if that was the question or not. But I think that's what is meant by restartability.",Data Engineering Zoomcamp,2022,
177008,"Alexey
Well, it’s up to you. If you have an AWS account, you can go with a transfer service. That's probably easiest because you just need to click a few buttons. If you want to set up a GCP virtual machine, you will need to spend some time setting everything up. It's worth doing this because for other weeks – for spark and probably for Kafka as well – it will be useful for you. This is probably worth the time investment. But if you just want to quickly copy the files, the transfer service is probably the easiest way.",Data Engineering Zoomcamp,2022,
794038,"Ankush
When we were planning the syllabus, we actually removed NoSQL databases because we wanted to focus more on some of the most important tools of data engineering. That's covering Spark, data warehouse, Kafka, DBT, and Airflow. But NoSQL is definitely an important part for data engineers to know. If the community really wants it, maybe we can develop a couple of videos on NoSQL and how to use it – maybe give an example of Cassandra or something.
Alexey
We also ran a survey before we started this course and there were eight different topics concerning what the community wants to hear about. Analytics engineering was the top one in terms of preference, and NoSQL was the last one. Whether it was about the internals of NoSQL, I don't remember. We based some of our decisions on that survey as well. 
It seemed like there was that much interest in NoSQL databases, but a lot more interest in analytics engineering. That affected the way we decided to come up with the syllabus for the course. Maybe you can also let us know what specifically you have in mind, because NoSQL is such a broad term. Are you talking about Redis? Or are you talking about Mongo, or Cassandra, or Dynamo, or what exactly?",Data Engineering Zoomcamp,2022,
277928,"Alexey
It doesn't affect the score. We count links, not likes. Counting likes would just be too difficult. We would need to go to LinkedIn and then… It's just too difficult. There are many different social networks, so we just count the number of posts to make. But don't overdo this – we cap it at seven. Let's say you're following a course and every day you share your progress – that's why it's capped at seven. You can, of course, share more but we will only count seven.
Victoria
Everything that I see where it says DataTalks, I go and react.
Alexey
I also comment and then it usually gets a widened reach. But don't worry about that.",Data Engineering Zoomcamp,2022,
868860,"Alexey
Yes or no. Let me tell you why “no” first. No, because we use peer reviewing, and if the project deadline is already up, then there are no peers that can review your project because we cannot, let's say, “support” the course forever. That's why, if you want to get your project reviewed, you need to do it now. You need to submit it within the deadline and then your peers will review it and give you feedback. This is why “no”. 
I also said “yes”. So yes, because as you might remember, a few weeks ago, we made an announcement that there will be another project cohort right after the first one. We will finish the project and then we will have another project immediately after that. Because of the war in Ukraine, people cannot work on the project right now. For them, and for everyone else who cannot work on the project right now for some reason, we're giving you a chance to work on this. 
Let's say you just joined the course and you cannot catch up with everything, then you can submit your project in the second “trial/iteration/cohort” – basically, after one month (after three weeks) we will have another project cohort and then you can submit your project there. But after that, no, there will be no way to submit the project again.",Data Engineering Zoomcamp,2022,
102126,"Alexey
You can. You don't have to use Spark for that, depending on exactly what you're doing. Maybe you can  tell us what exactly your dataset is and then we can maybe give you better recommendations. But you don't have to use Spark. It's optional. Of course, you can use it if you want. You don't have to use it if you don't want to. There are other tools. I think just using Airflow is also fine. Maybe consider using DPT. For your use case, it may be easier than just doing the whole thing in Airflow.",Data Engineering Zoomcamp,2022,
940334,"Victoria
I would say the video from Ankush was a nice recap. [chuckles]
Ankush
Actually, that was the recap. All right. Data warehouse vs data lake. Let's just go through the differences quickly. Data warehouse is basically meant for structured data, where you already know your schema of the data. You already know how different tables interact with each other. You are more or less sure what your future data will look like and you're not changing that on a regular basis. In those cases, a data warehouse is the perfect solution. You put your data in a particular schema nicely into your tables in the data warehouse and then you can basically query across a huge amount of data with respect to joins or different SQL queries you have on top of that. This is basically the concept of a data warehouse. So you have a schema, you already know everything your data size is generally in terabytes, not petabytes, let’s say. And you have long-running SQL queries working on top of them. 
When it comes to a data lake, the basic idea is that you do not know what kind of schema you have. You do not know how this data will be used in the future. What you really want to do at this moment is to save the data as soon as possible in some place where it can be used in the future, or it can be used for different use cases that are maybe not covered right now. In those cases, what you will do is basically take your data, whatever schema it is – doesn't matter. It can be JSON, it can be parquet – it can be whatever. It's good to have some standards, but even if there aren’t any, you can still dump that data to cloud storage or an S3 bucket on an HDFS instance. You basically dump your whole data in whatever format you want. 
The idea should be that this data should be recoverable. You should have some sort of metadata attached to it so that this information is searchable. You know what the column names are and what they mean, so that if another team wants to interact with this data, they can. With the data lake, the idea would be that, “hey, I don't know what the use case is right now, but I dump it and then tomorrow or in one year, data scientists or data analysts come to it, use this data, see that this information is available to me, maybe transform this data, and put that into a data warehouse, or it put it into a different bucket and then use it.” A use case typically can be a machine learning model that collects a lot of data over the years. Let's say you have data for recommendations or data for people buying different products for the last five years. Then you can build up a recommendation system on top of that. But if you lose this data, then basically, you cannot generate these kinds of models. That's where a data lake really plays a vital role. 
Alexey
That's a pretty comprehensive recap. 
Ankush
I think it was not a good one. [chuckles]
Victoria
At the end, also what does the trick for me (a silly trick) is to think about the word. How do you store things in a warehouse and how do you store things in a lake? You don't. You just have everything available there. So the image, at least for me, helps me understand the concepts as well. In a normal warehouse, you would have shelves, things with structure and this is also how the data looks in a data warehouse, kind of.
Alexey
The way I understand them, maybe this is not the best way of thinking about them, but a data lake is like your file storage. You write parquet files, it could be S3 or Google Cloud Storage, or HDFS – for us it’s HDFS – and then you have a bunch of buckets. These buckets have some structure but this is just a bunch of files. These files are documented. They follow the same schema more or less. But this is just a bunch of files. If you want to access these files, something needs to go there, read these files – it could be a Spark thing, it could be an Airflow thing, it could be a simple Python script – but it's just files. 
When it comes to data warehouses, it's not just files. It is a thing. It's properly structured. There are tables, you know the schema, there are some use cases, etc. It's also faster. Because for a data lake, usually you need to download these files, do something with them, put them somewhere. But a data warehouse is optimized for the specific analytical queries you have. Let's say you want to create a dashboard and then in the data warehouse, you model this data in such a way that it's relatively fast to do this. You don't need to go there, download the files, run aggregations – it's a lot easier. This is how I understand the difference. A very quick recap would be – files dumped on S3, that is a data lake. Nice schema database, then it's a data warehouse. Or is this too much of a simplification?
Victoria
I think it explains the big difference between a data warehouse and a data lake, which is the bigger concept. And then from there, you can always go deeper.",Data Engineering Zoomcamp,2022,
811487,"Yeah, I guess. Of course, I would like it to be about the course. Otherwise, I don't see why you should get a point. But if your experience involves the course and you share it on your tech blog, then yeah – of course. You don't have to use Twitter or LinkedIn – you can use anything. I think a blog is perfectly fine.",Machine Learning Zoomcamp,2021,
254018,"Dmitry
Regarding the question – does “filters” mean regarding data augmentation?
Alexey
I think this is talking about convolutional filters. We apply a filter to an image and we get a feature map. 
Dmitry
But this is not the image anymore.
Alexey
Yeah, but I remember the first big architecture, AlexNet – there was an article about that, or maybe even the original paper about AlexNet, where they showed the filters. These filters were different, like stripes or color, color shapes, something like this. I actually don't know what’s there if we, let's say, take filters from exception and try to visualize them. But in principle, these filters are available as weights of that layer. Maybe you can just use matplotlib and try to see what’s there. I have not done this. I remember seeing that paper about AlexNet and I found it really interesting that you have all these stripes there. There is a comment that somebody saw this in the AlexNet paper.",Machine Learning Zoomcamp,2021,
499542,"Sejal
A pipeline for someone… “for someone”?
Alexey
Maybe if you're a data engineer freelancer, somebody comes to you with a pile of money and says, “Hey, implement a pipeline for me.” How would you approach that?
Sejal
You would start with gathering the requirements based on how automated of a solution you want to make, how much you want to invest, and if it is a really simple and straightforward task. I'm just giving an example here for data ingestion – you can have your own custom pipelines built in Airflow or you could just use something like Fivetran, which has automated ETL solutions, where you just provide the input and output and integrate it with your targets, such as Snowflake. With a minimal or standardized level of ETL, it will do the job by just providing the schema and just playing around with SQL. But for more custom ingestion pipelines, you may need to use Airflow, for example, to add some additional steps to preprocessing. It really depends on the use case. What would you say are Alexey and Ankush?
Ankush
What I would do is, let’s say you have some pipeline in mind and you already know what kind of steps you have, then Google provides a very nice calculator. This is called Google Cloud Pricing Calculator. In this, you can basically put what all resources you want, you can put what kind of compute engines you want. There should be Dataflow also, GCP. BigQuery, for example, you have cloud storage, you have BigQuery, you have data prog, data flow. Now you know what kind of tools you want to use. Once you know that you can basically do this on-demand or flat rate and then you can have a very good estimation of your price. Obviously, you should always overprice, or overestimate, and then maybe cut the resources based on that. This can also help you to decide – maybe you don't want to deploy your own Kafka cluster, you just want to use Pub/Sub, because Pub/Sub is much cheaper for your use case. In all these cases, you can basically do these kinds of estimations on the fly. It gets tricky, because obviously, you cannot estimate everything, but you can make some signs of sort of comparison. I think similar ones are also offered for AWS,
Alexey
It’s maybe not as convenient as this one. I think for AWS, at least not to my knowledge, you don't have everything in one place. For example, if you want to calculate pricing for Lambda, there is a separate page just for Lambda. Maybe there is something now that is as convenient as this one but I'm not aware of that.
Ankush
So I found that just Googling here because I'm more on GCP. There is this NetApp AWS calculator.
Alexey
This is like a third-party thing, right?
Ankush
Yeah, I think it's a third-party thing. I'm not 100% sure what AWS has to offer. Over here you can also kind of estimate your costs by doing the same thing as you did in Google Cloud. But the Google Cloud one is just way better.
Sejal
You may also want to think about… it's always a good option to begin with, if you're building it for someone, to begin with a cloud-based solution, because most of them are managed services. But in cases where, let's say for security reasons, they want to have their own cluster and manage their own stuff, if they already have a DevOps team, in the future you can also consider a Kubernetes cluster and deploy a self-hosted solution there with an open source stack. There are a lot of trade-offs that you need to make in order to satisfy the need.
Ankush
AWS does have a calculator. Calculator.AWS.
Alexey
I wanted to share this link for Scling. This is from of one of my DataTalks.Club podcast guests. The podcast is this DataOps 101 episode. He collected quite a lot of resources that are related to data engineering, like batch processing, stream processing, and so on. Maybe this is something where you can probably find something useful there.
Ankush
The last link I saw there was Transitioning to Scala. See? You cannot live without Scala. Alexey, you need to learn it now.
Alexey
I did have the pleasure of having to use Scala for too long. [chuckles]
Ankush
[laughs] Too long, okay.
Alexey
Like it was three years or something like this. 
Ankush
I also see there's also Moving a team from Scala to Golang. 
Alexey
I think there was an article that I read about this star ship operator. If you come across this thing <|*|> in code and maybe try to Google this, and you get “No matching documents”. [chuckles] This is a funny article. Then they figured out that this is a starship operator from some functional programming library in Scala Z. I think there is a bunch of them, right. Cats. Yeah. That's a funny article. I think with Scala, you have to set some standards, and then make sure that people do not get too carried away.
Sejal
Yeah, my rule is – learn Python unless you're joining a company like Spotify that requires Scala. [chuckles]
Ankush
True. And even if you start with Scala, don't start with all the functional libraries and stuff. Try to keep it simple in the beginning.",Data Engineering Zoomcamp,2022,
620250,"Sejal
Tricky question. Actually, there are multiple ways. I had to really either brush myself off and do multiple projects. I would say that you can start taking up opportunities within your current company or your current team and see what can overlap on the data engineering side. If you're a data analyst or data scientist maybe take some automation tasks and start from there. You could also learn from your team, who are already experts. I think starting with that aspect gives you an understanding of how to align with the standards of your current company, the technologies that your current company uses, the business domain, and the knowledge required with respect to your company. Many different companies have their different ways of doing things. For example, not everyone uses GCP or AWS. My recommendation would be to start from there.
Ankush
I think that's definitely one of the paths to go. As a fresher you can, you can also learn on your own. There are plenty of resources available. We already recommended some books. There’s this course itself. If you complete this course and go through all the videos and do the homework, maybe even do the project, you will have a very strong foundation to develop into a data engineer. Most probably, you would be able to take more tasks as you grow your knowledge with these videos. That can actually help you to land a full-time job from an internship. If you're interviewing right now for an internship position or a data engineering position, I would say that the best way would be to do it via showing your work and showing your progress on LinkedIn, and sharing your updates on LinkedIn or other publicly available channels that will show to the eventual employer that you are interested in this domain, and that you are making gradual progress towards becoming a good data engineer. That would definitely be one of the recommended ways to go forward.
Alexey
That's why we encourage this Learning in Public thing. That's why we give extra points for it. I know that it's not easy to actually tell the world that you're learning. That's why we want to give you a little bit of push and a little bit of encouragement by giving you extra points. I hope you will be successful in finding your internship soon and that you will not need to spend a lot of time on that. Good luck.",Data Engineering Zoomcamp,2022,
643810,"Yes, there are very good references – the course we have. This course is called Machine Learning Zoomcamp. It will cover all these parts except Conda and Git. But I assume for the midterm project, you use Git. As for Conda – I use Conda as a Python interpreter, and then I install Pipenv there to manage dependencies. I don't know any other good references on that, to be honest. Maybe there are. I don't know if I answered your question or not. But I hope the references we have in the course are good enough. If they’re not, please let me know what you think can be improved there or what you think is missing.",Machine Learning Zoomcamp,2021,
747048,"Sejal
Recently, I did a podcast episode on this, which Alexey coincidentally just published on Spotify and Anchor just a day before. This is about transitioning from data engineering to machine learning engineering. It covers a lot about how there are some overlapping skills. I would recommend you listen to that. In a nutshell, I would say this particular course would be really foundational for you to build the basics and see how it really overlaps on building the production skills – to productionize ML workflows, for example. There are a lot of technologies in data engineering that you can use on the ML engineering and MLOps side. So I think it is definitely useful and not an overkill.
Alexey
We have another Zoomcamp which is here. This is a course that is specifically for training you as a machine learning engineer. My wife did this nice illustration. In this course, you will learn the basics of machine learning. If you know the basics, you don't need that. But you can take a few other modules there, like deploying machine learning models, serverless, deep learning, Kubernetes, and TensorFlow serving – these kinds of things. Of course, in that course, we don't cover data preparation. For data preparation, this course (DE Zoomcamp) is quite useful.",Data Engineering Zoomcamp,2022,
747232,"Yes. At work (OLX) we quite extensively use Cookiecutter. There are some who have some internal structures. For example, there is a thing called AWS patch, which is a way to deploy models that we use often. And for example, we have a Cookiecutter template that kick starts a project for AWS batch. Then we already have this template and we just use it. Yeah, Cookiecutter is a great tool. If you haven't heard of this tool, I suggest checking it out. Sometimes we just know, “Okay, we need to have this folder, this folder, this folder, because in our team, everybody makes projects this way.” So sometimes there are just these informal guidelines as well.",Machine Learning Zoomcamp,2021,
167856,"Ankush
I don't know if there's an example project.
Alexey
Well, we for sure will have some good example projects after this project, right? [chuckles] Some of you will create great projects and these will be example projects. But right now, I cannot think of any good project that is open and I can recommend. There must be some projects, but I just don't know. “Ideal pipeline” is such a vague thing.
Ankush
Yeah, it really depends on your use case. For me, an ideal pipeline would be one that has the least manual steps and requires the least intervention even for adding new data sources. So if you have, let's say, similar kinds of data, then it can consume that. If you are producing more data, then it can scale accordingly. There is as minimum intervention from your side as possible, as a data engineer. That would be a good pipeline, I would say.
Alexey
In this definition, can we have an ideal pipeline if we stitch together all these open source tools like Airflow, Spark, and all that?
Ankush
[chuckles] No, I'm pretty sure the product manager will come up with a new use case in which your ideal pipeline would no longer be ideal. [laughs]
Alexey
I think there are tools like Fivetran, which I never used, that are closer to this more ideal one. But in the case of Fivetran, it works until, as you said, a product manager comes with a use case that no longer fits that, because this is more of a closed-source platform. Therefore, you will need to figure out how to actually make these things work. But I have no experience working with these things, so I don't know. At OLX, we love to stitch together open source products and hope for the best.
Ankush
If you are going for open source, I think the tools we discussed are the most used ones out there.
Alexey
Are they ideal? I wouldn't say so. But they work.
Ankush
I think some of the enterprise ones are just very much better – they are much easier – but then they are less flexible as well. If you have basic use cases, that's perfectly fine, but if you want to do something very specific, you would be stuck with it and you might need to pay more or ask for some improvements, which might take months to come. These kinds of things always really depend upon what kind of majority your company is in and what kind of use cases you have.",Data Engineering Zoomcamp,2022,
879860,"Alexey
I think I just answered that question, more or less. Dmitry, what do you think? Do we always need to scale features or not?
Dmitry
It really depends on the model that you use, but if we're talking about the linear model, then for sure. You need to scale it before [audio corrupted]. There can be a situation where they’re already more or less scaled, and you don't need to do anything. But overall, yeah.
Alexey
And for this particular dataset, we saw that we have features on different scales. For the model, it could be confusing – when we use an iterative solver to find the best solution, it can be confusing for these kinds of solvers (for iterative solvers).",Machine Learning Zoomcamp,2021,
634036,"Ankush
You can use it to substitute Kafka Streams, yes. If that's what you mean, yes, you can definitely use Spark Streaming to consume data from Kafka and then do your transformations (joins, windowing, whatever you want) and then put it out to another Kafka topic or to some other file system. Whatever you want. Of course, you need a Kafka cluster. You need the broker of the Kafka to actually publish messages to Kafka, but you don't have to use the Kafka Streaming thing. Because I think as a lot of people have realized, the Python library for us is very limited. If you're a Python person and you really want to use streaming, then Spark Streaming might be the only option.
Alexey
Do you know if Spark can connect to Kinesis in AWS?
Ankush
Yes, Spark can connect to Kinesis, also.
Alexey
So you have multiple options, right? You different brokers? What do you call these tools? Streaming engine sort of? 
Ankush
Brokers, yeah. 
Alexey
Brokers. So it can be Kafka, Kinesis, Pulsar – are there others?
Ankush
You can basically connect to Google Cloud Storage. You can connect to BigQuery. You can connect to anything you want. Not anything you want, but whatever connectors are available. But there are lots of connectors that are available in Spark. You can basically do that very easily.
Alexey
Okay. So you do that and then you have your Spark Streaming thing, right?
Ankush
Yeah, that's the biggest limitation of Kafka Streams as well. Kafka Streams is only Kafka to Kafka. So if you want to do something else, you need to either export your data into Kafka or just use something else, like Spark Streaming.
Alexey
And for Flink, you can do pretty much the same thing. You can connect it to Kafka, to Pulsar, to Kinesis, right? 
Ankush
Exactly.",Data Engineering Zoomcamp,2022,
547758,"In this case, if everyone uses Docker, you will not have this problem. If they don't use Docker, then they just don't get the point for using Docker. If you can’t run it, you just write, “Sorry, I cannot run this.” I hope this doesn't happen. That's actually the whole idea behind Docker – it's transferable between different environments. Let's say if they use Gunicorn and they don't use Docker, then you will not be able to test it. In this case, you will use the scoring matrix and evaluate this accordingly. If there is Gunicorn, you tick that box. But I would encourage you to still try to learn from that submission, even if you cannot run it for some reason. I think the only reason that you cannot run something on Windows from Ubuntu is Gunicorn. Other than that, everything should be runnable.",Machine Learning Zoomcamp,2021,
136400,"Sejal
I think this is related to orchestration. Since you've covered the concepts of orchestration, Alexey, maybe you can take this one. But just putting in my two cents here – Airflow is definitely meant for orchestration of multiple jobs, sometimes even in parallel or asynchronously running jobs. It's not really meant for sequential run or small-scale usage, but definitely for more resource-intensive jobs. 
Alexey
Airflow itself is quite heavy indeed, but the benefits that it gives you – the mechanisms, the UI, viewing the history – this is very useful the moment you have more than three DAGs running in parallel. When you have one, Airflow might be overkill. But if you know that you're going to have more pipelines, more workflows, then it's time to think about some kind of  workflow orchestration tool. 
It doesn't have to be Airflow, but Airflow is quite widespread. Let's say you have a team and you're making a decision on which workflow orchestration tool to use, Airflow would be quite a good choice because data engineers on the market know it. Data scientists also know it, more or less. Usually, like this is a good tool. But of course, there are disadvantages – it is indeed resource-intensive. Then it needs to run somewhere – you typically don't set it up on your computer.",Data Engineering Zoomcamp,2022,
383446,"Alexey
Unfortunately, you will need to wait because this process is quite manual. We will need to create all these things ourselves and can only do this after the deadline.",Data Engineering Zoomcamp,2022,
886326,"Alexey
When we created this form, we forgot to include this field. I found where the form is located. Now I also have edit permissions for this form. I will just go there and add this URL field. When I do this, I will make an announcement and please resubmit your work. 
But if for some reason, you already submitted this and you do not have a chance to resubmit it, maybe we'll have to tweak the grading script a little bit so it will accept the work of people who already submitted without the URL. But yeah, it will change and to have a URL there.",Data Engineering Zoomcamp,2022,
545113,"My experience with Spark ML LIB is that it was added to Spark as more of an afterthought. It's not the focus of Spark. They thought, “Okay, it would be cool to show that we can also do machine learning.” But the models that you get from Spark ML LIB are not really good. Usually, in Python, you use a library called SciKit Learn for training machine learning models, and they typically are a lot better than Spark models. They're also faster to train. My explanation for that is that ML LIB was added to Spark just to show that it's possible – like a proof of concept – and they really didn't invest much time into that. I don't know if it's actively maintained or not. 
My experience wasn't good with this. The way we usually use Spark is for preparing the data and then once the dataset is there in a parquet file, then we would use something else – not Spark – to load this data and train a model. Then for applying the model, I already released a video about that. It's the last one about map partition. This is how we typically apply machine learning models with Spark. You can check it out. As for what I’m using for building machine learning for big data – you can just use a big machine that has a lot of memory (a lot of RAM) and then you just load this dataset and use something like Scikit Learn to train a model.",Data Engineering Zoomcamp,2022,
148173,"Alexey
I'm not sure I am able to parse the question. But to my understanding is, “Are there more data engineering freelance jobs than web software engineering?” I don't know if any of you are monitoring it, but my gut feeling is that no, there are not so many freelance jobs compared to web software engineering. Simply because there are so many people who need a website, like a WordPress website, compared to a data pipeline – maybe not so many people need that. But still, it's sufficient to find a couple of gigs here and there. But I'm not a freelancer, so I'm just speculating.
Ankush
Neither am I. But what I can tell you is – if you are good, you will definitely find a freelancing job in data engineering. I’m pretty sure about that.
Alexey
And it's not fair to compare it with web software engineering, because the demand for a simple website is so high. I think this is the most popular engineering kind of remote job.",Data Engineering Zoomcamp,2022,
561046,"Lisa
Well, I thought by doing them singly, it was a little more hands-on. A lot of times, I would just try GridSearch, but I think we were trying to demonstrate “Oh, we have skills that we can actually try these.” I have used things like DataRobot and things like that, which are great for, “Okay, let me try 100 models,” but then you usually have your own business sense or subject matter expertise. So you kind of know, “Okay. Well, maybe this influences things more.” Something like either the GridSearch or DataRobot auto-machine learning are I think good places to start. I wouldn't just use that. 
Also, what's nice about DataRobot is that you can put your own models in. [chuckles] But it's really handy in terms of saving you time. You saw how long the notebook got when I tried, “Okay, let me just tune the learning rate,” or “Let me just tune different things.” GridSearch does it a lot faster. Some of the auto-machine learning will dump out, “Here's your chart. Here's your comparison. Here’s your leaderboard.” It’s so much faster than I could code it. I think they each have their place. It depends on what your use case is, I guess – how fast you need to get something done, what your goal is, that kind of thing.
Alexey
Yeah. With DataRobot, you just give it a dataset, you press a button, it does some magic, and then it gives you the best model, right? This is how it works, doesn’t it?
Lisa
Pretty much. You can only have one dataset, so everything has to be prepared to go in that dataset. You can tune a bunch of things. What's nice is that you can have the intern run it or the PhD. The intern may not know as much about the subject matter or how to work with it. I think both can use those kinds of tools. Yeah, it is a big button [laughs] to start with.
Alexey
The way I would answer this question is – there are, of course, advantages and disadvantages. Both deserve a try. I would go with GridSearch if I know that my dataset is small, and I can try to fit a lot of models. Let's say we have only a couple thousand observations in our dataset, then training a single model doesn't take more than one second or two. Then you can go crazy and test as many combinations as you want. Because what GridSearch is doing – it's trying every single combination. Let's say you have the C parameter, then you have random forest. 
You can try with Bootstrap and without. You can try a number of estimators this or that. You can try max depth on that. That it basically creates like 50, 100, 1000 different combinations. If your dataset is small and fitting one model doesn't take a lot of time, you can just go with that and then see. But usually, when you have a relatively large dataset, like 20-30 thousand, then fitting one model takes some time. It's not one second – it's 10 seconds, 20 seconds, 30 seconds. Then trying GridSearch just takes forever. 
This is when I would actually go and try to tune the features singly, exactly like how I showed it in the course. For example, with XGBoost what I showed was more like a heuristic, which usually works. For XGBoost it doesn't give you the best parameters necessarily. So maybe with GridSearch, if you give it enough compute, GridSearch would find better parameters than when you turn them singly. But you can find an OK solution faster. So GridSearch takes a lot of time, but finds a better solution, a more optimal one. Tuning features singly takes less time, it's more menial, but if you have a large dataset, you can arrive at an OK model faster.",Machine Learning Zoomcamp,2021,
200835,"Alexey
Not today. [laughs]
Ankush
[laughs] Maybe after a long break,
Alexey
The thing with ML Zoomcamp, I already had a book and then I thought, “Okay, maybe I should do a course.” But here, that's different. But actually..
Sejal
But good idea though.
Alexey
I think maybe, Alvaro, if you're watching this – I think you could be quite good at writing books, because the notes that you have here in your repo are quite comprehensive. You can just take this markdown and go to O'Reilly. Tell them “Hey, look what I have.” [chuckles] Anyway, maybe you don't have to go to O'Reilly, but I think notes from Alvaro are quite good. And other notes as well are quite good. It’s just that Alvaro has been doing this for quite a while. He was also taking a lot of notes for the other course. And he's always first to submit his notes in pull requests. Thanks for doing a great job with notes.
Ankush 
Feel free to use them, just share the royalties with us. [chuckles]",Data Engineering Zoomcamp,2022,
393833,"Ankush
Master node.
Alexey
Master node? But not necessarily, right? A master node is something that coordinates the execution. But then, let's say if I do Spark submit from my computer, or I open Jupyter on my computer and connect to a master, then the driver node becomes the thing from which I do Spark submit. Right?
Ankush
I think that's where the Spark context lies. If you are doing it in-memory, then it will be your machine. The Spark context would start and then you would connect to different workers, give your graph to them, give your data to them, and then they will start executing. Right?
Alexey
If I open the video Anatomy of a Spark Cluster. [image] This thing here is the driver node, a laptop or whatever you use, if you submit your job from Airflow, then whenever you invoke the Spark submit, or you connect to Spark to master – as Ankush said, wherever this Spark context lives. Let's say if it lives in my Jupyter notebook on my laptop, then my laptop is the driver. Then master is the thing that coordinates execution of a job. It watches which executors are available and if one of the executors dies, it reassigns – so it has the coordinating role. You can also watch the progress of the jobs from the master node.
Ankush
You are right. I just also looked it up. Basically, it can run on worker or master node. It's not always running on master node, but it makes more sense to run it on a master node because you'd want your Spark context to be alive throughout the process.
Alexey
So when you do Spark submit, you can say where the process should live, “Do you want to keep it on the master node or you want to keep it somewhere else?” 
Ankush
Yeah. You can specify with config spark.driver.host and that might provide the driver that would run on this particular host machine.
Alexey
By default, I think this is the laptop or the computer from which you connect to Spark master. But then you can override this and you can say that it should live somewhere else.
Ankush
That's what I see from the documentation I’m reading right now.",Data Engineering Zoomcamp,2022,133761_qa9_2022__de_zoomcamp_week_6_office_hours_image_one.jpg
205621,"Alexey
In terms of how exactly you should hand in your code – it doesn't matter. Just put it somehow in any format, and then add the link to GitHub. 
Ankush
I actually have a solution here. If you are doing this just for yourself and to learn – maybe want to do it as something for your learning purposes, that's perfectly fine. You can do it in a .md file or whatever file. But if you are thinking of this as a project you would like to highlight to a future employer, or you want to attract future employers by putting this on LinkedIn or open source, then I would actually also focus on putting the right file format and everything. It's easier in GitHub to read those, it's easier when you download them locally, to open it up in intelligence to just have the color coding there. So I think it's kind of nice to do it. But don't stress too much on it. If you put it in an .md That's fine. But maybe in the future, you can put it in a .sql file.",Data Engineering Zoomcamp,2022,
797668,"Ankush
The simple answer is “I don't know.” [chuckles] I think we should save this question for Victoria. I think she would be the best person to answer this. Maybe we can put this in the Slack channel and tag her.
Alexey
But also, for example, for these taxi trip data – do we use any star schemas or anything? Well, let's kind of a star, just with one ray, right? [chuckles]
Ankush
Kind of like that. I think the most common one is Star. But I don't know, as I have not done enough data modeling to know what Data Vault 2.0 would provide us for a particular dataset. I don't know what's the most relevant thing right now, honestly.
Alexey
But if you look at the criteria, you'll notice that there is no dimension that is called “data modeling”. Then, instead of DBT... you don't have to use DBT, because most of these things you can do with Spark, if you like Spark more. If you don't like Spark, you can go with DBT. Even though they are a little bit different, in the end, they're sometimes interchangeable. For example, you can maybe put the data in a data lake and transform the data for Spark and then put it to a data warehouse in the form that is ready to be queried by your dashboard. Or you can decide to put the raw data in your data warehouse, and then do transformation there with DBT. So it's up to you. You can go with either approach. I don't think there is a right or wrong here. [Ankush agrees]",Data Engineering Zoomcamp,2022,
457354,"Alexey
For that one, I will individually contact the 100 people and ask for their permission and probably some links like LinkedIn, GitHub, and whatever you want to include. If you're one of the top 100 and do not want to be there, you can also tell me that and I will put your name there.",Machine Learning Zoomcamp,2021,
138367,"Alexey
Yeah. You will be on the leaderboard. If you open the leaderboard, you will see that it is long and there are people who only attempted some projects. So they're still here on the board. At the end, there will be another tab here saying who passed the project and there will be a separate leaderboard for them.",Data Engineering Zoomcamp,2022,
785562,"Alexey
I think it is. Actually, when we were talking about the course and who this course is for – this is one of the profiles we considered. I think this is very popular now among data scientists to think, “Is there anything else besides data science?” Things like preparing data for our machine learning jobs, or for our machine learning models, or what happens after we built our model. That's why this is definitely one of the profiles that we considered and indeed, this is the right place for you. It's not only for data scientists, it's also for software engineers who know how to program. This is also the right place for you. For analysts, if you're comfortable with programming and the command line and you know Python, this is also the right place for you. In general, as we discussed the prerequisites – if you know how to program, if you know how to use the command line and if you know a bit of SQL, then this is the course for you.",Data Engineering Zoomcamp,2022,
375930,"Alexey
Yes, as I mentioned, there will be a certificate. And it does require the end project to be validated. The way we will do this is through peer reviews. You will get to review three projects of your peers and we'll have some formal criteria that you will need to use to evaluate your peers and your score will be based on that. Some of you might wonder, “Okay, is it fair that somebody else judges my work?” We tested this way of evaluating projects in a different course that we were running, the Machine Learning Zoomcamp, and it worked quite well. This is a good approach and for you, this is also a chance to learn from your peers because every time you review a project, you learn something new. Some people already know something, and maybe your peers will use something else that wasn't covered in the course. Of course, in this case, we will ask them to document everything so you do not feel lost and get an idea exactly what the project is about. So you can also see this as a way of learning extra things and this is how we are going to validate the projects.",Data Engineering Zoomcamp,2022,
489731,"Yes, there is a leaderboard. I think I shared the link a couple of weeks ago in Slack. Maybe I actually need to add the link to the course repo so it's just there all the time. But if you just look it up in Slack in the course ML Zoomcamp channel, you will find it.",Machine Learning Zoomcamp,2021,
586825,"Alexey
Maybe I was like that. Every time I heard that there was a new language, a new tool, a new something, I wanted to try it. But then I realized that these new tools are usually pretty raw. They look shiny, but when you start using them, a lot of problems come up. Sometimes it's better to use time-proven tools and be a bit conservative. This is something that happened with experience of trying these tools and then seeing that even though they look shiny, there are still some bugs, especially if it's a new tool. How to mitigate this mindset in the workplace? I don't know. I guess it depends on the use case, but just ask yourself (or the team) “How much does this new tool actually bring you? What are the pros and cons? How much work will you add to what you have when you integrate the new tool and what are the potential benefits?” If the benefits outweigh the headaches you will have, then you can go with this. Usually, when you do this analysis, you see that maybe you don't actually need that new tool and the old one is fine.
Victoria
Have a critical mindset. Do not just go with something just because there's hype. I always go read everything and then I can kind of see, “Is this something that I'm already using? Will this add that much value? How much headaches will this add?” Then, also, it's another tool to maintain. So I consider all of this. Have a critical mindset, definitely.",Data Engineering Zoomcamp,2022,
965735,"Victoria
Next week is a more difficult one, right? [chuckles]
Alexey
You think DBT will be the most difficult one?
Victoria
No. I'm already in DBT week. I meant week five.
Alexey
Week five? I don't know. I think Kafka will be the most difficult one. In terms of setup, it can give more problems. Every time I need to set up Kafka, it's a nightmare. But I haven't really needed to set up Airflow locally before that. So I think it's similar. Let's see. I hope it will be easier. 
The week about BigQuery should be relatively easy because it's managed. You don't need to do any infra setup on your machine. The DBT part is also relatively simple. Maybe at the end of the course, we'll have a survey and we'll see what the most difficult one was. I hope it was Airflow. I hope Kafka and Spark will be simpler.",Data Engineering Zoomcamp,2022,
872175,"I don't think it matters, actually. I think these two things are quite unrelated to each other. You can add a categorical feature to the entire dataset, or you can take a categorical feature to a small sample. Maybe first taking a sample and then doing feature engineering is better, simply because your dataset is smaller, so you can do these things faster. But I don't think it actually matters",Machine Learning Zoomcamp,2021,
673779,"I still have to finish grading your homework for this. Hopefully, I'll do this soon. Sorry, I didn't have time last week to grade it. I will do it as soon as I can. But don't worry, I've saved all your submissions. I just need to do a couple of tweaks in my Python script for grading. That's why it's taking me so long because I haven't done these things yet.",Machine Learning Zoomcamp,2021,
811052,"Alexey
Do you know anything about Apache Hop, Victoria? This is the first time I hear about it. According to Google it's a data and metadata orchestration platform. Interesting. Well, since it's Apache, I assume it's cool. But other than that. [chuckles] Usually, in order to become an Apache project – a top-level project – a project needs to undergo some process, which is not easy. For those projects that did undergo this, I think they're cool by definition, I guess. But I'm still not sure what this thing is exactly.
Victoria
Loading big datasets, but maybe more focused on orchestrating your metadata?
Alexey
I don't know if it's shiny. I wouldn't classify this documentation as shiny, to be honest. There are shinier tools, I would say. [chuckles]
Victoria 
Yeah, it's more old school. That's good. [chuckles]",Data Engineering Zoomcamp,2022,
432981,"Ankush
If you're talking specifically about interviews – if the employer is sending you software engineering interviews for being a data engineer, maybe that's not the right role or maybe that's not the correct company for you. Maybe talking to the tech recruiter and understanding the role might give you more insights into what the exact role is. Because honestly, “data engineer” has been used a lot these days and a lot of companies also use “data engineers” as a way to describe data product developers or software engineers who are working on very data-intensive applications. So it might be worth clearing that up. But having knowledge of algorithms and data structures I think are the basics for any software engineering – be it data engineering, be it analytical, or be it data scientists. Learning algorithms and data structures would always help you in your career, irrespective of the fact that you are going towards data engineering, data science, or any other domain.
Alexey
I would add that even for a data scientist, which is a less engineering-heavy specialization, it's very useful to know some data structures and algorithms. For example, using a list instead of set can degrade the performance of your data pipelines significantly, and then sometimes just one line change can increase the performance of these data pipelines like 10 times, 100 times. Knowing things like that is useful. Do you need to read Kleppmann's book cover to cover? Maybe not. But knowing lists sets, hashmaps, dictionaries – these things could be quite useful. Being proficient in LeetCode’s hard questions – I don't know how that translates to real life.
Ankush
Yeah, I think you would be able to clear a lot of interview tests if you do a lot of LeetCode. That might not convert directly to you performing better in your job.",Data Engineering Zoomcamp,2022,
839942,"I don't know if it's reasonable to expect this. I really want you to run it and learn from this, but if you look at the evaluation criteria, it doesn't say, “Okay, this is not runnable.” Ideally, you should do this – you should be able to do this and do Git clone, do Docker build, do Docker run, and test that it works. The evaluation criteria do not ask you to do this. You can just see what is there. For example, the criteria here say that there should be code for deploying or a Docker file is provided, and the readme should clearly describe how to actually build the container and how to run it. So if you want, you can clone it, and there should be no errors. Actually, maybe I should also add a section about errors here. But it's up to you, actually, if you want to run it or not. I advise you to run it, but I also recognize that maybe for some of you, it will require too much time – if you need to run three different projects and some of them do not run, it might be difficult. So I will leave this decision to you. If you have time, if you want to learn from others, I highly encourage you to actually run this. But I would say it's optional.",Machine Learning Zoomcamp,2021,
9861,"Ankush
Yes, definitely. You can explore AWS and Azure if you are more comfortable with that. We will be using GCP for this course and we will be using some of the GCP tools like BigQuery, which might not be available in the other platforms. There might be similarities and alternatives to that and we might touch on that in the course. But this course will be based upon Google Cloud Platform. One of the biggest reasons for doing that is that if you create a new account, you get $300 credit. That's really useful for everybody who's not familiar with the cloud. They can just create a free account to do this course and, ideally, not spend any money on the platform services.
Alexey
And I think another reason we decided to go with GCP is the connection of DBT with BigQuery. With Athena, I think it's a bit tricky. If we want to go with Snowflake for DBT, then it's super expensive. This is not something you want to pay for during the course. That's why we were thinking whether we should go with AWS or with GCP – A) GCP gives free credits and B) DBT works with BigQuery. That’s what led to this decision of doing this in GCP. But as Sejal mentioned, we will be able to run everything locally, except the BigQuery part, of course, because BigQuery lives in the cloud. But the rest of the stuff should be runnable locally.",Data Engineering Zoomcamp,2022,
64697,"Alexey
I don't know. Week 3 seems lighter to me than what we did previously because we don't need to do a lot of local setup. I think this local setup is where most of the time was spent. Is that the case? Am I correct? 
Ankush
I think so. 
Alexey
But then there is week 4, in which I don't know how complex it is to set things up.
Sejal
No, I think it's cloud-based. Right, Victoria? 
Victoria
Yeah, it's called cloud-based and then there's the option of installing it locally. But then it's just doing a simple pip install, or brew install, or something like that. We can also provide the links to use a Docker image, now that everyone is a Docker expert. But it shouldn't be super hard in terms of installation. I would expect a little bit more complexity in trying to understand the whole concept. The project is simple, so hopefully, you can follow along with the videos.
Sejal
I would like to add the same as for week 3 as well. Most of the setup that we've done is already available from week 1 and week 2, the only thing is – we had to extend that setup to add more BigQuery tables. There would be barely any time invested on the setup side of things. And definitely no Docker or Docker related issues. [chuckles] Hopefully.
Alexey
Yeah, but then we have week 5, which is about Spark. And for Spark, we need to have a JVM, which is short for Java Virtual Machine for those who don't know. We will need to install this locally and we need to install a specific version of the Java virtual machine. It should be, I think, 8th or 11th. They already have a tutorial for that for Windows. For Windows, this is where things usually get tricky. And I think for Kafka, we will also need to have a JVM. [Ankush confirms] But hopefully it will be smoother.
Ankush
For Kafka, we are planning to do Docker. I guess we will not run into the JVM issue, but we might run into memory issues – we may run out of memory, basically what Alexey has said. I hope we can figure out a workaround for that. Maybe shutting down Chrome.
Alexey
[chuckles] Maybe you can just use the browser from your phone. Somehow on a phone, it works better than on my computer. I don't know why.
Ankush
Yeah, maybe Firefox? [chuckles]
Alexey
Yeah. But also, the amount of time you need to allocate also depends if this is your first experience with Docker. In this case, it could be tough. There is nothing wrong with this. We cannot know the background of everyone to give a good estimate of exactly how much time you will spend. What we will do, however, is at the end of each week, when you submit your homework, we will ask you how much time is spent on watching the videos and doing the homework. 
This way, we can get a better understanding of exactly how much time is spent on these things. For us, this is the first time we're doing the course, so It's very hard to have a good estimate. Everyone has different backgrounds, which makes it even harder. But I think it should be less time. Then, for week four, it's also cloud-based. Let's see. Hopefully not so much time.",Data Engineering Zoomcamp,2022,
956576,"Ankush
I don't know, but I think in AWS, it should be possible to do it without any restrictions. 
Alexey
Probably with a public open bucket, like we have with these taxi trips, theoretically, you just do the same thing as you would do with your own private S3 buckets?
Ankush
Yeah. I think the problem would be if the open source data is in Google Cloud Storage, then you definitely need to move it to an S3 bucket.",Data Engineering Zoomcamp,2022,
191857,"I think I explained that in the video. So just check it out. You'll find the answers there. It's called model selection. I don't think I should spend time now explaining this. If you watch the video, and you have questions about that. Actually, you said tran-dev set partition. Sorry. Dev in this case is the same as validation. So dev is just for testing the parameters. I think they usually refer to dev the same way as validation.",Machine Learning Zoomcamp,2021,
574881,"It is data snooping and you cannot impute missing data before splitting the data. Because in real life, you cannot go in the future and see what kind of data there is. All you have is what you have. You only have the available data, so you need to kind of mimic this scenario of not having access to future data. This is your test dataset. You don't have access to that, so you just forget about this until you are ready with your final model.",Machine Learning Zoomcamp,2021,
862021,"Alexey
I can add a comment to that. At least in my workplace, we have Airflow Instance set up for us. I, as a data scientist, would just go to an instance of Airflow that is managed by data engineers and DevOps engineers, and I would just use this. I never needed to actually run it on Docker Compose. 
The moment you have more than multiple DAGs, an orchestration tool will be quite useful. I would personally maybe go with Luigi, because this is my favorite one. But if I need to think about other people as well, since very few people know Luigi and not many like it, maybe Airflow would actually be a better choice to consider.
Sejal
I would also like to add to the question on Docker Compose. We are used to Docker Compose with Airflow, but not in a style where we use the official setup. I was seeing it for the first time as well, but it's unnecessarily complicated, especially with the XCOM variables and so on, so forth. Generally, Docker Compose acts as a wrapper to your Docker containers. In most production workflows, the Docker containers themselves are used in integration with your CI/CD pipelines wherever you're going to deploy it. Docker Compose. 
Only the Docker Compose part can be helpful in your development environments in case you have, let's say, a mock version of your Airflow environment from production and you want to test some DAGs on your local development environment. Then you can just use Docker Compose to make things easier for you, just running simple commands like ‘docker compose up’ would make it fine instead of running ‘docker, run, container name, etc, whatever you want’. So, just in terms of more convenience. But in terms of usage it’s generally used in development environments, not production.
Alexey
What do you think about this first part of the question, “how close to your usual data engineer role are the workflows?” Because the workflows I see at work are more complex. We have more steps. This is probably also the case for you. But you need to start with something. We already have a – I wouldn't call it a complex pipeline – but there are three, four steps, which is already something. Eventually, it might grow bigger and bigger when we add other steps there.
Ankush
I would also like to add that if you're in cloud, maybe you will not use Airflow and deploy it on Docker. Maybe you will actually use some service provided by the cloud provider itself, one that is similar to Airflow. I think Airflow would only be used internally, like a cloud composite and Google Cloud Platform. And for that, you would be using Terraform.
Sejal
Yeah, I have used AWS Step Functions for the AWS setup. Step Functions in combination with Lambda in place of Airflow. But here, what Ankush said is also something that I wanted to say. It really depends. In production. If you have a self-hosted cluster on Kubernetes or wherever and you want to use a very pure Airflow version, then your company could have that kind of setup, and you just use native Airflow as it is on a cluster. Or you could have a cloud-based setup and that is a more convenient option, to use managed services such as Cloud Composer. AWS also has its own version now called Managed Airflow.",Data Engineering Zoomcamp,2022,
697507,"Sejal
This is something that I was planning to cover in the code video for this week. The videos for that have not been updated yet. There are actually multiple ways – you can use cloud services, some of which are in an auto-scaling mode. Some of them provide that kind of usability to auto-scale clusters, such as EC2 clusters. The other way, if you have a very local project and still you want to run it on scale, then some data scientists might say that you need better hardware [chuckles] like having more GPUs, for example. I think this question is a little broad, but we will be covering different use cases in our course material. 
Ankush
Yeah. I think there are a lot of things missing from this question. Just taking your example – it really depends on how you are ingesting it and how you are consuming it. If you are consuming it in real time, you will have a totally different architecture. If you're consuming it as a batch, then why do you care about consuming it every minute? So there are these kinds of questions. It would be way much easier to answer this if we know all of this upfront. I will actually encourage you to put this in our engineering channel in Slack. I hope somebody can pick this up or we can also pick this up. But I will also suggest that you think about the end usage. How are you ingesting it? How are you using it? Because that will also determine what the architecture will look like.",Data Engineering Zoomcamp,2022,
229781,"Alexey
I must admit that I understand what each word by itself means, but collectively – maybe I’m too much of a data scientist to answer that. Too bad Ankush is not here. Maybe he would be able to answer that. What about you, Victoria?
Victoria
No, I would say I am also not familiar with BigQuery. I haven’t worked with BigQuery outside of this project, in particular. But I would imagine that it's the same as any data warehouse. You could just use a key and then you start to limit a loading timestamp or something like that.",Data Engineering Zoomcamp,2022,
531532,"I don't think so. If it works for you, go ahead. I haven't really tried that. I know it works, but I haven't experimented. I think I'm too used to this screen. [image 5] I don't even use Jupyter lab. I used Jupyter notebooks. Maybe I'll try it, but I don't think there are any cons. In the end, it's the same Jupyter. It doesn't matter what exactly you use for that.",Machine Learning Zoomcamp,2021,414704_qa4_ml_zoomcamp_office_hours__week_6_image_5.png
173681,I think I just answered that in the previous question. I will make sure to document that as well in this file. We'll put deliverables here.,Machine Learning Zoomcamp,2021,
380014,"It's actually not. I think it's useful for you, but you don't have to do this. This train.py should produce a pickle file. Let's say you want to retrain this model using different parameters – for you, it's useful to see if it changed your score or not. In real life, like for projects I have at work, this train.py (it's not always just one file, sometimes it's a bunch of files) but all of them somehow log the performance. So I think it's useful to have. But if you don't want to for some reason, you don't have to do this.",Machine Learning Zoomcamp,2021,
912439,"Yes. In addition to that, ideally, it will be a repo in Git or a folder in your Git repo that contains the notebook showing the EDA, data pre-processing. Then, in this notebook, I assume you would play with different models. You would play with, let's say, logistic regression, decision tree, random forest, XGBoost – maybe you want to play with some other model and you want to tune parameters. You should also keep this in the notebook – this exploration of trying different parameters. 
Then, from this notebook, you extract the training script with just the best model and you also need to deploy this model. You need to create the Python file that loads your pickle file or whatever way you use to save the model. It loads that and then it serves with a web service. So you need that. Then you need a way of managing dependencies. I recommend using Pipenv, but you can use something else like Poetry, or Virtualenv. But I think it’s better to stick to Pipenv. And then you also need to package this in Docker. 
You have to have a Docker file and you also need to have a readme that explains how you actually run this thing. Let's say, your peer (somebody who is reviewing this) wants to run your project – they need to have clear instructions on how to do this. In the readme you can just say, “First execute that (could be dockerbuild -tname) and then execute docker run -it –rm.” Something like this – so they know what exactly they're supposed to do to run this.",Machine Learning Zoomcamp,2021,
852249,"Dmitry
Usually, if you're working with linear models, for sure, you need to have the same scale. It’s a good question, but it wasn't covered in this homework due to us trying to make it a bit simpler at the initial stages. But for sure, this is very important to know if you're working with the linear model, you need to have the same scale. 
Alexey
Here, we use normal equation, and for normal equation, maybe it matters less. Of course, if your features vary significantly, if in one case you have features that are in billions and another is between zero and one, then you probably need to do some normalization. But I think in cases like mileage and engine horsepower – yes, they are different, but for normal equation that we used here, it's not as important as for other types of finding solutions. We didn't talk about stochastic gradient descent or gradient descent in this course (in general). We will talk a bit about this in neural networks. But for this type of finding solution, there you need to scale features. Not for normal equation, though. Normal equation, you will be fine most of the time, I think, except if there is a huge discrepancy. 
Dmitry
Yeah, for sure it will affect it. But, as you said, if there is a huge difference, then definitely. 
Alexey
Yeah, I saw this question multiple times already, by the way, about stochastic gradient descent. This course from Andrew Ng – I don't remember whether he starts with normal equation or with gradient descent or the other way around. But today, Dmitry shared a good article with me. Maybe you can share it in the channels? Well, I think it's a nice one. If you're interested in other ways of finding solutions to linear regression, that's a good read. For that (for stochastic gradient descent) you need to normalize features.",Machine Learning Zoomcamp,2021,
393030,"I think you use the full train dataset. Let me look at GridSearchCV. For those who don’t know, GridSearchCV is just a way of doing parameter tuning and selecting the best parameters. Internally, it is doing cross-validation to find the best parameters. I think there must be an example here somewhere. Here’s a good example – parameter estimating using cross-validation. You can just check out this example. 
They are tuning the support vector machine classifier and they’re tuning parameters. They are trying different values for C. They're trying different values of gamma. And then for a linear kernel – this is just internal details for SVM, doesn't matter. They're trying different sets of parameters. And then what they do is – they have this train/test dataset separation, and they are doing what we call a full train here. They are doing the fit on the full train dataset. They don't do the split again for train/validation. They're using the full train dataset and then GridSearchCV is doing the split internally. I think this is a cool thing.",Machine Learning Zoomcamp,2021,
164801,"Alexey
I guess that you're right, there will be. This may be a bit of a spoiler. In the DataTalks GitHub, there is already a repo here, which I don't think contains anything. Not a lot of content. But, yeah there will be a course, eventually. 
Sejal
Maybe once we take a break from data engineering? [Alexey agrees] I'm gonna go on a long vacation, and [inaudible]
Alexey
But these courses are not dependent on each other. Date Engineering Zoomcamp is not a prerequisite for MLOps Zoomcamp. These are two separate courses. But yeah, I think if you took this one and you understood everything and it wasn't too difficult for you and you were able to finish it, then I think you're in perfect condition for the MLOps Zoomcamp. What is probably more useful for that one is actually the Machine Learning Zoomcamp because that’s where we already talked about things like deployment, Kubernetes, and things like that. That one would be more useful.",Data Engineering Zoomcamp,2022,
659528,"Model deployment stages, yes. Deep learning, no. I mean, it's good to know what deep learning is and what kind of problems you can solve with it. If I were to interview a junior data scientist and the person doesn't know how convolutional neural networks work, that's not a big deal. Actually, it’s the same for any level. If I were to interview a senior data scientist and they don't know how deep learning works – again, not a big deal, especially if this position doesn't require deep learning. If the position requires deep learning, that's a different question. But most positions don't require that, not only at OLX, but in general. Maybe there are some companies that specialize in computer vision, then deep learning is a must there. But I would say that the majority of companies do not require deep learning because they use other models as well, not just deep learning.",Machine Learning Zoomcamp,2021,
214199,"Yes. If you're running on Windows, what you can do is first install WSL (Windows Subsystem for Linux) and then use it. The second thing you can do is just altboot and install Ubuntu on your computer. This way, you can keep Windows and then for, let's say, development work, you just go and use Linux. Then every time you don't know how to do something, you Google it. At least this is how I learned Linux. I think you will be able to learn it this way as well. 
What you can do is – you have a command line, so just try to do as much as possible without leaving the command line. For example, say In this is Ls. If you want to look inside some file, you use “less”. Get familiar with tools like these simple Unix tools, like ls, less, CD, vd and so on – things like this. I think that should be sufficient for most of your work.",Machine Learning Zoomcamp,2021,
619776,"Alexey
You need to use some sort of orchestrator, whether it's Airflow or something else. But doing things manually – I don't think it's a good idea for the project. You can still finish the project. We haven't started working on the dimensions for assessing the project. But one of the criteria for evaluating the project will be whether you use a workflow orchestrator or not. So if you decide not to use a workflow orchestrator, you will not get points for that criterion. Maybe you'll lose two, three points. We still need to finalize that, but that's the idea. It’s better to use it, because in real life, you will need to use an orchestrator. It also depends on what exactly you mean by “the rest”. Maybe if you can give us a bit more details, let's say in Slack, we can answer it. But yeah – it's better to use it. 
Victoria
It’s probably also worth mentioning something that we mentioned before that they could use. They could use another scheduler, if they don't like Airflow. Just make sure to clarify that in your project details, so that the person that will evaluate your project in the peer review will be able to understand what you've done. They may not have experience with that particular scheduler. But that could also be an alternate way if that's what you find easier.",Data Engineering Zoomcamp,2022,
841445,"The project that we will have next week will be something very similar to what we did in the lectures and what we did in the homework. You will need to find a dataset – we will propose some datasets, but you can also find the dataset yourself. Then you will need to explain what kind of thing you want to predict. Should it be regression or should it be classification – what kind of problem you want to solve. And then you need to do some sort of exploratory data analysis, do some feature importance analysis. Then you will need to, of course, do some data preparation. And then you will need to train a model, actually multiple models – because now we're on week six, we will learn how to use tree based models. 
So, no matter if you're solving regression or classification problems, you can use a linear model, which is linear regression or logistic regression. Or you will be able to use a tree based model, like tree decision, tree classifier, or decision tree regressor, for example. So you will need to try multiple models and then deploy this at least locally. This is the rough outline of the project. That's the main idea. You learned a lot already. You learned how to do basically all that we covered so far. Now it's time for you to do this yourselves, without guidance – without just repeating the videos. For this project, I want to do peer reviewing. You will also need to review the answers of your peers. This way, you’ll also learn from them.",Machine Learning Zoomcamp,2021,
404476,"Victoria
Every table is a materialized table. Isn’t it?
Ankush
Exactly. We answered that in the previous question about external tables. I think that's what the meaning from this question was, basically, external tables versus internal tables.
Alexey
Okay. There is also the concept of a materialized view, which is when you have a view and the view is nothing but a SQL query that is executed every time you want to do something with a table or with a view. You're kind of creating a query. The way to speed it up is – you materialize this view, meaning you create a table with all this data, and then instead of querying a query, you query the actual data. Did I get that right? [Ankush and Victoria agree]",Data Engineering Zoomcamp,2022,
36798,"Alexey
I think it's helpful. It's not necessary, but it is helpful. Because what happens is we have these DAGs that we run in Airflow. In DAGs, you have tasks and these tasks are often Kubernetes jobs or something like that. So you have some infra – in many companies, this infrastructure is managed by Kubernetes because it's quite simple to get a new container and execute a new job in Kubernetes. So I don't think it's necessary, but I think it's helpful. 
Then again, I'm not a data engineer – I'm a data scientist. But for me, as a data scientist, knowledge of Kubernetes was quite helpful because I could debug all the jobs that I'm running on Airflow (that run on Kubernetes) and I could see logs and do all that. I would say that if you want to learn it, it will not hurt. You can get hired without Kubernetes knowledge. With the materials that we have in this course, it should be sufficient to get hired and then you can pick up Kubernetes at work, for example. Do you agree, Victoria? 
Victoria
Yeah. I think Kubernetes is getting more and more popular, so it can't hurt.",Data Engineering Zoomcamp,2022,
774098,"Yes, you can.",Machine Learning Zoomcamp,2021,
460382,"I do not know that to be honest. The first thing you probably need to realize, especially when it comes to matrices, is that all these things (matrix multiplication, for example) everything can be expressed with Python code, or with any code. Once you see that, then you can just take a formula and try to decompose it one by one and try to see how it translates. I don't know if this actually answers your question, but there is no better way than just try and translate. 
What also helps is, when you have a reference implementation, let's say when we were implementing matrix multiplication in plain Python, we could compare the results with implementation from NumPy and see that the results are the same. So if you have some existing implementation, you can compare your implementation with the reference implementation. That probably helps.",Machine Learning Zoomcamp,2021,
942864,"Victoria
Around the tool, specifically? So where is DBT going, or…? How do you read this question?
Alexey
I think it's about the tool, not the company, right?
Victoria
Yep. So what will be in their roadmap or something like that? What would I expect to see for them? Or us in the project?
Alexey
Probably for them – where is the tool going? What is next for the tool?
Victoria
That's very interesting. DBT, in one of the first videos, they have two parts. Basically, one is the DBT Core and that part is open source. That's the part that does the whole magic. And then they have DBT Cloud. I think their next steps are going to be around developing DBT Cloud, because that's where they make the money. The thing would be in DBT Core open source, is that they literally provide their information to the competition. So then, they would try to DBT Cloud stronger to compete better. I would say that. 
Then, more specifically, things that they recently introduced and I would really like to see would be more around data cataloging or data governance. They recently introduced something called Metrics. You could actually define something like utilization rate or net revenue or stuff like that – how that is calculated and where that comes from. 
That's a big problem in companies. It's very, very common that everyone has their own spreadsheet. They're like, “No, no, in the calculation that revenues is this.” And numbers change. So treating more data as a product, in general, I would say would be a next step as well. That said, this is my opinion.",Data Engineering Zoomcamp,2022,
234073,"Alexey
For that, we will ask for your help, because it's very difficult for us to prepare the materials and also create a curated list of frequently asked questions. So if any of you can help us – you're already doing this in Slack by answering questions – but if you can help us and do that, we'll much appreciate that. But I am not sure if it's physically possible for us to also invest time into that right now, at least for me. I'm not sure I'll be able to do that. 
So please, if you want to do this, we will be happy to support you. You can just create pull requests, for example, for our repo with a Frequently Asked Questions section. Or, for example, in the readme – if there is a place where many of you have errors, maybe you can create a pull request saying, “If you have an error like this, please do that.” It will be helpful for many people.
With Slack, I think it's a bit overwhelming right now with the amount of questions there. Many of them are repetitive, so I kind of wish we had a list with frequently asked questions as well. Then it could be easier to direct it. It's just that putting it together requires attention.

Sejal
Another thing I was actually thinking about is that maybe we can create a GitHub readme page of FAQs and people can just create pull requests to add questions there. In cases where there are pull requests with repeated questions, we can just mark it as stale because they can already go and look over there. On the other end, as Alexey said, since we have a big time crunch and resource crunch – we are swamped with preparing the material as well as managing our full-time jobs. 
We're trying to do as much as we can, but if someone can volunteer to prepare maybe a Notion doc, for example, and organize all the questions that are asked in the Slack channel, and then put it all onto that in a Notion doc or Google Doc, whatever you like, that will be very, very helpful for us.",Data Engineering Zoomcamp,2022,
75919,"Victoria
SQL is definitely super important. It will depend, as in any other role, on what the company is going to do. I personally don't really use much Python, outside of maybe some API scripts. But SQL is definitely the one I use every day and a lot. It could be very good if you're at least familiar with Python and can write with it. Then come data modeling concepts, because keep in mind that you have to focus a lot on transforming that data to be lighter use. Then it's very important that you know how to transform the data. Then, you would need to know some BI – being able to expose that data later on to some BI tools. 
This doesn't mean that you need to focus on building dashboards or things like that, but at least know how that data could be used from a business perspective. Once you have that, then it’s also important to know concepts around data warehousing – at least know the parts of the ETL, ELT, all of these concepts. Know how the data gets to the data warehouse and be able to understand that to kind of cover both parts. It would be very important.",Data Engineering Zoomcamp,2022,
571709,"That's out of the scope of this course. But there is a good video on our channel, called A/B Testing from my former colleague, Agnes. Agnes talked about how we use A/B Testing at OLX. It also goes into hypothesis testing a little bit. But we will not cover it in this course.",Machine Learning Zoomcamp,2021,
859650,"Alexey
I think it is. We still have two days to finish your first homework, although maybe it could be a bit challenging, considering that some people already spent 10 hours last week. If you need to catch up, maybe take it a bit easier. You can join now, but maybe don't rush and try to do homework 1 and homework 2 now. Again, I want to say that, at the end, when we say that you have finished the course, we look at the project. If you completed the project, then we considered that you completed the course. So don't stress too much about the homework.
Ankush
I think it's definitely doable even if you start now. It's still doable. I would also encourage you to look into the topics and if you see that, “Hey, you know what? Terraform is not something I want to learn right now.” Maybe you want to skip that. It’s the same for Docker and the same for any particular technology you think you are comfortable with – you can skip those videos for now if you are already running late on those topics.",Data Engineering Zoomcamp,2022,
619852,"Alexey
Let me go see the criteria. Here, we have one of the criteria, which is cloud. You get zero points if you do not use cloud at all. You get two points if you use Cloud and use UI for setting up things. And then you get four points if you use Cloud and you use infrastructures as code tools. So yes, you can use UI. You don't have to use Terraform or anything similar. But if you do use that, you will just get two more points.",Data Engineering Zoomcamp,2022,
223605,"It probably can. To be honest, I'm not a big fan of test-driven development, in a way. I mean, tests are important. But the way TDD works is that you first come up with a test that fails, then you implement things and then you fix it (make the test green) and then you have this iteration. I don't find it super useful, nor do I like it. That's just my personal preference. I usually implement something and then I test it. But those are more like general software engineering practices. If you’re talking about test driven development for machine learning in the way that you first write a test and then implement something – I don’t know. If you’re talking about the model and then you say, “Okay, this model has to have this kind of accuracy,” you train a model, and then it's not the way you expected, maybe it doesn't make much sense. 
But on the other hand, there is a lot of code around the model that needs testing. For example, all these things that I showed today – all of them may fail or behave not in a way that you expect. Having tests around that, I think is quite important, if you want to make sure that you have things under control. For example, this could actually be a good test. You have an example, you transform it, and then you want to make sure that this neighborhood (Fordham) doesn't appear in your categories. This could be a good test, maybe. Actually, it makes sense to write tests – not for the actual machine learning thing, not for the model – but for all the things around the model. There is a data preparation pipeline, so you probably want to test that. If you get the results and you do some post-processing of these results, you want to test them. But testing the model itself could be tricky.",Machine Learning Zoomcamp,2021,
245146,"Sejal
Yes, there is but, again, I am not really a Kubernetes expert. Maybe Alexey can help.
Alexey
If we do a bit of Googling, we can see that there is a Kubernetes operator. That probably answers that. But yeah, there is an operator and you can use Kubernetes for that. It will internally, I think, create a Kubernetes job and it will just execute. What we typically use is not Kubernetes operators, but we use AWS Batch operators, which is essentially the same thing, except you're not running Kubernetes on AWS Batch. From what I see, this is usually the pattern of how exactly you use Airflow. So you do not use Python operators or bash operators – you do not execute these things on the workers – you usually make workers pretty dumb (they don't have a lot of resources) and they instead delegate to some external compute environment like Kubernetes, AWS batch, maybe ECS, or alternatives on Google Cloud, or Azure. 
You state how much resources you need for this specific job and where you want to run it, what the Docker image is, what the parameters are, and they are executed somewhere – not on the worker. This is the pattern I see. Or, usually, it's either SQL operators that you execute on BigQuery, or Athena, or Presto, or something else, or these kinds of AWS batch Kubernetes jobs, or Spark, for example.",Data Engineering Zoomcamp,2022,
478055,"No. Like, as I said, it's always problem- and data-specific. For some datasets, you just cannot get very good performance",Machine Learning Zoomcamp,2021,
384801,"Alexey
Google form works fine. Don't worry about that. If you clicked “submit,” then it worked and it was accepted. But I guess your question is about your scores and you want to see if your answers were correct. For that, we will show the solution on Wednesday. We'll just upload the video with the solution and there will also be some sort of leaderboard. Maybe I can actually show you because we already did something similar for the other course. There is a leaderboard that shows how many points you get for each homework. We don't show your email here, of course, it's a hash of your email and then you see how many points you got for each question. As a result, you get a leaderboard with the total amount of points. You will know soon which of your answers are correct or not.",Data Engineering Zoomcamp,2022,
152693,"I don't know if it's easy or not for you. It's up to you. If you have a job, then I think it's difficult for you to invest time. You know the minimum requirements and just work from these minimum requirements. The minimum requirement is that at the end, you have a web service that is deployed in Docker, that you try multiple models and you also have a description of your project. Basically, if you do all that, you're good. If you want to spend more time – do it, by all means. If you satisfy the requirements, you will get a good enough score, don’t worry. I will also ask how many hours you actually invested, because it will be quite interesting for me to see how much time people spent. I see that some of the students spend like 20 hours on some of the materials. When we do other iterations of the course, I will already know how much time people will need to invest. So when you submit your homework, please tell me how much time you spent on this.",Machine Learning Zoomcamp,2021,
194806,"This is when we have classification. In classification, let's say if this here, we weren't predicting the price, but let's say we were predicting if it's above average number of hours. [image 1] In this case, we would need to have another layer here – or rather not a layer, but it's called activation. If we don't have this activation, we're still… I’m trying to formulate it in a way that it's not confusing. But I think it's better if you just go to the lecture and rewatch it, because I’m essentially just retelling the same thing. Here, we do not need from_logits, because we have the price variable. Our loss is a mean squared error. There are no logits. But if our loss is categorical cross-entropy, and categorical cross entropy is usually used when we have multiple categories, then this is when we need to use logits, because it's numerically stable. 
We don't have to use it – we can stay activation softmax and then we don't use legit. I hope it's clear. For regression, we don't need to use it. For homework, you will not need to use from_logits either, because in the homework, we have just two classes. It's a binary classification problem. But when we have multiple classes – let's say three classes, or four classes, or five classes – this is when we need to use logits, because then the loss we have is categorical cross-entropy. When the loss is categorical cross-entropy, then it's recommended to use from_logits=True because it's numerically more stable.",Machine Learning Zoomcamp,2021,645047_qa9_ml_zoomcamp_office_hours__week_11_pic1.jpg
565439,"Ankush
For example, if you're doing a data pipeline for the first time, you might want to consider something like a data lake or a data warehouse solution. That would be your landing zone. But in a lot of cases, what I see is people already start to think in a very wide range and say, “Okay, this is my raw landing zone and then there is my first transformation landing zone and this is my main transformation landing zone.” They also name it “bronze” and “gold”. Yes, you can keep maybe a couple of them. But don't overcomplicate it. Don't keep like seven of them because that will just be overkill for your first pipeline.",Data Engineering Zoomcamp,2022,
416472,"Alexey
Maybe I would suggest just to start applying and then see what they ask you. What we’ve covered is already sufficient to start working, in my opinion. Of course, we already expected that you know some SQL. Becoming better at SQL is always a good idea. Python coding is also helpful. I think that the best thing you can do now is just to start applying and then you will either get a job immediately or you will get some feedback from companies. You will see, “Okay, it seems like they need this thing, but I don't know this. Let me improve this.” 
Victoria
I feel the same way. If you start applying, just know that you're going to get rejected, but you'll get a good idea of what you're missing. Have in consideration that every company has their own expectations and different things, but you get a sense of where you stand. Try to ask for feedback, as well. Especially if you get to the home assignment challenge – if you get to the technical interview stage, that would be super helpful to get you oriented. People usually are nice, I would say. You can get to the technical interviews. I think I had people that were nice in the technical interviews. You actually can have a discussion on the technology and they can even orient you a little bit. So it could be a good resource, I guess.
Alexey
Also, maybe do projects. Instead of focusing on, “Should I improve coding? Should I learn AWS?” Let's say instead of just learning AWS, maybe come up with a project and then use AWS for this project. In my opinion, this is much more productive rather than just preparing for an AWS certification. This is a good thing, of course, but when you focus on the project, you focus on the things that you really need. Then you have something at the end rather than just AWS knowledge. You get something that you can put on the CV and then people can go to your GitHub and check it out. I would recommend doing projects instead of just learning things.
Victoria
It's also more fun. You have a lot of troubleshooting to do when you do a project, plus you have motivation – you have to analyze the stream, or prepare data or stuff like that. It's definitely more entertaining as well. You get to know a bit of everything, which are things you've got to do as well when you're working.
Alexey
There is a thing called “just in time learning” where you have a problem and you need to focus on solving this particular problem. Then you start learning what exactly you need for that. Projects are cool.",Data Engineering Zoomcamp,2022,
163456,"It's actually a good question. There are datasets like Titanic or IRIS or Wine Quality dataset, Boston Housing is also a good example because of the ethical issues. There are datasets that are just so common that I don't think if you use them, you will learn much because there are so many tutorials out there. I will have a list of datasets that I do not suggest to use. If you really want to predict if somebody will survive the Titanic catastrophe, go for it… I'm trying to say it’s a boring dataset but besides that… people actually died. 
Anyways, just go to Kaggle and pick a dataset and you'll be fine.",Machine Learning Zoomcamp,2021,
153957,"Please use Flask. But if you want to use Django, you can use that as well. Just be clear when explaining what exactly you're doing so that others can use and test it. Ideally, as I said, everything you do should be a Docker image at the end that people can just run and test it. This way, they don't really have to know if there is Django or if there is FastAPI or if there’s Flask. It shouldn't be a Docker image that is runnable.",Machine Learning Zoomcamp,2021,
347810,"I think I did recommend a course once. You can watch a course on Coursera by Andrew Ng. It's called machine learning. It covers theory quite well. The theory is also on an intuitive level – the math there is comparatively less difficult when compared to textbooks. Then, of course, you can get textbooks. A good textbook is Elements of Statistical Learning. It’s really good – there is a lot of math. I think there is a book called Pattern Recognition, or Machine Learning Probabilistic Approach – that's another book. One is Bishop, and for the other one, the author is Murphy. They're very mathematical. There is a lot of math. You really need to have a couple of years of calculus in your background to be able to understand that. But if you don't have that background, the course by Andrew Ng is good.",Machine Learning Zoomcamp,2021,
588990,"Alexey
They are not important. I hire people and I have never asked anyone to show me their certificates because I don't care. Usually, you have an interview and if you see that the person you're interviewing knows something, then they know something and you don't care if they have certificates or not. I wouldn't say it's important but maybe it's also nice-to-have thing. It gives you some sort of achievement, but I don't think you should stress too much about that. Just take the course at your own pace. The most important thing here is learning and not whether you get a certificate at the end or not.
Ankush
I would like to add to that. Whatever Alexey said is absolutely correct. It's more about learning, it's not about the certificate. But in case you are a fresher and you are starting your first job, then I don't know if it would be helpful, but it might be helpful. Think of it like this. If you do get a certificate, put it on LinkedIn, so that people who review your profile can actually see that you have it.
Alexey
It will also serve as an advertisement for our course, right? They will see “Okay, Data Engineering Zoomcamp? What is that?” Then they will click on this thing and they will find this amazing course – they will know that the course is amazing and they will immediately trust that you learned something and they will maybe hire you.
Ankush
That was not the direction I was going into, but yeah – why not? [chuckles]",Data Engineering Zoomcamp,2022,
924163,"Alexey
I think the question is how we can actually make the results reproducible in neural networks. Every time we actually create a layer, it gets initialized with random weights. This actually does happen.
Dmitry
We cannot talk about full reproducibility here because of the stochastic nature, and there will always be some fluctuations. The question is how severe they will be.
Alexey
For neural networks, it's very tricky to make sure it’s reproducible. That's why the homework for this module was also a bit tricky.
Dmitry 
Regarding the answer 0.45. It was a bit strange. You mentioned that there was the answer 0.45. 
Alexey
It is strange indeed. I think the main reason for that was because there was no shuffle. [Dmitry agrees] When I added shuffling maybe some people submitted it before I explicitly wrote it in the instructions that you need to shuffle. That could be the reason as well. Let's say you're evaluating it, and then you just get a bad batch with only dogs. Then maybe for this particular batch the accuracy is not great. It can happen.",Machine Learning Zoomcamp,2021,
238126,"Ankush
I don't know the answer to this because I only tried it with CSV. I didn't try it with parquet. The thing is, with CSV, it automatically detects it as date time. And with parquet, the date fields are converted into byte arrays. I think I've seen this issue multiple times on Slack. I have not sorted it right now. What I can suggest is that, I can try it, I can maybe then put the solution on Slack.
Alexey
I think the problem with this one is because you already have schema embedded in your parquet files, right? Then if BigQuery sees that in one parquet file, you have one schema, and then in the other one, you have a different type, then it doesn't know what to do with this. Right?
Ankush
Yeah, that can definitely be the case.",Data Engineering Zoomcamp,2022,
594349,"Alexey
I think this is one of the topics that relates to the very first question regarding what is important, but outside of the scope of the course. These data governance tools could be one of those things. I don't know if they're data engineering per se. They're related to data engineering, but I think they are more about general data governance – how you organize data and how you manage data in your company. This is definitely an important thing, but I don't know which of us can actually put something together for that. Does any one of you know anything about OpenMetadata, etc.?
Victoria
I work a lot on trying to select a data catalog but haven't used OpenMetadata or anything like that. I would suggest, if you have the time, yes, try it. It's also something, as it is quite new, maybe not as advanced as it should be. But judging by the size of the modern data stack and all of that, it's probably not something that any company will ask you, so don't feel pressured. What I'm trying to say is, yes, it's very important – it would be quite nice if you can read about it, understand the concepts and all that. If you have the time to try OpenMetadata or any other open source data catalogs that are out there, that's great. Do not feel the pressure to do so. You'll probably get to do it. There's also more on the data engineering side, I would say. Probably like a Glue crawler or something like that. That's also maybe what the data engineer would participate in regarding the data catalog. We also kind of saw a data catalog from the DBT part, but that only covers that part. It's a bit more than that, if you think about the whole pipeline that everyone made for the project. Maybe we'll do something for the passwords, if we have the time.
Alexey
We just use an internal data catalog. It's pretty convenient. I don't think I'll be able to record anything about that. But actually, maybe it's a good idea to invite somebody to do a webinar about that. If any of you knows someone who did talk about any of these tools, maybe you can connect us, and then we can have a webinar about that.
Victoria
You've had an episode about data governance, right? A long time ago?
Alexey
Yes. 
Victoria
I can't remember the name, but they had a book.
Alexey
Yes, they do. They have a book.
Victoria 
And I have this book. I read it during my project around selecting a data catalog.
Alexey
But this one is more like a discussion about why we need data governance and a data catalog. I think one of the points here is that data governance is just about having a data catalog. This is a nice episode, where it's more theoretical. If you're looking for hands-on practice, this is not the right place. But this is definitely an interesting place to at least start to understand what data governance is. Yes, they have a book, so check it out.",Data Engineering Zoomcamp,2022,
700463,"You can take it at your own pace. You can just follow it from the very beginning. And if you have any questions, just ask us in Slack. I guess that's the best approach. Or if you're a just-in-time learner, you can just go ahead and start working on the projects. The next project we'll have will be a capstone project. You can start working on the capstone project right now and just figure out what you need to learn to actually do the project. It’s up to you. The way I imagined it in my head – now it's, we're doing this together, but all the videos stay there on YouTube forever and anyone can join it at any time and follow the videos, do the homework, and ask questions.",Machine Learning Zoomcamp,2021,
567765,"Yeah, this is tricky. I know. I saw a link in Slack that somebody posted about best practices of how you can actually do that. That's one of the reasons I usually don't use Conda in Docker. I like Conda – I love Conda. I love Anaconda. I have them installed on my laptops for local development. But when it comes to deploying things, then yeah… There are advantages and disadvantages. But I usually use something like Pipenv to manage the environment, and I use Conda for local development. Because I can just install Anaconda and have all the libraries I want on my local computer and then I just train a model there. When it comes to productionizing, I create the environment file and put all the scripts there. That was a long answer. The short one is just go to Slack and look for the link with the best practices for putting Conda in Docker.",Machine Learning Zoomcamp,2021,
173078,"Multi-classification – yes. Clustering – I'm not sure, probably not a good idea because we don't cover it here. It will be hard – you will not know how to evaluate it and your peers will not know what to do with it. Better not to not do clustering.",Machine Learning Zoomcamp,2021,
676681,"Yes, we are.",Machine Learning Zoomcamp,2021,
704244,"Victoria
I would say it’s also because there's a different set of products. For example, for a data warehouse, if I were to start a data tech stack from scratch, then I would definitely use Redshift because the cost is lower. But then, in the end, when you need more, you probably would need to migrate to something like BigQuery or Snowflake. For that, they have a lot of documentation, it's pretty easy to set up, it's been around for longer – I think the reason why it has a broader adoption is mainly around those things.
Alexey
It seems to me that AWS is more popular as well, at least in Berlin. If I look at companies that use some cloud, maybe 70% of people use AWS, 20% may use GCP, and then the remaining 10% use Azure. That's roughly how I see it. Maybe I'm wrong. In this course, I also got to use GCP a bit and it seems to me that the UI is nicer. When you go to the web console, it's a little bit nicer. But I think when it comes to tools, it's a bit more difficult. 
For example, in AWS, you just do a pip install for AWS CLI and you have it. With Google Cloud, it's a bit more difficult. Maybe that's one reason that AWS is more popular – it’s more mature, maybe. But I think GCP is catching up because they have a nice interface and sometimes some things are cheaper. BigQuery is also, I think, a big advantage that maybe other clouds don't really have.
Victoria
Plus AWS has been around for 10 years. I did my thesis using AWS and there was no such thing as GCP yet. There's a lot of people that go with what they're familiar with, outside of the stability.
Alexey
But I think in most of the cloud services, the services are quite similar. For me, I’m more used to AWS. For me, many things in GCP were straightforward and some of them weren't. Some of them were typical, but more or less, I think many concepts can map from one to another.",Data Engineering Zoomcamp,2022,
105368,"Yes, I can. I think I just wrote you the code. [image 2] This is the code I used for creating model2. You can take a screenshot and copy it and then just run – then you will have it. I can also send it in Slack if you want. Just tell me if you need it.",Machine Learning Zoomcamp,2021,39696_qa4_ml_zoomcamp_office_hours__week_6_image_2.png
200161,"It really depends on the cloud. I showed you how to do this with Elastic Beanstalk. I don't think I needed to change much there, if anything. I don't remember changing anything there. So the only thing I changed was the URL for the service. In this test.py script, that was the only thing I needed to change. I think for Heroku, it's similar. Actually, there are a couple of tutorials about using PythonAnywhere and Heroku. They will probably explain what you need to change here. Just go through this and see if you need to change anything or not. Probably not much.",Machine Learning Zoomcamp,2021,
757451,"I think DictVectorizer is the easiest one. This is the reason I thought it would be good to use for this course. Let me show you. This is how you can use a one-hot encoder. I also showed you that you need to do “column stack,” which makes it a bit more difficult than using DictVectorizer. With DictVectorizer, you just throw everything in and you get a matrix out, which I find easier than a one-hot encoder. I'll share this code as well. You can take a look at this and decide which is easier for you – one-hot encoder or DictVectorizer.",Machine Learning Zoomcamp,2021,609079_qa7_ml_zoomcamp_office_hours__week_4_by_dmitry_muzalevskiy_pic1.jpg
342609,"Ridge regression is an implementation of regression. We have linear regression, which is regression without regularization, and then we have regression, which is regression that has regularization. Ridge regression is very similar to what we've had in the previous lesson. It's more optimized. Other regression types (or regression models, let's say) that you can have are tree-based models that we will cover soon. Then there are neural networks, which can also solve regression problems. There are definitely many other models that you can use for solving regression. I think maybe for the week when we will train tree-based models, we will also solve a regression problem using trees.",Machine Learning Zoomcamp,2021,
120784,"For that, the way I want to organize it is: now, everyone is working on a project and then will submit them by the first of November. On the first of November, I will see who submitted the project and then from these people, for each of the projects, I randomly select three people to review it. This means that each one of you will get to review three projects. I will send an email (I hope it will work, I still haven't written a script for sending the emails) saying, “Hey. You will need to review these three projects. Here's the form. You will need to put this hash there and you will need to put your assessment there.” You will have one week to do that.",Machine Learning Zoomcamp,2021,
759839,"Well, to be honest, I don't know how many hours of daily study are recommended. That's why I added a question about that in the form. I will share the results later with you about how many hours it usually takes on average. But again, the answer depends on your background. If you're comfortable with coding, then maybe you don't need to spend that much time compared to if you're new to Python, for example, because then you would need to put some more effort in. 
As for the numbers, at the very least, maybe you want to watch lectures first and then spend one or two hours on the exercises. Maybe in total – three, four hours. Again, it depends. I don't really know. As for “How to study continuously without a long break?” Maybe you shouldn't study continuously without breaks. I don't think it's good for you. So it’s better to have breaks.",Machine Learning Zoomcamp,2021,
993199,"Alexey
The purpose of this method is, let's say you have this… Well, I'm going to say RDD, but they haven't released videos about RDD so I'll try to explain briefly. RDD is a distributed dataset. Let's say you have a usual python list, this is the same, but this is not a usual Python – this is a list that is distributed across many machines or data frames. Data frames are also a sort of distributed collection that we have, which consists of many partitions. Then you do some operations on this data frame or on this RDD, you invoke this .collect() to get everything that this data frame or this RDD contains in a memory object. You kind of move to think you have (a distributed collection) and you turn this into a collection that you have in your Python driver node. Maybe that was a long explanation. Maybe you have a shorter one, Ankush?
Ankush
If you want to count all the elements, you need to put it in to one node to count it. So that's where you will use .collect(). But do remember, if you use .collect() on large datasets, you will also run out of memory. Be careful of using it and be aware of what you're doing. It's really helpful if you want to collect and then write to one file. Then that can be helpful. But be careful while using it.
Alexey
So this is a transformation. When I was talking about actions and transformations, most of the operations in Spark are transformations, but some of them are actions. As an example of actions, I talked about “show,” I talked about “take,” I talked about “head,” and I also talked about “write”. But “collect” is also an action. It triggers the entire execution, but then instead of saving it somewhere, you get it as an object in your driver node.",Data Engineering Zoomcamp,2022,
94614,There are no guidelines. I do not have any resources. Just write something. It's the same way you write a document – just use markdown for that.,Machine Learning Zoomcamp,2021,
477416,"Victoria
Yes. Don't you think so? I received a bunch of LinkedIn messages from people offering me to outsource engineers. I don't know where from, though. 
Alexey
They approach you, but it doesn't mean that companies actually agree.
Victoria
Oh, yeah. I don't know who agrees. I've never agreed so far. I’ve never even talked to them. There must be someone if an agency is providing that service, right?
Alexey
I think that when it comes to data, it's a little bit trickier than with basic coding. The data can be sensitive, then you need to have all the right things in place. First of all, you need to have a lawyer that would put a proper data regulation contract. Then you also need to have an infrastructure that allows you to maybe not send sensitive data to these outsourcing partners. 
I think it becomes a bit trickier than with, let's say, traditional software engineering. On the other hand, with traditional software engineering, you also get to see things like phone numbers, emails, etc. Maybe we will see something like this. I see that, for example, data science is often kept in-house rather than outsourced. Something similar probably still happens with data engineering. I don't know. 
Maybe we'll see this in one year. It changes. But people also reach out to me saying, “We have this great team in Minsk, Belarus. We want to start working on your projects.” And then I have to answer them saying, “Sorry, we don't need your services right now.” 
Victoria
Yeah, same. 
Alexey
I was probably assuming too much about the distinction between data engineers and software engineers. I remember that when I was in Poland, I worked at a bank and it was actually an outsourced sort of team. We didn't see personal data – we had some sort of masked data – but we still needed to write code that would deal with all this sensitive data. So in this aspect I think there are ways to ensure that data is not leaked. Maybe we just need to wait a bit until this area gets more mature and then we'll see more data engineer outsourced work.
Victoria
I’ve also seen that in a company that came from Ukraine. They were in Ukraine and they used to come to the office once a year or something like that. They had complete access, but they had to sign something.
Alexey
And they also needed to come to the office, right?
Victoria
Yeah, once a year for a week. I guess it was to have meetings.",Data Engineering Zoomcamp,2022,
317865,"Alexey
Yeah, I think they're helpful. Not all of them. For example, machine learning engineering often requires building some data pipelines for the model. Sometimes it's done by data engineers and sometimes it's done by data scientists. Sometimes it's done by… basically everyone can take part there. 
If a team doesn't have a dedicated data engineer, somebody still needs to do this – a machine learning engineer or a data scientist –knowing tools like Airflow, Spark, maybe DBT, is quite helpful to prepare the data in the form that is useful for machine learning models.",Data Engineering Zoomcamp,2022,
924360,"Alexey
This is our Slack. You can see the default channels that we have here. For any course-related questions, you should go to the #course-data-engineering channel. You need to click on the plus icon, then “browse channel” write “course” in the search and look for the relevant course channel (#course-data-engineering). You can just click on “join” and you will have it here. This is the channel that you should use for asking course-related questions. 
We have a lot of other stuff happening in the community. For example, #welcome is where you introduce yourself. I think some of you just did that. This (#course-ml-zoomcamp) is the channel for the other course we had. You just click on this, browse channels and you can see what we have here. For example, we have a channel about engineering, which is about discussing engineering things. We have a channel about data science. We have another cool channel that is called #book-of-the-week. We invite book authors and they come and answer our questions. You can check this out as well. 
To register in Slack, if you still haven't done so, you can go to DataTalks.Club and just leave your email here on the main page where there is a prompt to do so and click on “join”. You will receive a link. If you don't receive a link, reach out to me through Twitter or LinkedIn – whichever way you prefer, and give me your email. I will use a different way of adding you to Slack.",Data Engineering Zoomcamp,2022,
425941,"Dmitry
Usually I use the NumPy function. 
Alexey
I think also the question is, “Do we need to transform the y variable in any way apart from this logarithmic transformation? Are there other transformations that we need to know?”
Dmitry
I can think of the Box Cox transformation. There are certain types of transformations of the target variable, especially in the regression tasks. You have to make it more randomly distributed, for example. For sure, you can leave it like that, but the results, especially in a linear model because linear models really depend on the linearity, or the normal distribution of the target variable, so the results can be not very good. 
For sure, throughout the course of the Zoomcamp, we will talk about the nonlinear methods such as end symbols, for example, end there. You can see that, for example, you can use it without transformation – using the trees.
Alexey
Yeah, thanks. “Cox Box transformation” you said? [Dmitry agrees] I think this one turns any variable into a normal, right? [Dmitry agrees]",Machine Learning Zoomcamp,2021,
602439,"Alexey
I’m guessing this is some sort of managed solution for doing ETL. If it is, then the advantage is that you don't need to manage Airflow. For Airflow, you need to host it somewhere, somebody needs to look after it, if your DAGs don't run, then somebody needs to go there and check why they don't run. It could be you. Do you want to do this or not? If it's a managed solution, then all these headaches go away. But I don't know how flexible it is. So that's the advantage. But there’s also the disadvantage that Airflow can do a lot, while Azure DataFactory might be quite specific to Azure. Maybe you cannot do things that are outside of Azure. But I'm just speculating. I've never used it.
Victoria
Actually, it turns out that it's the cloud version of the information service. I think it is from SQL. I've used it in the past and I would say I'm also not a super-expert Airflow, but it's definitely flexible. I'm guessing it's better now in Azure and all of that, but the problem there is that you need to have the SQL Server and all of this. You're limited in the sorts of things you can use, you're limited in the things that you can do. It wasn't super easy. You don't have a DAG – you're gonna generate it yourself. You can generate all these steps but it's not that clear, at least from my experience in Airflow and then using SSIS.
Alexey
Azure DataFactory is something similar to SSIS, right?
Victoria
Yeah. It's like the cloud version, kind of. It says that it's looking to modernize SSIS.
Alexey
I remember the first time when I saw this SSIS, I was like, “Why did I spend all this time coding things in Java if I could just drag and drop things and connect them with arrows and it just works?” The answer is that it just doesn't always work. Sometimes you need some flexibility and that's why you need Java developers who would go there and fix it.",Data Engineering Zoomcamp,2022,
254116,"Yes, I think this is what they showed you. I'm not sure what you mean. But yeah  I think it's a good idea to do this.",Machine Learning Zoomcamp,2021,
268864,"Nothing comes to mind, but maybe if you're uncertain about something, please ask.",Machine Learning Zoomcamp,2021,
728436,"Maybe you're referring to this plus here. [image for reference] Let me maybe say it once again. First, when we compute the mean without missing values, we use only that part of the array (the number values). The formula for that is Σxi. But now, let's say we include the missing values, and we fill these missing values with the mean x̄. Now, instead of just having n elements, we have k more elements. In total, we have k+n. So that's why you have k+n here. That's how you have the plus here. Because first, you need to sum these elements and then you need to sum these elements. So that's why you have this sum here. I hope I understood the question.",Machine Learning Zoomcamp,2021,944834_qa6_10.jpg
207316,"Maybe? I don't know. You still need to have some names. I think the question is referring to this part here. [image 1] So you need to know that 1 means “ok” and 2 means default. You need to know that 1 means “rent” and 2 means “owner,” “private,” and so on. So if you have this information somewhere, it can be JSON, or CSV file, whatever. So if you have some information somewhere, then you can just load this. This could be stored in a JSON file. And if you have that, then you can just use that. But again, you need to know that 1 stands for “rent,” 2 stands for “owner,” etc. If you don't care about this, machine learning. models don't care – they don't care if it's “rent,” “owner,” or if it's 1 or 2 – for this case, you can just turn this home variable into a string by using something like that. [image 2] Then it will stay encoded as a number, but it will be a string, and when you use a Dictionary Vectorizer it will treat it as a string, not as a number. I hope that answers the question.",Machine Learning Zoomcamp,2021,"322580_qa2_ml_zoomcamp_office_hours__week_5_pic2.jpg,322580_qa2_ml_zoomcamp_office_hours__week_5_pic1.jpg"
703640,"One thing that immediately comes to mind for me is click prediction. Let's say you go to some website and there is a banner there. What happens behind this banner is that there is a model that predicts “What is the probability that this particular user that comes to this webpage (meaning you) will click on this banner?” Usually, there is some sort of linear logistic regression under the hood because they are pretty fast. For advertisements, they're used quite a lot, especially when we talk about real-time advertisements. When you enter a website and you immediately get an ad, this is where linear models such as logistic regression are used.",Machine Learning Zoomcamp,2021,
250778,"Alexey
Individual. With groups, I think it becomes trickier.
Victoria
Yes, but I guess you can always discuss it in Slack. Someone else could be using the same dataset.
Alexey
Yeah, I think that becomes a group project in this case, right? If the dataset is the same, but you have different use cases at the end.",Data Engineering Zoomcamp,2022,
355453,"Ankush
This would, again, really depend upon the kind of job as a data engineer you will be doing. If you're doing an analytical data engineering job, or an MLOps machine learning data engineering job, backend development might not be a very interesting topic or the knowledge of backend development might not really help you. But if you're doing data engineering in terms of data products, maybe doing a lot of real-time streaming and working with a lot of microservices, then this becomes really important and really interesting, because now you can use or combine your knowledge of backend development and can now maybe use NoSQL technology to build a data product. You can use Kafka to do stream streaming into your data product. In those cases, the backend developer knowledge becomes really helpful and you can really excel in your job because of your background. On the other hand, I would also say that backend development knowledge, if you are learning in terms of SQL, and connecting sources or microservices, will help you in the future anyway, because that's the future we are going towards as a software development group. I think there's a saying, “No knowledge is lost.” So there will never be the case that you cannot use your backend development knowledge, but you would use it more in terms of if you are building a data product. 
Alexey
Maybe then we should ask ourselves, “What actually is backend development knowledge?” Are we talking about general software engineering principles, or are we talking about how to use a specific microservice framework? Maybe you will not need this microservice framework for data engineering, maybe you will – who knows? You never know. But the general software engineering principles and practices apply for any engineering job. That will be useful, for sure. I think the skills of backend engineers will translate to data engineers, machine learning engineers – any sort of engineers.",Data Engineering Zoomcamp,2022,
908368,"Alexey
I feel very sorry for you if you need to deal with that at work. [chuckles] I don't really have experience with that. 
Sejal  
I used to when I was working with Oracle but it was like a long time ago. I’m assuming the person that wrote this question is still working with some legacy databases like Oracle. I really don't have an answer to this, either. [chuckles] I guess I'll just talk in more generic terms about CI/CD pipelines and wherever you're deploying your infrastructure, basically, or wherever you're deploying your schemas. I think that would be a better place to keep these things in.
Ankush
Maybe using Git?
Alexey
I think the issue here is because the code is stored on the database itself, so you cannot easily version control it. Somebody, at some point, thought it was a good idea to let databases run some arbitrary code, like triggers?
Sejal
Possibly Victoria would be able to answer that in case there is any coupling of CI/CD pipelines with DBT pipelines? I don't know.
Alexey
I think, usually what people do these days is try to extract this logic and put it into your backend – move this from the database and put it to the backend, where it can be version controlled.",Data Engineering Zoomcamp,2022,
522491,"For this one, I think it's just a good way of checking feature importance. There is no mathematical/intuitive reason behind that. Let's say, before you even train your first model, you can do that and see if there are any features that already give you good separation between positive and negative classes. If you remember about the attrition of AUC interpretation, that AUC is the probability that a randomly selected positive example has a higher score than a randomly selected negative example. 
Here, the score can also mean a feature. Here, it’s if a randomly selected positive example has a value of this feature higher than the randomly selected negative example. If you think about this, then it probably makes sense to try this. Let's say, for seniority – it was a negative correlation, so maybe it's a little bit more complex. But you can already see what the important features are, and maybe some of these features are already good enough, so maybe you don't even need a model. You can just use this feature as your prediction, roll it out, and then in your second iteration, do a proper model. So that's why I think it's a good idea. And that's why I added this exercise.",Machine Learning Zoomcamp,2021,
141765,"Victoria
I haven't used it with Anaconda. I think it's also in the documentation, but just install DBT locally in general, and if you're using Anaconda and you're using it in your environment, you can install it there. You can just use Homebrew if you're using a Mac, or you can also use pip install. Then you just do pip install DBT Postgres, for example, if you need that adapter or DBT Snowflake or DBT Big Query – whatever you want to use locally. And then that's it, you're installed. Then you have to set up the profiles.yml that show in the videos, create your project, or clone it or whatever, and that's it. That's all you need to use it locally. I never install it using Anaconda, but I'm guessing it's probably something similar. You probably do Conda Forge and install it like that, if it's possible. Otherwise, I would just go with the pip install.
Alexey
In Anaconda, you usually have pip.
Victoria 
Yeah. So if you're in that environment you just install it like that and then that's it.",Data Engineering Zoomcamp,2022,
159394,"Alexey
Well, good job. You can just keep working on that. You almost finished the first week, so you can finish it and then start with week two and then finish it. Then there will be week three. [chuckles] Just follow the sequence of videos in the playlist and in GitHub and that should be sufficient, I think. And if you have any questions, go to Slack
Ankush  
Just keep going. 
Alexey
[chuckles] Yes, exactly. You’ve almost finished the first week, which is a good sign, because the first week in this year was pretty tough for many people. We had a lot of problems with Docker and whatnot. So if you're almost finished, this is a good sign. You’re on a good track. Keep working on this.",Data Engineering Zoomcamp,2022,
777459,"Victoria
Data Mesh is one of the topics that we are going to cover. Those are going to be a smaller video. There's no practice around it or anything like that, like in the Weeks. It's more around explaining the concept, so you have a familiarity with it. Then you can deepen the concept if you need to, once you start working, let's say. I have Data Mesh, specifically, and I'll be recording that at the end of this week, once I'm back in Berlin. I'm not sure when we plan to upload all of them, but it should be soon enough – before the deadline.
Alexey
I actually want to actually record one or more videos about Spark, which is something I planned like a month ago. I still haven't done that, so I want to do that this week probably, because I see that some of you have problems with reading from Google Cloud, or running things on Dataproc. So I want to do that first. After that, I will take care of the MLOps video. We will at least have two videos – Data Mesh and MLOps. I don't know what the rest are.",Data Engineering Zoomcamp,2022,
64950,Yes they do.,Machine Learning Zoomcamp,2021,
261747,"Yes, you can use whatever you want. Just be sure you document this.",Machine Learning Zoomcamp,2021,
69511,"Alexey
I'm not sure if the question is about the internals of these technologies. 
Sejal
Maybe I can answer that. If I understand correctly, what they are talking about is the architecture that we have on the GitHub repository. I'm not sure if it represents Docker or Terraform. That is because Docker and Terraform are tools used in order to create your data engineering framework. But the actual data engineering framework – the architecture that you'll see on the GitHub repository – looking at that framework, the main technologies would be starting from this week (week 2) onwards, which is Airflow and Airflow for orchestration. Then there will also be DBT for transformations, there will be Spark for another kind of batch processing transformation-related job, and Kafka streaming. This is the core set of technologies generally used in data engineering and also what we will be covering in terms of the architecture that you’re talking about, if I understood your question correctly.
Alexey
So what you're saying is, the foundational technology is there, but it's not visible? [Sejal agrees].",Data Engineering Zoomcamp,2022,
990905,"Alexey
Let me show you what that is. Again, I will use another course as an example. So we will have something like that as well. We have this public leaderboard. Maybe we'll design it better, because right now it's just very boring – very textural. It's less than 100 people, but these are the people who wanted to share some information about them. So we'll have something like that. I don't know if the question was actually about that. I don't know what the question is actually asking, but this is the idea behind the public leaderboard and I think it will look something like that.
Victoria
I also want to say something with the points – with homework or something like that?
Alexey
The one with the points and homework is anonymized. 
Victoria
I also don't think we could disclose those.
Alexey
Yes, exactly. So it will stay like that.
Victoria
I mean, at least you kind of know, because you can see the hash. So you know who you are. You'll know if you're in first place.
Alexey
But then it's up to you, if you want to go from this thing here, which is a hash, to this thing here, which is your name and your contact and whatever link you want to share. Because when I sent this email to 100 people, not everyone replied. Not everyone wants to be on this page, which I can totally understand.
Sejal
Towards the end, are we going to, at least reveal the names of the top three people on the leaderboard or we are not going to declare that for the same reasons?
Victoria
It's just that we can't say any names, unless there's an approval from them somehow. Otherwise, it goes against GDPR.
Alexey
We didn't ask for permission to share the names when we were collecting this data.
Sejal
Okay, fair enough.",Data Engineering Zoomcamp,2022,
707527,"Victoria
I use all common tools, I guess. [chuckles] But we also use Etleap for the pipeline. Instead of doing Airflow and all that, we load the data basically into Snowflake from the S3 buckets with Etleap. It is not well-known. It's very, very small. It's not I wouldn't say it's my favorite at like ETL
Alexey
I think when I interviewed you about analytics engineering, you mentioned this tool. I remember trying to understand what you were saying and then map it to a tool. I think I spent, like, five minutes doing that. [chuckles]
Victoria
Yeah. In the end, it’s just an integration. It's not well known. I wouldn't say it's my favorite, but it's very easy to use. It does what it needs to do. In five minutes, you have a pipeline. It works. I would say that’s the most non-common tool that I've used. And I really liked MetaBase. I get to know [inaudible] to provide the local version too. I really liked that. I only get to know it for that. But I really liked the idea that you select something like a table and it gives you a lot of graphs and all of the things that I show a little bit in the videos. It was very easy to use. It has, of course, some limitations if you compare it with bigger ones like Looker, for example. It's clear that it's newer and all that, but it works very well. At least that was my small experience – not using it for work.
Alexey
I’m trying to think of what would have been my favorite non-common tools that I use for data engineering and analytics. Frankly, I don’t know. [chuckles] The tools I use are pretty common. I don't think I have any favorite ones. We use Airflow. We use Spark. Pretty boring stuff.
Victoria
Data quality, for example, I guess you get there when your team is more mature to actually have a tool outside of that. Data catalogs. There are some very good ones.
Alexey
We use some in-house data catalog, so I haven't really checked. But having a data catalog is a good idea. The purpose for this is – for all the data sources, for all the data that flows through your data pipelines, you need some sort of recommendation. You need to know who is responsible for this dataset/data source or if there is any personal data there. We have an in-house tool, but I think there are many open source tools. There are many non-open source tools for that.
Victoria
If you're interested in that, I would recommend checking out Amundsen. That's probably one of the first ones to explore. They have been in a bunch of conferences, in podcasts, and everything. So there are a lot of resources. That one is open source from Leap, if I remember correctly. But now, all the big ones have it. LinkedIn has a data catalog and all that. I think most of those are open source, so you can check them out as well. Now they even build data catalogs built on top of those.
Alexey
You mean proprietary tools that are based on open source tools?
Victoria
Yeah. [chuckles]",Data Engineering Zoomcamp,2022,
265598,"Alexey
A resource manager – let's say we have this YARN thing. YARN is yet another resource negotiator. Remember the picture [image] where we had the driver, and then we had a master, and then we had executors. What this resource manager does – and you can also think about Kubernetes, because Kubernetes also does that –what it's doing is locating new sources. You submit a Spark job and you say, “For this Spark job, I need executors with this amount of memory, with this amount of CPU, and I need 10 of them.” You submit this to your master and then they communicate with the resource manager and the resource manager provisions these executors. Before, YARN was mostly used for that. There is a thing called Mesos, I think, that works with Spark. Lately, I think Kubernetes is usually used for that. 
Ankush
Let's assume you say “I want four worker nodes.” This resource manager would be responsible for spinning up these four worker nodes.
Alexey
You have a bunch of machines, and then it will look at it like, “Okay, I have these machines. Let me allocate an executor node there. I’ll take a worker node there.” This is what YARN will do. I think with Spark running on YARN, your master also runs on YARN. A master is kind of doing both things. At the same time, a master is a thing to which you sent requests, and then it's closely connected to this resource manager, because this is the thing that provisions resources. Right? [Ankush agrees] I remember days when I would Google “YARN” and I would find things about Hadoop. 
Ankush
You still find things about Hadoop. 
Alexey
But where? It's almost at the end. And I guess it’s because I clicked on it a couple of times.
Sejal
Yeah, I think the search is curated by user. So if you're a techie, then you would get technical definitions. 
Alexey
Yeah, but this is a JavaScript package manager, which is more popular. “YARN is software packaging system.” You don't get the Hadoop YARN. I still don't know what the difference between NPM and YARN is. Maybe this is a topic for a different discussion. I think these days Kubernetes is getting more and more popular for this. At OLX we have YARN as well, but most of our workloads now run on Spark and Kubernetes. Same for you Ankush?
Ankush
Yeah. We use Flink, but we are also running it on Kubernetes. Kubernetes as a resource manager is pretty cool.
Alexey
I remember attending a talk like three four years ago at Berlin Buzzwords. It was about running Spark on Kubernetes. It was so difficult. There were so many details. They ran into so many problems. Then they managed to run it at the end and I thought “Okay, it's not worth it.” I can just go to AWS, click on AWS EMR, and then I'll have a YARN cluster without worrying about all that. But now Kubernetes is becoming more and more popular.
Ankush
Yeah, because everybody's using Kubernetes. Who wants to spend more hardware on running Spark jobs? 
Alexey
Yeah. Because you already have Kubernetes, you don't want to have another resource manager nearby. With YARN, I think you also usually install the whole Hadoop thing – you will need HDFS there and all the stuff that you don't typically use these days. You keep your data in Cloud Storage, so you don't need all this HDFS kind of stuff. Do you still use HDFS, Ankush, Sejal?
Ankush
There was one company where I used it.
Sejal
Yeah, me too. And that a long time ago.
Ankush
Exactly.
Sejal
Ever since AWS and S3 has come up, I think HDFS is more or less forgotten.
Alexey
I think today, there was a talk from Indeed, which is a company for job searches. They have an on-premise Hadoop cluster and they run Spark on that. But I think this has becomes more and more rare these days. I think Criteo, the company we talked about it has this huge dataset, at some point were bragging that they have the largest Hadoop cluster in Europe. But I thought, “Okay, but is that really a good thing or not?” [chuckles]
Sejal
Must be an old enterprise company. 
Ankush
Legacy. 
Alexey
Yeah, probably.
Ankush
I think SoundCloud also has that. I don’t know. I’ve heard that.",Data Engineering Zoomcamp,2022,948380_qa9_2022__de_zoomcamp_week_6_office_hours_image_two.jpg
463305,"Ankush
I have one recommendation. I think Designing Data-Intensive Applications by Martin Kleppmann is definitely one of the books that every data engineer should read. Another one, which is my favorite, is Database Internals. That one is also really nice. But the first recommendation is definitely the top priority. 
Alexey
I would like to mention that this Designing Data-Intensive Applications is a really great book, but it might look intimidating simply by looking at the sheer volume of the book. We do not expect you to read the book cover to cover, but this is something that you can buy and have as reference that you will go to throughout your data engineering career. Even if you're not a data engineer – I'm a data scientist and I find this book very useful. I keep it as a reference.",Data Engineering Zoomcamp,2022,
392844,"Alexey
It's too early to talk about future iterations of Data Engineering Zoomcamp, I think. [chuckles] But for ML camp, yes – the videos will be the same. The only difference being the homework. That's still up to discussion, so I don't know. We'll see.
Ankush
Yeah, I think we had a slight discussion about this when we met. We are not sure if we are going to repeat the ML and DE Zoomcamp. But let's see the response and the participation. If we feel that it was definitely a success, we will definitely do that. Feel free to pass around the course and give us some feedback so that we can maybe make a better decision.
Alexey
Re-recording the videos… It's just too much effort. So I don't think I'll want to do this, let's say, for the ML Zoomcamp, for example. It just was too much effort and I'd rather reuse the same videos instead of making new ones. I hope the videos are good, so that if we decide to rerun it, we will not need to re-record them. But yeah, we'll see.",Data Engineering Zoomcamp,2022,
868350,"Alexey
I have no idea. Do you have any idea, Dmitry? You said that you split the data into two parts, right?
Dmitry
Yeah. As far as I understand, it can be used so that we can basically separate the set and run it on different machines. For example, if we have quite a big dataset, we can split it between the machines and run.",Machine Learning Zoomcamp,2021,
348025,"Alexey
Yes. For the previous question, I just showed the review assignment process. As for the passing grade, we will see about that. We'll take a look at this course. Because if we say right now, “Okay, in order to pass the course, you need to get 30 points.” We say that and only two or three people pass. This is not the situation that we want to have. [chuckles] We want to first look at the projects and then come up with a fair threshold to see that most of you actually pass.",Data Engineering Zoomcamp,2022,
516357,"Victoria
I bought a book about Data Vaults, and it's called The Elephant in the Fridge. I found it quite nice to read, but I didn't read it all. The reason for that is because what I saw with Data Vault is that there is no practical use, from my point of view, and for the work I do and all that. My understanding is that it generates an extra layer and then you have something else like a data mart. Right now, we have a warehouse, where we load the data, and then we transform it there, and then we build something like data marts or the star schema or something like that, depending on what you want to build, or on the data modeling concepts that you want to apply. All of this lies in our data warehouse. From what I understand from the Data Vault – and sorry, if I'm understanding this the wrong way – it generates an extra layer before and then you have something else, like another database or something, which is like the data marts. It tries to denormalize the data similar to what happens with the star schema. But the reason, or the solution they're looking for, is that the star schema is too difficult to understand. They first define the business need, and then they accommodate that. They have similar concepts to the star schema, but they accommodate that, and then they generate this extra layer. This is the main difference from what I understood. But probably go Google a little bit more. I also don't know companies that use it, to be honest. I've never met anyone that I could have a conversation on how this looks like in real life.
Alexey
What exactly does vault mean? This is a place where you keep things, right?
Victoria
Yeah. It’s technically a data warehouse, but you cannot get out. [chuckles] You have bolted this extra layer where you have this data, and then you don't expose that data – it doesn't get out of the vault – except for those marts. Similar to the concept of data marts, they have it more on the business process group, but there’s this process. I can share the book that I read, but not much more than that.
Alexey
It was something with elephants, right?
Victoria 
The Elephant in the Fridge is what it’s called. The reason for that is because the fridge can be also called a vault. And it's quite nice. It's not from the creators of Data Vault, it’s from someone that actually explains what the creator did. I found it very useful. It’s very clearly written. But that's all.",Data Engineering Zoomcamp,2022,
124694,"Alexey
I wonder what we can do. Maybe we can just start a Google document and if you can help us actually come up with this list, that would be really helpful. Maybe we can then turn this Google document into a Wiki page on GitHub or something like this. I guess we can just start a Google document and then see if it gets any traction – if people start putting links there. If it happens, then you will have it.",Data Engineering Zoomcamp,2022,
759758,"Alexey
No, I don't think so. Some of these projects that you will work on will become standard examples for the future generations. You have the honor of being the first one and making it easier for the future generations. We still haven't decided if we want to do it one more time. But if we decide that, then it will be simpler for them.
Ankush
Even if we don't decide to do that, the videos are out there.",Data Engineering Zoomcamp,2022,
162410,"Yeah, I guess it is okay.",Machine Learning Zoomcamp,2021,
207165,"I think it is. You have one day and a half. You can try. Actually, I just want to remind everyone that we will have a third project, which is optional. If you're behind with this midterm project, what you can do is keep working on this project now, at your own pace, without worrying about deadlines, and when the time comes to submit the third project, you just take the project and submit it as your third project. This way, you will be able to still submit something even if you had to skip a week for some reason. Keep working on your project. You can start today. You can keep working on this and then you can submit it as your third project. That will be fine.",Machine Learning Zoomcamp,2021,
470847,"Alexey
I'm afraid if I go over it again, it will be the same words. I'm not sure what else I can add here. If you want specific instructions, let me do this. So this in the week 2 folder, in Airflow, there is this data ingestion DAG, which you saw in the video. You will actually need to take that and modify it in order to turn this into a DAG that runs for this yellow taxi data and puts this data to Google Cloud Storage. 
You will use this as your base and then on top of that you will work on this, change and modify this code, which will copy all this data to Google Cloud Storage. Another thing that will also be quite helpful for you is this local DAGs folder. This data ingestion local DAG is the code from that long video that actually shows you how you can parameterize your jobs. 
When I was solving the homework, what I did was take that script for GCP and then I took some things from here and then I merged them. As a result, I got the solution for the first two questions. I think once you do this, you will figure out how exactly you should approach the remaining ones. I hope I didn't give away too much and didn't spoil the fun of figuring this out yourself.
Ankush
I think that was a really good explanation. Again, but if you still have questions, feel free to use the Slack channel. I guess that would be the best place because that’s where you can actually put specific examples you're struggling with, or specific errors that you are seeing and I'm pretty sure the community can help you there.",Data Engineering Zoomcamp,2022,
337027,"I think I covered that more or less. Here, “functional-style coding and how it's different from object-oriented programming” – I'm not sure if it's related to object-oriented programming or not, but the functional-style here, in this sense, is that you think in terms of functions. Let's say you have a function and the function has some input and then the function produces some output. In this case, the function can be something like a Keras layer dense of size 10 “DENSE(10)”. This is our function. The input of this function could be an array of dimensionality 183 and the output would be like something of size 10. It kind of forces you to think about “What is the input? What is the output?” And then each time you put together multiple functions, you have sort of a chain of transformations. 
You have one thing, then another thing, then even a smaller thing. You have this chain. Of course, you can have this chain with the sequential way of building models. They're quite similar. I don't think the “sequential” has anything to do with object-oriented programming in this particular case. You can also think of sequential as – you take something, this is the first operation you apply, this is the second operation you apply on the first function – F1, F2. You just don't explicitly say what the input is or what the output is. It's happening under the hood in the sequential model. [image 3] You don't need to define it, but it's defined internally.",Machine Learning Zoomcamp,2021,314586_qa9_ml_zoomcamp_office_hours__week_11_pic3.jpg
325935,"You mean another iteration of the course? Yes. If you go to our course repository, you will see the answer there. First of all, you can take the course in the self-paced mode. All the videos are available, except for the last few videos from Kserve. You can start watching them. It's important that you first attempt the homework without looking at the answer. So do that and then check the solutions. And then do at least one project. That's also important, because if you don't do this (if you just follow the videos) I am sure you will learn something, but it's better if you solidify your learning by also practicing. Do at least one project if you're taking this course in self-paced mode. If you want to get some feedback on your project, you can also share a link in Slack and we will be happy to give you feedback on that project. 
To answer your actual question regarding the next cohort – it will start in September. This cohort started in September. I will probably start it a bit earlier. We started in the middle of September this time, so we will probably start maybe one week earlier. If you want to be notified about this, to be informed about this, there is a link to the form. Click on this, put your email here, and I will send you an email when the course starts.",Machine Learning Zoomcamp,2021,
780406,"Alexey
I think I saw this thing a while back when we were talking about Dataproc and the BigQuery Spark connector, probably. Unfortunately, I haven't used this tool. If you use it from Dataproc, I don't know what the reason could be. But if you're using it from your VM, the reason could be that your VM maybe does not have permissions to write to BigQuery, so you will need to check how to configure this connector in order to be able to use the key. I suspect that this could be the problem. Also make sure that the role you use for accessing Google Cloud Storage and so on, can actually write to BigQuery. I think it's “BigQuery admin” or something like this. So make sure it has that permission. 
It's called a “service account,” I think. So the service account should have the right permissions. In case you created a separate service account, make sure the permission is there. Unfortunately, I do not have a working example that I can share right now. But this is something I want to check this weekend and maybe record a video, if I have time. Please feel free to share this in Slack. I think some of the other students can help with that as well, because I think some of you managed to make it work. Some of you used Spark on GCP, so please share this problem in Slack and then, perhaps somebody who solved this problem already can help you.",Data Engineering Zoomcamp,2022,
752792,"Alexey
Ankush, the way you did it, is you just stopped it. You started the transfer using a transfer service and then you stopped it when it transferred this data, right?
Ankush
It will automatically stop. But you can also use prefixes. There are some options to use prefixes and filters. And you can definitely try to use them.",Data Engineering Zoomcamp,2022,
340381,"Yes, I could. As soon as I can, I will do this and I will post a message in Slack. I just need to find time to do a couple of tweaks in the script.",Machine Learning Zoomcamp,2021,
461922,"Victoria
Not sure if I understand them, because you're taking it from a CSV, not really a database, right?
Alexey
Yeah, I think this question is more general and not related to homework, if I understand correctly. Let's say you have some database in ecommerce (it could be about orders or something like this) and this data is constantly changing. For example, today, the price is this – tomorrow, the price is that. It changes. The question is, how do you put this in your data warehouse if this can change? Do you know how to do this?
Victoria
I don't know if the question is targeting a schema evolution, because your example was more about the same data changes, right? In that case, you would have a primary key and then you would have to do some kind of deduplication, because I assume you'll have the original record and then a modified record. 
When you're modeling the data, you will have to do some kind of deduplication to always take the last record on a primary key, let's say “order number” in that case. As for a schema evolution, if that's where the question is going, then that will be a little bit more complex, because you'll have to somehow adapt your pipeline. Or if you're using ETL as a service tools like Fivetran (there were some questions about that as well) some of those tools also adapt to schema evolution.
Alexey
Another thing to add to that is – let's say you have a database with some records. This can be item ID, item name, price, and let's say the price changes. What you capture is this change. You have a new version, so you know that some record in a database was updated, and you save this change to the data warehouse. Therefore, every time a record changes, you save it to your data warehouse, and you have a bunch of rows that change. 
Basically, you capture every time something changes, you save it in a data warehouse, and then you can sort of travel back in time – you can see what the state was for this record on this particular date. I'm not sure though, if it's specific for a data warehouse – but at least this is how we do this. Every time the record changes, it will capture this change somehow and in our SQL query, we can say that “We want to see this date at this timestamp.” And we will see the latest price, for example. 
As for how to implement this, I actually have no idea. Maybe Fivetran can do this. For all the databases – let's say if you use MySQL – there is something that can send these data source changes or something like this. For Postgres, there is probably something similar.
Victoria
Yeah, you'll implement a pipeline that is basically logging what's going on. Even though in the original database, maybe if it's an order (like in this case) it will update, but that will generate a second entry, like Alexey mentioned, because you're logging. With Fivetran, you could do it as well, of course.
Alexey
Fivetran is a low-code thing. You just say, “Okay. This is my source, this is my destination,” and then it does the thing, right? It just moves data from one location to another. Is this how it works?
Victoria
Yeah, more or less. [chuckles] You set up a connection to a S3 bucket or whatever you want – they have several adapters. You can set run rule scripts and then just set a destination.",Data Engineering Zoomcamp,2022,
984603,"Alexey
Ankush, you like Scala, right? [chuckles] 
Ankush
I think Scala is definitely one of the best languages out there for data engineers. [chuckles] The biggest part is that it's typesafe and it's not as heavy or as verbose as Java. Scala has a perfect combination, at least for me, coming from Java background, I love Scala. I would say, yes, it's kind of an important language to know as a data engineer, especially because Spark is written in Scala. Sometimes when you see errors, which are very Spark- or Java-native, you might get confused if you do not have any knowledge. So I like Scala, personally. I use it for developing some pipelines. I also use it to write code in Spark and sometimes also in Beam. But do you actually need it? No, not really. If you know Python well enough and you are very, very proficient in Python, you can get away with minimum knowledge of Java and Scala, I would say. Unfortunately. I would say I like Scala personally. So if you like it, definitely go ahead and learn it. It's still a good language.
Alexey
But I think you get some performance benefits because Python is an extra layer of abstraction on top of Spark, which adds a bit of overhead. There are some cases when you want to have more control over things. Right?
Ankush 
In my personal experience, whenever we have written batch jobs – I'm not talking about real-time, I'm talking about batch jobs, where it doesn't matter how much time it takes – it was always more than half an hour, 10-15 minutes minimum. So adding another 30 seconds or one minute because of the Python performance doesn't really matter. In real-time streaming, yes, this would be a huge loss. I don't think that's the case with PySpark in real-time, but in batch, it doesn't really matter. 
Alexey
I think serialization is better when you use Scala, because you can use these “case classes” or whatever they are called. Then you can easily turn your RDD to this case class. I think this uses some very efficient serialization mechanism, which you do not have in Python. Sometimes it can be just a difference of minutes, but I remember we had a case when the difference was like 20 minutes. We took a pipeline and rewrote it in Scala, the difference was big.
Ankush
I would say that was Spark 1.0-something because after all data frames, that's also not really useful anymore.
Alexey
It was with data frames. Yeah, it was 1.2. I remember that. We used Python in every step of our pipeline and when we found this one job that we could optimize with Scala, the whole pipeline was a lot faster. But it was something where I needed Scala only once in a year of using Spark. [chuckles] What do you think is more important – to know Java or Scala for a data engineer?
Ankush
If you are very good at Python, then you would use it once. If you already know Java, you don’t need Scala. If you don't know either, then learn Scala, because Scala would be much easier to learn in this application, especially for Spark.
Alexey
I will not argue with that because we don't have a lot of time. [chuckles] [cross-talk] 
Ankush
I have my own preferences. I am biased here, because I really like Scala. My answers are not neutral. They are definitely biased towards Scala.
Alexey
Victoria, you didn't want to add anything, did you?
Victoria
No. I don't know. I haven't used Scala in several years. For some reason, people offer me jobs in Java development on LinkedIn. [chuckles] 
Alexey
Do you use it?
Victoria
No, no. I used it at university because I had to prove it to get the title. I would not use it again.",Data Engineering Zoomcamp,2022,
782870,"Lisa
There are all kinds of things. Do you have missing data? Is it text data? Sometimes you need to do some cleaning, like if street is written as “st” and “street,” so you might want to combine them. If there's missing data, you might want to take an average or median. If there’s outliers, depending if it makes sense or not, you might want to throw that out. Say Elon Musk's salary is in your dataset, that might not be a good model to go on. [chuckles]
Alexey
I think his salary is zero, right? He wrote recently in his tweet that he doesn't have any salary. He only has shares. [Lisa agrees, laughs] He would be an outlier, but in a different way.
Lisa
In the other direction, actually. [laughs] So things like that, you would want to look out for. You might use inner core tiles and cut off the tops and the bottoms. There are a lot of different techniques you can use.
Alexey
Data cleaning is very broad and also very abstract. Unfortunately, it's very manual. There is no magic button like in DataRobot. It would be nice if you give it a bunch of dirty data and there is this magic wand button, you click on this, and then your data becomes clean. That would be a nice DataRobot. Maybe you can have two buttons – “Make my data nice, "" and “Make my data clean.” And then “Okay, now find the best model with this data.” Maybe there is some research in automated data cleaning as well. There must be something.
Lisa
There are some companies out there. Some of the companies have been buying each other up and consolidating, so that the trend seems to be end-to-end. Also deploying the models and machine learning ops to make sure you don't have data drift and things like that. But it's not always as robust as you might want for your particular situation, because they try to generalize when they build the tools.",Machine Learning Zoomcamp,2021,
249183,"I just showed you PythonAnywhere. And Heroku is also free. For AWS, you can just create a new account and get another free year. For that maybe you’ll need a different credit card. But I don't know.",Machine Learning Zoomcamp,2021,
126404,"Alexey
I did know. Did you know, Ankush?
Ankush
No, I didn't. But I definitely know now. Thank you so much for this comment. 
Alexey
Yeah, thanks. We also appreciate that you're taking this course – that you're spending time on taking this course and for all your feedback that you give us. Thanks a lot. 
Ankush
Yeah, definitely. I think the feedback is really important. Please keep it coming so that we can improve the course. Maybe in the future, we can have even better videos and courses.",Data Engineering Zoomcamp,2022,
435481,"I wouldn't look at train at all, to be honest. Sometimes I do look, but I think the most important thing is the validation result. 93% for validation seems high, but the only way to check if you're overfitting or not is to use the test dataset that you have. After tuning the model, after selecting the best one, use the test dataset. And if you see that the number is close to that, then no, you're not overfitting. If you see 70% there, then you probably accidentally overfit for some reason. I don't know why. Maybe for you, it will make sense to choose a different split or try to figure out what the problem is.",Machine Learning Zoomcamp,2021,
274012,"I took this course a while ago. Back then it was useful for me. I do think that doing something where you use Python immediately could be more helpful. I'm just not sure what would be a good recommendation for that. The current course, of course, is good. But I wouldn't compare it with the Stanford course from Andrew Ng, simply because they are very different. In that course, you'll get a lot more theory. This one is very practical, very hands-on – it's mostly writing code together. Maybe you can just take both.",Machine Learning Zoomcamp,2021,
134714,"It's really case-dependent. I saw in Slack, if I remember correctly, Carlos asked if he needs to drop features or not – there it seemed okay not to drop. The highly correlated feature was the assistant name and the target variable was churn. Basically, people who work with one particular person tend to turn less than people who work with somebody else. I think in this case, it seems like a relevant feature to include. I don't know – it's really case-dependent. Usually, you need to have some sort of domain expertise to decide if this feature introduces leakage or not. In Carlos’ case, I think it's safe to leave it. If you drop, you can just see how your performance and validation dataset changes and this gives you some idea how important this feature is.",Machine Learning Zoomcamp,2021,
724151,"Sejal
I'm not sure if you've seen the video, but I've already explained this. If you use a Docker setup, then you can actually change your configuration to increase your memory. If you're not using a Docker setup, then there are special instructions, especially for Linux VMs, that we can update in the FAQ in order to be able to do that. 
Alternatively, if you maxed out your memory usage, that is, let's say if your machine only provides up to 4 GB and you cannot extend it further, then I suggest using the no frills version or using a cloud VM to be able to use this. Then you won't have to deal with manually configuring these things. 
Alexey
It's controlled by how much resources this WSL 2 resources the backend has. It says “resources limits are managed by Windows” and you can configure that. On Windows, I didn't have problems with that. I think the way it works on Ubuntu is that Docker can get everything that you have on your host machine. I think on Windows WSL 2 it’s similar. And in MacOS, I saw how to do this in the videos. Also my computer doesn't like when I do Docker compose up. It’s a bit heavy.
Sejal
I read somewhere that Docker Compose does not have that level of maturity and compatibility with Windows or WSL. I think the GitHub issues link that I sent you yesterday also had explicitly stated that.
Alexey
It’s those Windows haters that write that. [Sejal laughs]They made fake accounts. [laughs] 
Ankush
I am definitely one. 
Alexey
[laughs] Well, maybe those aren’t fake accounts.",Data Engineering Zoomcamp,2022,
668438,"Alexey
We haven't created a form yet, but it will be the same as week one. There will be a Google Form, we will choose questions and we will have to trust you that you actually do the homework you code, because you can guess some of the answers without doing the work. But as we said, if you do this, you will not be able to work in week 3. So you better do that properly and maybe I'll include a checkbox in the form that you pinky-swear that you did the code. Of course, you have to include the link to the code. So if we have time, we may randomly pick a few submissions and check it. So you should do the code.",Data Engineering Zoomcamp,2022,
366700,"Ankush
If you're starting, I would suggest you keep your basics correct. But try to keep it as simple as possible. The more you complicate the data pipeline, the harder it will be. Let's rephrase, and maybe talk about the basics, first of all. Use Terraform, use Docker, use Spark, for example, or any other technology that can scale. Don't use Hadoop – use S3 or Google Cloud Storage, if you're already on the cloud services. Try to use the services already provided by your cloud provider. For example, in AWS, there can be EMR (Elastic MapReduce) which can deploy your Spark jobs. In Google Cloud services, there is Dataproc, Dataflow. There is also an Airflow cloud composer, which is called Cloud Composer | Google Cloud. There is BigQuery. So try to use them as much as possible, but try to keep everything (the whole data pipeline) as simple as possible. The less steps you're going to have, the easier it's going to be. Only introduce another step or another landing zone or something like that if you really need to. The more simple you keep – it's easier to maintain, it's easier to change, and obviously we'll change it. Obviously, it will get complicated when you grow and have other use cases. But try to keep the basics right and in the beginning, just start with the simple use cases.
Sejal
I would say the crux of a data pipeline is in the end, and at an automatic level and ETL pipeline. The basic steps are: extract, transform, load, or if you're using the modern version, it will be ELT, which is: extract, load, and transform. So just focus on building those core components before you add more complexity around those areas and how you can automate it and things like that. Just like what Ankush said.",Data Engineering Zoomcamp,2022,
226200,I think I mentioned it in the homework. You saw how I created the Docker file. This is how you see that it exists.,Machine Learning Zoomcamp,2021,
310446,"Ideally, you just need to think about the users of your application. Do they want you to just predict or do they want you to make a decision? I think usually, it's the latter – the users of your application are not interested only in raw predictions. Let's take a look at this example. [image 1] Here, we explicitly say what the decision is. We say the user will churn through, “Okay, now send an email campaign to this user.” If we just output probability here, then maybe the users of the service would be like, “Okay, it's 72% probability of churning. What do I do with this?” That's why I think it's a good idea to actually automate some decision-making to basically make a decision for the user.",Machine Learning Zoomcamp,2021,376678_qa4_ml_zoomcamp_office_hours__week_6_image_1.png
196532,"I don't think you can because you actually need a web server. GitHub Pages can only serve static pages like HTML. Unfortunately, you cannot host machine learning models on GitHub Pages.",Machine Learning Zoomcamp,2021,
474674,"Ankush
That really depends upon your interest and what kind of knowledge you already have. For example, if you have a lot of SQL knowledge, then understanding DBT would be super easy for you. Maybe Spark and Kafka streaming would be something hard. If you're coming more from a software background, if you've been doing software development, especially backend engineering for some time, and you have been working with distributed databases or with some sort of streaming, then switching over to Kafka and Spark would be relatively easier. So it really depends upon your background. But I would say, generally, all the topics are relatively intermediate. I'm pretty sure in the videos, we will be able to explain to you in a way that you understand all of them. 
Alexey
Maybe I would add that the internals of databases is a pretty complex topic. We will not cover that in detail. But again, these books that we suggested – they are not easy, but they are interesting. For me, for example, Martin Kleppmann's book, I need to reread some of the chapters. It was worth it. It wasn't easy, but it was worth it.
Ankush
Definitely. I think I finished the whole book in a couple of years. Basically, I picked up the topics that were interesting for me, and then went back and did the other parts later on.",Data Engineering Zoomcamp,2022,
131069,"Alexey
Yes. That's the homework and the first two questions are about yellow taxi data. You will need to write a DAG for putting this to Google Cloud Storage for years 2019 and 2020, for yellow taxi. For this week, in the video about Airflow, you get some code that you can use as a starter for putting files to Google Cloud Storage. You will need to change them a little bit, or maybe not a little bit, but you will need to change them. The instructions “Re-running the DAGs for past dates” tells you what exactly you will need to do in order to re-execute this. 
You will need to set up the catch up parameter to true. Then you should be careful with running many jobs in parallel. Some students reported that they had problems with this. Their computer froze when they tried to execute many jobs in parallel, even the virtual machine in the cloud also had problems with that. So don't do that. And then, this is an important thing, you need to rename your DAG. If you don't, Airflow will not see that it changed. 
Let's say, depending on your answer here – if the answer is different from what is currently there, then you will need to do a rename in order for Airflow to pick this up. Also because we changed the start date, Airflow will also need to rediscover that. That's why you want to change the name. There is one extra thing that you might need to do. Because these files are quite large, it will be helpful to manually remove them so they don't take a lot of space. 
Then question 3 is about for-hire vehicles. You will just need to do the same thing as you did for the yellow taxis, but for this dataset. Then question 4 is the same thing but for zones. Eventually there will be a form for submitting solutions and you can use that forum to submit. It will be the same thing as week 1. 
I think that's a good high-level overview. The main point here is that you will learn how to create a DAG, you will learn how to move data from this web to your Google Cloud Storage, and then the other thing is that you will have data for week 3.",Data Engineering Zoomcamp,2022,
520809,"Sejal
I like enjoying the sun after work. It's been consistently sunny for the last couple of weeks, so that's really nice for Berlin. At least as per the Berlin weather.
Victoria
Yeah, definitely enjoying the sun, going to park, being a tourist, also – even in Berlin, or in Argentina. Reading, being with friends. I think I'm pretty normal. [chuckles] A bit boring
Alexey
I like Lego. My son and I make these things from Lego. It's something I quite enjoy doing. I don't know why – it has this meditative effect when you have like 3000 pieces and you need to build something from this. This is pretty fun. [chuckles]
Sejal
I'm inclined to get a jigsaw puzzle for the same reasons. Because at times, I feel like I don't know what to do with myself. It would be nice to do something where I'm building – not code or technical projects, but just something fun for a change. [chuckles]
Alexey
What is a jigsaw puzzle?
Victoria
It's just a puzzle. I have several as well. I can lend you some. Because after you've done it once, then the second time is kind of boring.
Alexey
Yeah, I saw this one. But the one I saw was just a white thing, and you need to figure out how to build it. [chuckles] But that's probably very cruel, and this is for people who are already good with solving the usual ones. Yes, I guess that's all we do. Well, I guess that's not the only thing I do, but...
Sejal
Yeah, we know you like building things – building communities, building projects.
Alexey
But I don't know if it counts us “outside of data or work”. Does it? I mean, outside of work, yes. But outside of data? Not sure.
Victoria
I think it counts. It counts as a hobby. 
Alexey
Okay. [chuckles]",Data Engineering Zoomcamp,2022,
937167,"Does your flask app produce a result? Does it actually return a prediction? If it does, then you're good. If it doesn't, then you need to fix that. To answer your question regarding how many points will be deducted. I think the whole project kind of assumes that you managed to deploy it with Flask, because if you don't, then you will not be able to put it in Docker. You will not be able to deploy it in the cloud and then you will lose like five or six points or something along those lines. So I think it's a good idea to make sure that your Flask returns something. 
If you have problems with that, please share your code. I think I know who is asking that question, because we talked about that already in Slack. Maybe there is an error somewhere in your predict.py script. You just need to make sure that you fix this error and then it should work. Perhaps what you can do is just take the code from the lectures that we did – I think you have the XGBoost model there and not logistic regression. You will need to put your x variable into a t-matrix from XGBoost, and then instead of using predict_proba, you will need to use predict. And I think that's it. Do that – if it doesn't work, please write in Slack and let's try to figure out what's not working.",Machine Learning Zoomcamp,2021,
426370,"What I would suggest you do is try Python 3.7 on this Ubuntu. Either upgrade Ubuntu to 20.04 or downgrade your Python to 3.7. On my laptop, I have the exact same setup and Python 3.8 doesn't work. Or run this in Docker. If you run this in Docker, you won't have any problems. But if you really want to run it on your host machine with this Ubuntu and this Python, you have to either upgrade Ubuntu or downgrade Python.",Machine Learning Zoomcamp,2021,
859807,"Victoria
I would say it's not the main focus for us. For example, at our company the data engineers don't really use DBT. Everyone in the office has access, but it's something that the analytics engineer will do. Then the data engineer mainly focuses on what happens before the warehouse. 
Right now, with Kafka, S3 buckets to pipelines and all of this. But I think it's still good to know what's going on because where is your data going as a data engineer, let's say. Then your stakeholder happens to be the analytics engineer so I think it's good knowledge to know what is actually happening there.
Alexey
The reason I mentioned Ankush is because one year ago or maybe more, when we first met, he was still working at a different company as a data engineer. He mentioned this tool, DBT and I had no idea what this tool was, so I looked it up. He told me that this is something data engineers use in that company. This was the first time I heard about this. Apparently, some data engineers also need to use this tool sometimes.
Victoria
If you think about it, many companies don't have analytics engineers. It's quite a new role. Either the data engineer has to pick it up or the data analyst and I'd say that it makes more sense that the data engineer picks it up. But I think companies that use DBT are already quite familiar with the role of the analytics engineer so it makes more sense that that would be the person that would take over DBT.
Alexey
And I think I’ve asked you that in our podcast, about analytics engineering, but some data engineers transition to analytics engineering sometimes, right?
Victoria
Yeah. The analytics engineer is a person that up to a year ago was doing something else – either data engineering or data analysis. Then you just either go get a little bit more technical or go get a little bit more business focus. So, yes.",Data Engineering Zoomcamp,2022,
308266,"I do not have anything like that, but feel free to ask anything in Slack. I don't think I did anything special there. A lot of links were already shared in Slack, like how to set up the environment in VCL. So maybe you can also do a search – just go to Slack and look up WSL and you'll see some links there.",Machine Learning Zoomcamp,2021,
371990,"Alexey
The way we thought about this is – just watching the course is not enough. Maybe you watch a course and you feel like you know everything, but when you try to do something, you realize that it's actually not as easy as you thought. We want you to have this experience. We want you to sit down and try to apply all this knowledge that you picked up. That's why we decided that the only way you can get a certificate at the end is if you did the project. That's the purpose. It's not enough just to watch it. You need to do these things yourself. So the answer to this is no, there is no way. You need to do a project if you're interested in a certificate.",Data Engineering Zoomcamp,2022,
83741,"Victoria
I can actually tell a funny anecdote. I built a program for my parents with Java and it had a database. It was a MySQL database. Yesterday, I was trying to fix something and I checked everything. I tried to use the workbench [inaudible] old version in 2014. It was very hard for me to use it. I was like, “Oh my God! How well things have improved over all these years that now I find it difficult to do this compared to when I was still studying.” I didn't have much experience or anything. I would say definitely tools have made things very, very easy. The fact that you can do a pipeline in five minutes with a tool, like with Leap or with Airflow, instead of having to do [inaudible].
Alexey
But on the other hand, MySQL is still MySQL, right? It was 10 years ago, so some things changed but some things still stay the same. There are things that have changed, indeed, but there are some fundamental things like Postgres, which I don't think changed significantly. When I started with Postgres – when I first used it – it was a usual desktop app, a pgAdmin. Now it’s a web app, so now it's different. But it kind of still has the same look and feel. Previously, you needed to install Postgres. Now we can just run it from Docker, which is helpful, at least for me. I know many of you had problems with running it from Docker. But I remember how difficult it is first to install Postgres and then figure out how to actually delete it when you no longer need it. Because it runs as a service and then it's hard to kill. I remember that it gave me some problems. Now with Docker, you just start it (provided that it works) and when you no longer need it, you just forget about this. That’s pretty cool.
Victoria
Before (I'm not sure if you used the same thing) I used to use VirtualBox a lot as well. So Docker has definitely been a game changer. I remember for my thesis, I had to do a distributed system and I had like five year VirtualBoxes running in my machine to fake those five servers. Now you can do it all on the web with cloud warehouses. Because there was no such thing as free trials and things like that.
Alexey
With Docker Compose, you can just start five Docker containers. I think now, as you mentioned, the Etleap tools – they're like no-code tools, almost. You can connect things with arrows. You just say “Okay, there is your data source, there is the destination.” You connect them and it works. This is also cool. 
Victoria
Now you have way more options, which can be very overwhelming because a few years ago your data warehouse was probably one of the big ones, if you were in a company, like Oracle or SQL Server. But right now, there are a lot of options. There's a lot of competition, so the technology improves very fast. But there are also easier things to do. You can still code, but you can also do no-code and still get to have the parts you need.
Alexey
If you're worried about learning something but a new tool appears – fundamentally, things haven't changed that much. We still have ETL processes, but instead of using tools like Informatica, we now use something else for that.",Data Engineering Zoomcamp,2022,
774098,"Yes, you can.",Machine Learning Zoomcamp,2021,
969531,"Victoria
No. 
Alexey
To some extent, you can. I think they are more like general purpose CI/CD tools. You don't use them for data workflows. You can, in principle. But things like Airflow and other workflow engines are tailored to a specific use case. You have all these nice things there.
Victoria
Definitely for a different use case. For me, they overlap, but they fit different use cases.
Alexey
I must admit that we do use Jenkins for that at my work. But don't do that. [chuckles] Please don't do that. You will regret it. [chuckles] 
Victoria
You said before that I was legacy, so…
Alexey
Yeah, it is still legacy. We just need to maintain it. Nobody wants to touch it, because you don't know what happens and nobody wants to spend time moving it elsewhere. So it just kind of runs there. I personally wouldn't come anywhere near Jenkins. But if you like Jenkins, why not?",Data Engineering Zoomcamp,2022,
86155,"I think many people actually shared their midterm projects on LinkedIn and Twitter. Perhaps you can follow the #mlzoomcamp hashtag to see what kind of projects there are. I think it's also a good idea. After this midterm project is over and we do peer reviewing, we will just ask everyone who wants to share their work – they will just share and you will be able to look at it. I don't really want to share the form because maybe some people don't want to share their work with everyone. But if somebody wants to share, you can just post the links to Slack.",Machine Learning Zoomcamp,2021,
323402,I need to check what exactly I promised in the first lesson. 5-10 maybe – something like this. I will write about this. We don't need to write an article yet.,Machine Learning Zoomcamp,2021,
32377,"That's an interesting one. Let's say we have a NumPy array. [image for reference] We have some numbers here –  from 1 to 6 – and we compute the mean here. The mean is 3.5. What we can do here is we add this (3.5) multiple times, the mean doesn't change. This is what happens when we have nan here. Pandas simply ignores it – it doesn't look at this. It pretends it's not there. 
Basically, what Pandas does is it ignores these nans. It looks only at these numbers. When we replace this nans with mean (in this case is 3.5) we get more data, effectively and the mean doesn't change. I can try to show it with a formula. Let's say we have. There will be a bit of math here now. Let's say we have four cities, and then we have a couple of N/As. Basically, this part is the usual values and this part is N/As. This is the missing part. Let's say we want to compute the mean of this, when we do .mean() in Pandas. What happens is, let's say we have n numbers like that, and k is missing values. In total, our series is k+n – the total length. When we compute the mean on just this part, what we get is 1/nΣxi=x̄. This is our mean. In our case, this is 3.5. 
Now let's replace these N/As with this x̄. So what happens now is that instead of looking at 1/n, we look at 1/n+k, because this is the total number of elements here. This is when we already filled in. Then we have the sum that we had before and we have another sum of this x̄. So we have k of this, and we have n of this. Because we summed k times this x̄, it becomes kx̄. And if we have a formula like this, we can just multiply it by n/n. n/n is simply 1. So it's the same as just multiplying this thing by one. What we get is n*x̄ because this part here is our mean. [image for reference] For those who do not have a clue what I'm talking about, just bear with me. It's almost over. 
Then what we have as a result is (1/(n+k))(n*x̄+kx̄). Now we can leave x out of this equation (1/(n+k))(n+k)*x̄=x̄. Basically, this shows that you can add as many means as you like here, and the result will always be x̄. I'm not sure if everyone enjoyed this little proof. [image for reference] But this is how you can prove it. I saw this question before the meeting and I got curious, like “How can I show this mathematically?” And I spent like 10 minutes trying to figure out how. It was a nice exercise for my brain. But I'm not sure I would be able to show this proof on the spot, to be honest.",Machine Learning Zoomcamp,2021,"121034_qa6_06.jpg,121034_qa6_07.jpg,121034_qa6_08.jpg"
6416,"Victoria
There's a DBT package. There are two ways. If you use DBT cloud, you could actually put an Airflow trigger shell into DBT Cloud that you already have in there. Something to think about, for example, we created some shells in there to run everything from Airflow. That job will have an ID, and I could trigger that you send my credentials from DBT Cloud. That would be one way. But I'm guessing this is targeted to a shell using Airflow as a scheduler and not DBT cloud, which is completely valid. For that, there's a package that you could use. There's another thing that says “Invoking DBT through the bash operator.” On this page is the official documentation on how to run the DBT in production in general. One of the options is to do it via Airflow. You can check it out. There's the link of this package, and also the link to an explanation of how to use it via bash operator. You can also check out the other ways, I guess. For example, I suggested using Chrome, if you're working locally. This is also something that's there. It’s not as fancy as having an Airflow setup that Chrome never fails.
Alexey
Well, Airflow is nothing but a fancy Chrome, right? 
Victoria
Yeah, exactly. [chuckles]",Data Engineering Zoomcamp,2022,
579348,"Alexey
Have you used Airbyte, Ankush? 
Ankush
No. What is it? 
Alexey
Airbyte is something similar to Fivetran, but open source. This is also like a pipeline tool 
Yeah, open source data integration for modern data teams. It's very similar to Fivetran, but it's open source and they have open source connectors. I haven't used it. I want to learn it – I want to try it. It's unlikely that we will use it at work because we already have an existing data infrastructure. But it looks pretty interesting.
Ankush
I think it's like a data engineering template, right? Just choose the technologies and fit them in.
Alexey
I want to try it and see how it works.
Ankush
It should be powerful. 
Alexey
Maybe if you want to explore this for a project, it could be a good idea. But on the other hand, don't spend too much time on that. If you want to give it a try, remember that you're time-bounded, and if you see that you cannot figure out how to make it work, then move on and use a tool from the course so that you can make sure that you can finish the project.",Data Engineering Zoomcamp,2022,
820879,"or that you can do this. For example, we have this model. [image for reference] First of all, you can write integration tests. You put this in Docker. You set it up, so it lives on your machine and you can start hitting it with different HTTPS requests and then you get some results. Then you want to first test that the model doesn't take too long to respond and then you want to look at the numbers and then maybe with these numbers, you look at the mean prediction, the maximum prediction, and things like this. Maybe for some cars, it doesn't have to be a specific number. But let's say you can expect for a car like this (worth $50K), maybe the price should be between $40K and $50K. If your model, all of a sudden, stops producing the price for this car within this age range, then you look at this and you try to figure out what happened. 
Basically, again, you have a bunch of test cases for which you want to test that the predictions of your model are the same. And if some of the tests break, you want to take a look at and understand why this is happening. So mainly these two. Then of course, there are some models – for example, we talked about bugs – so maybe this bug has nothing to do with a model and it's just a data pre-processing issue. For that, you can write usual unit tests.
Basically, for machine learning, you write the same tests as you write for usual software, except they're maybe a little bit different in nature, because you expect that the ranges are not always the same answer.",Machine Learning Zoomcamp,2021,54328_qa6_09.jpg
959228,"I don't think there were slides. I don't remember. Maybe there were one or two drawings that I did. Maybe I just didn't see the point of uploading them. It was mostly hands-on, without slides.",Machine Learning Zoomcamp,2021,
484604,"No, you don't. You don't have to use AWS. You can use any other cloud provider. You can use Heroku. You can use PythonAnywhere – some of them actually give you free access, where you can deploy one thing for free. But this is just for your experiments. For homework, you do not need to deploy anything anywhere – only locally. The last question there is to deploy a model locally and score a customer. For homework, you do not need to do this. However, I highly encourage you to actually try deploying your model to the cloud. It doesn't have to be AWS. You can try to find something else. For example, for Google Cloud, they give a couple of hundred dollars for credit that you can spend on their platform. They give it for free and you can play with it and then deploy your models there. I highly encourage you to do that.",Machine Learning Zoomcamp,2021,
814959,It's 404. Somebody managed to do it right before I stopped accepting responses. So somebody got lucky.,Machine Learning Zoomcamp,2021,
588778,"Ankush
They are pretty similar but Spark SQL does not offer everything. There are some limitations when it comes to Spark SQL. In those cases, you might want to go towards the more “raw” level of data, let's say – basically, data frame or RDD. But if you are more comfortable with SQL, then start with SQL and start using Spark SQL and most probably, quite a few of your cases would be covered there. If you're coming from a data science background, then data frame might fit more naturally to you. It can also be a choice that you can make, but for simple use cases Spark SQL works fine, but maybe it doesn't work for more complicated or more optimized jobs.
Alexey
The way I often use it is try to have best of both worlds. For some things where writing a SQL query is faster, for example, if we need to do join, or group by – in simple cases, I would just go with SQL. But if I need to use a UDF or something like this, then I would do some initial preprocessing with SQL, then a SQL execution of SQL returns another data frame. Then you can execute some things on that data frame as well. So you can combine them.",Data Engineering Zoomcamp,2022,
754877,I don't know if you're referring to this. [image for reference] Maybe just watch the video afterwards and let me know if you have questions because I'm not sure what exactly you're referring to.,Machine Learning Zoomcamp,2021,866867_qa6_08.jpg
966706,"Alexey
There was a discussion/thread in the announcement channel a week ago – a lot of datasets were shared there. I didn't know that there are so many cool websites with datasets. So check it out. 
Victoria
I put it in the announcements once more.
Alexey
I also want to do a Slack dump from DataTalks.Club that you can also use to play around with this dataset and see what it can do. It's a bunch of JSON files. You can build some data pipelines there as well.",Data Engineering Zoomcamp,2022,
922812,"We didn't really cover PCA. I don't know how to explain PCA in a few words. Dmitry, maybe you know how to explain it in just a few words? I think we lost Dmitry. Okay. Then I'll try to do this. Basically, this is a feature – PCA finds a way to create new features from existing ones. Usually when you use PCA, the features that you get are less easy to interpret. Anyways, I don't want to go into much detail about PCA. You can just look it up using PCA for feature importance and then you can ask questions in Slack.",Machine Learning Zoomcamp,2021,
47753,"Ankush
As I said, for me, it was Alexey's machine learning course. As we said, Learning in Public is really important. For me, personally, it was also important to teach in public and learn from this experience. What about you guys?
Sejal
I think for me, it's just, like it says in the next question like, “Is it altruism?” Yeah, kind of. It is that. It's just a form of giving back to the community, because I also learned from the community – from looking at open source projects and looking at a lot of publicly available material. This is how I kind of proved myself in a self-made way. I did not really receive any theoretical course. I have a background in computer science, and I never had any data-related education. I just learned it all from my experience, from my colleagues and from the public material outside. So yeah, this is just a form of giving back to the community.
Victoria
For me it’s the same. I feel like if I have the knowledge and I can put in some time and effort, then why not do it? Especially if more people can get into data as a result. At least I get to see if that's something that they would be interested in, especially because the course covers several parts and topics. It's also a good opportunity for people not having to pay for a very expensive course or something like that, and still get a chance to learn if they put effort into it and actually shift their career to data engineering.
Alexey
I have nothing to add. [chuckles]
Victoria
Although you're the biggest contributor, so. [chuckles]
Alexey
Not to this one. The feedback I got from the previous course, Machine Learning Zoomcamp, was really awesome. Seeing feedback like “Do not do Coursera. Go directly to this course,” is really motivating. Although I don't think you should… [chuckles] This is a strange feeling that people compare my course with the courses I was actually taking and then say that mine was better. That is very motivating. And that's why I really like doing that. So we will do more of that. I hope you liked it. Also, please give us feedback. If you think you didn't understand something, your feedback will also be helpful. Positive feedback as well – like how you landed an internship after taking this course or how you found your job and things like this. This will be very helpful for us and very motivating. One of the reasons I'm doing this is because I really like this feedback. So please reach out to us with your stories.
Ankush
I think we should create a separate page for feedback, so we can put it there and so that we are more motivated to make more courses in the future.",Data Engineering Zoomcamp,2022,
629809,"Alexey
Maybe you can tell us what problems you are having? What are the concepts you have problems with? Maybe we can, again, ask you to help us with that. Perhaps it could be written and doesn't have to be a video. If there are some concepts that you have trouble understanding, you can just ask and then others will answer. We'll have that in some sort of document that you can use to make sure you really understand that. That could be one suggestion.
Sejal
I think we've also updated summarized notes on the repository. For week 1 and week 2, it's already there. Possibly, based on the other instructors’ convenience, they will be releasing other weeks also. My understanding was that this would be a summarization, or a wrap-up, if you ever want to go back and reference or just recall whatever you learned. That would also be a good place to go.
Alexey
Please let us know what concepts are hard to understand. It's also useful feedback for us. The next time we record something, maybe we'll try to explain it clearer. Or at least we will know that maybe this thing needs a better explanation.",Data Engineering Zoomcamp,2022,
452334,"Of course, it's possible. But let's say you want to work on multiple projects on your computer (on your host machine / on your laptop) you don't want to put everything in Docker every time because Docker adds some overhead. You need to build an image – it's just too much overhead. That's why having something lightweight to separate the different environments, different Python projects from each other, I think is a good idea. Of course, you can do this with Docker, but just for local development, for local testing, I think it's better to do it without Docker. But when you deploy it, I think you can do this, but I would suggest you use your environment manager.",Machine Learning Zoomcamp,2021,
902322,"Alexey
Yes, we have deadlines to keep you focused. It will help you to stay focused. However, as I said, if you feel overwhelmed and you cannot finish the homework in time, don't worry – you will still get the certificate if you complete the project (even though we just said that certificates don't matter). This one maybe also doesn't matter much. What matters is the knowledge you get at the end. Don't be hard on yourself. If you miss a deadline, it's fine. But in terms of scores, you will not get points for the homework that you don’t finish on time. But who cares? As long as you learn something new. That's the most important thing. Sorry to hear that you found a lot of issues. Hopefully you were able to resolve them.",Data Engineering Zoomcamp,2022,
61634,"Alexey
Yes, the YouTube playlist will still be available.",Data Engineering Zoomcamp,2022,
660940,"Alexey
Ankush, you're shaking your head – that means it's not possible?
Ankush
No. It depends upon the operation you are doing, but if you do an operation in which all your key value pairs have to be at the same node then that will happen on the same node and you will run out of memory. Then either you will need to upgrade your machines to have high memory or you will need to find out your skewed keys and then basically unskew them. That would be the case. But let's assume you are doing something like account or something like an average. An average would also work. If you do an average, for example, then you can have intermediate nodes which will do some precalculation and give only the precalculation ahead to the other nodes. In those cases you can imagine that the shuffling is not happening only on one node. Basically, not all the data is getting there in one node and then something is happening. There are three steps that are happening. In those cases, you might still be able to run with limited memory. But if you do something like a join, then you will need to think about your data skew and maybe your memory.
Alexey
For data skew, I think Ankush sent me a link about how to deal with this data skew. I think this article was about joins. [Ankush agees] Maybe you can share this link later as it is quite useful just to understand how joins work in Spark in addition to what I already explained, because I only scratched the surface. I'll try to find it, but maybe you can just post it.
Ankush
I will also have to look for it, but I think it should be in the slides.",Data Engineering Zoomcamp,2022,
539708,"Victoria
Yes. You can actually visualize not just the DAGs, but the whole DBT doc service on the cloud. The way you do it is through the terminal. I thought I had shown it in the video, but what you basically do is run the DBT doc spinner that will generate a manifest with all of the relationships and everything. And then you do DBT docs serve and this will host the DBT docs in your local host. It's in the repo, I'm pretty sure. You can see how to use it or something like that – you'll find that in the comments there. If you go into the project readme, you can see how to generate the DBT docs. If not, go to DBT Docs Generate to make sure you always have the latest version. Everything, depending on how you run it, you could post it somewhere else if you want it. But this is how you will look at it locally.
Alexey
And it will show a picture, right?
Victoria
Yeah, I will show the DBT Docs that I show in the videos. You will have kind of like a data catalog of everything. From every table, you can go and check the DAG as well.",Data Engineering Zoomcamp,2022,
95544,"Alexey
[chuckles] Yeah, the conversion is terrible. There were more than 6000 people who registered, 400 attempted the first homework, and around 50 submitted the last homework. So it's like 1% – a little bit less. But I think what happened is – life happened to most people. Many just left an email. When it came time to actually do the course many realized that there is something else. But that's the beauty of having it openly. Maybe many people just watch and they do not submit homework. That's fine as well. Just take the best out of this course and tell us if you need help – if you get stuck. Don't just stop if you get stuck. Ask for help and we'll be happy to help you.
Ankush
Definitely. I personally also registered for the ML course and watched a lot of videos, but had no time to do any of the homework assignments or finish the project. But I learned a lot from the videos. It was totally worth it.",Data Engineering Zoomcamp,2022,
633144,"Sejal
We will have a project in the end, over the last three weeks. We haven't exactly finalized our idea for the project but we will be covering all of the modules that were covered from the previous six weeks so that you can build a project around that. 
Alexey
We will still need to think exactly how to organize the project as it's still not finished, but you can already try to start looking for a dataset that you want to process. Then think about what exactly you want to do with this dataset. You can already start exploring this dataset. Basically, try to do what we did here in this course, but with this dataset. Another thing that you can do to reinforce what we learned is take notes – don’t just take notes in your notebook, but actually share these notes – put them in GitHub, for example, or on Notion. It's very time consuming, I know, but for me personally, notes that I took like five years ago, I still remember things from there. In contrast, the things that I didn’t take notes for, I don't remember. To me, personally, taking notes and then processing them and regurgitating them and putting them out there, is very helpful for information to stay in my brain. That would be one recommendation, but it's very time consuming.
Ankush
I also have another recommendation. If you are already in a data engineering or backend development job, and you see an opportunity to work on Docker or Terraform, take it. Because now you have learned it in the first week, so why not try it out in real life in production instances? If you are not working right now, maybe you’re a student, then just try to build a Docker image on your own. A simple Hello World will do. You do not have to overcomplicate it. And I think that will stick with you forever. If you build something like a Docker file from scratch and you Google around trying to figure out what the best approach to it is, how you will deploy it, and if you are successful in doing it, it will really stick with you more than anything else. The same goes for Terraform. You now have a Google Cloud service account, you have Terraform, which will work for it, and you know how to run it. Just create another storage bucket, just create another transfer service, or Google around what would be the easiest resource to create on the Google Cloud services and just do it. I think the things that you do hands-on on your own will really stick with you.
Alexey
Yeah, that's probably the best advice. Just get your hands dirty.",Data Engineering Zoomcamp,2022,
306892,"To be honest, I have no idea what this question means… Oh! FLT PT is floating pointers. Okay. So yes, in NumPy, you have these floating point errors. For those who don't know what this is – on computers, let's say, when we multiply two numbers or when we do some operations. When we take two numbers, you see that instead of 3, we have 3, followed by a lot of zeros and then a 4. [image for reference] Again, this is because floating point operations are not precise on computers. That's why when we do summing or multiplying, we have this little floating point arithmetic error. 
NumPy, because it's running on a computer, it's also prone to that. But I guess your question is “Why do I keep using logs after the data exploration?” You're probably referring to week 2, I think, when we use the logarithm of the price. The main reason for that is not numerical instability – it’s not because there are floating point errors – but (you'll probably learn about this in the video) simply because these distributions have long tails. Please refer to the EDA video (probably second or third) where I explain why we need to do this. And that's why we keep using logs for that. If we don't apply logarithm, then it's very difficult for machine learning models to actually learn from this data.",Machine Learning Zoomcamp,2021,576322_qa6_02.jpg
376599,"I just use tab. I don't think I have Jupyter running here on my computer and I need to start it. But maybe let me just show it with IPython – “import numpy” and then I just do tab. I just press tab and it shows what is there and then when I press tab, it shows what kind of things I can put here. It’s the same in the Jupyter Notebook. You just go inside the parenthesis and then press tab. Then you will see different options shown.",Machine Learning Zoomcamp,2021,
617301,"Yes, you can go to Ubuntu and work there. [chuckles] I don't know. I have a tablet with Docker and Docker doesn't work there because I have Arch 64 architecture on my Windows. People who maybe use the new chip from Mac know what I'm talking about. Basically, I don’t know how to install Docker on my Windows tablet. But I'm not sure I should. For regular laptops I think it's best to just install Linux and use it from there.",Machine Learning Zoomcamp,2021,
341839,"Victoria
Yes. All of the weeks belong to one bigger project, which is about the taxi data for New York that was shown at the very beginning. We cover that in different stages and the idea is that you also use that afterwards to build your own project as the final homework.",Data Engineering Zoomcamp,2022,
957731,"Alexey
In week 1, we had 400 submissions. In week 2, let me check one more time – it was 200 submissions. But Week two is quite difficult as well. It's 227 responses for week 2. Some of them are duplicates. For week 3, it's too early to say, because it's not over yet. Week 2 is not over yet either, but I don't think this number will increase too much. From what I see – from the number of views of videos – there is also a decay. Some of the videos from week 1 were watched like 5000 times, but for week 2, it's 1000. 
And for week 3, I think it’s like 500. It's also going down. I hope it's not too difficult for you. Maybe all this Docker and Airflow stuff is just too much. But I must admit that in real life, unfortunately, this is what you also have to deal with – things like setting up an environment. It's very annoying. It feels like you're not learning anything because you need to focus on setting up this Airflow thing instead of working on creating Airflow DAGS. But this is a very useful skill.
I hope the difficulties you encountered are not too discouraging for you. What doesn't kill us makes us stronger, right? This is a very good skill, being able to figure out these things. You will surely need them at work – figuring out why this tool doesn't work the way you want. I have to deal with this kind of stuff pretty much every day – if not every day, the once per month for sure.",Data Engineering Zoomcamp,2022,
365448,"Alexey
Let me see. It is the 31st of January right now. I think we released the homework on Friday, so the deadline is the 4th of February. I'm not sure where the 7th comes from. Is it because it's next Monday? I think at the beginning, we talked about making it on a Monday. Maybe this is something we'll talk about internally and we will see how you're doing. But right now the deadline is Friday. So stick to the deadline. We cannot promise that it will be on a Friday.",Data Engineering Zoomcamp,2022,
42794,"Alexey
Well, use your judgment. If you want to try it, we cannot force you to register an account in Azure and then run it there. If you want, you can do this. If you don't, then maybe just look at the code and see if there are any errors there. It's really up to you here.
Ankush
Maybe when we are doing the matching, we can ask for criteria – like which of those you used – and maybe we can match corresponding people.
Alexey
That's a nice idea. Let's see. I'm afraid it will complicate things a bit and I already have a script for matching and it doesn't use any criteria. It's just random. 
Ankush
So now we need to modify that script with one column. 
Alexey
Yeah, perhaps. Let's see. 
Ankush
Should be easier enough. We can just group them into different things.
Alexey
So when we create a form for submitting, we can ask what Cloud you used. Right? 
Ankush
Exactly. Right.
Alexey
That makes sense. But what we did in the Machine Learning Zoomcamp is – only those who wanted to re-execute the whole thing re-executed it. And then it's up to you. If you really want to learn from this project – because you execute this not just for grading, but also for learning – so if you want to learn more, and you really want to learn how these things work, you execute and then you learn by doing this. But if you don't have time, since executing on a different cloud might take a lot of time – if you don't have that much time, we cannot really ask you to invest something like five hours into figuring out how Azure works. So use your own judgment here. I guess that's the answer. But we will try to do matching to minimize this.",Data Engineering Zoomcamp,2022,
879855,"Dmitry
No? Before, it was kind of different things. I was more into Keras. Recently they kind of united. Right now it's one thing. But you, for sure, can still use Keras as a [cross-talk] 
Alexey
It can work as a standalone thing, right?
Dmitry
Yeah. I don't know when they plan to finish supporting it. I don't know their plans, but right now you can still use it.
Alexey
Initially Keras didn't work with TensorFlow. The first backend for Keras was… I don't remember what the name of that library was. Do you remember, Dmitry? TensorFlow appeared and everyone forgot about that library, I guess. But was it something that people used before TensorFlow? Theano was the name, exactly. That was the thing that was used as a backend for Keras. But then TensorFlow appeared, Keras added support for TensorFlow as backend, but then the main author of Keras joined Google. Maybe that's the main reason why Keras is now a part of TensorFlow. Just a move from Google to get more users for TensorFlow, I guess. [chuckles]
Dmitry
But we just don't know the answer. I think we’re just speculating.",Machine Learning Zoomcamp,2021,
908041,"Ankush
No, we do not work at the same company, as you have seen. We all work for different companies. I think Sejal is now moving to Berlin, whereas before she was living in Munich. So we were not even in the same city. We all met through DataTalks.Club. I think it was Alexey’s motivation and all the videos and the ML Zoomcamp he was putting out that gave us the encouragement to contribute and work with him towards creating a new course in data engineering.
Victoria
Both Sejal and I also participated in the podcast. That's how we met",Data Engineering Zoomcamp,2022,
985972,"Alexey
There are solutions like Kubernetes for deploying stateful applications like ECS (Amazon elastic container service) for example, which is like Kubernetes, but it's not Kubernetes. With ECS, you don't need to worry about upgrading to the latest version of Kubernetes and so on. It's less popular, maybe also as well-documented compared to Kubernetes, but you can use it. In some aspects, it's easier to manage and in some aspects, it's more difficult because the documentation is maybe not as good. 
But then I'm not sure if you really want to deploy your database there. If there is an option to use a managed database, then maybe you should go with that. Or you can go with something simpler like DynamoDB, which you don't need to manage at all. I think at Google, there is also something similar. I'm not sure what this part means, “which will restart the stateful application”.
There is also a thing called HashiCorp Nomad, which is like Kubernetes but cooler (more hip?). You can try that. They're all quite similar.",Data Engineering Zoomcamp,2022,
831391,"Yes, it can. It's really dataset dependent. For some datasets, this is a good score. For some datasets 0.6 is a good score. For example, on Kaggle, there was a competition about click prediction. The best models had 0.65 AUC there. So it really depends on your dataset. Already, even with this core, some of the models can be used. There is no rule for every model. Yeah, 0.70 is a reasonably good AUC value to go ahead and use this as a final model.",Machine Learning Zoomcamp,2021,
651754,"Let's say I do “import numpy as np” and then, let's say “np.random.exponential()” I noticed that what I do is press Shift+Tab. If I press Shift+Tab, then it shows the docstring, and if I do this multiple times (press Shift+Tab) then it expands. Then they can see what. The funny thing is, for me, I don't even realize that I do this. I have muscle memory, that's why I wasn't saying this out loud. Now you know how to do this. You just go there and do Shift+Tab. If you do this only once, you will have a small thing open. If you do this twice, it will expand. Another thing you can do is just put a question mark instead of the parentheses here“np.random.exponential?”  and then it will show the docstring. Another thing, I think you can do is put “help(np.random.exponential)” and it will just print the docstring.",Machine Learning Zoomcamp,2021,
733226,"Alexey
Splunk – I don’t know. It's not a data warehouse, is it? Victoria, do you know? 
Victoria
No. Snowflake has an integration with Splunk and Splunk is a data observability platform. I’ve never used it. I don't feel confident enough to explain any use case. I know Monte Carlo, for example. Do you know it? Have you used it? [Alexey confirms] I think it's kind of the same as Monte Carlo. It's just observability. It also provides some security features around your data in your data warehouse. Can we use these tools for the project? I mean, you can use it. It doesn't fully do anything. We taught to do it some other way around, as far as I know. It's kind of like a layer on top, I would say. It adds more information that you would probably end up using in a company but it's up to you. If you want to try it, I think it's a good opportunity since you already have a project. Other than that, it’s not necessary. Plus I guess you'll have to use it with Snowflake. I'm sure if it’s warehouse agnostic or it's just for Snowflake. That part I'm missing.
Alexey
Is there a trial version for Snowflake? 
Victoria
Yes, 300 credits as well. You could use them more because you can only use the warehouse. [cross-talk]
Alexey
You can have Snowflake in AWS, you can have it in Google Cloud Platform. You can have it in any cloud. Can you? 
Victoria
Yeah. I'm not entirely sure, when you do the free trial, if you have some kind of limitation on where they host it. When you buy it, you choose definitely that – where to host it, location and everything. For the free trial, I can’t remember now. But I know that it's $300.
Alexey
Basically, the use case for Snowflake is the same – an alternative for BigQuery. You would use it in the same way that you would use BigQuery. It's a data warehouse. Another alternative could be Redshift, for example, which is from AWS. Then there’s Firebolt, I think, or something like this. 
Victoria
Yeah. But Firebolt it’s not entirely… because Firebolt is more oriented to performance. For example, I've heard some very interesting use cases where people use Snowflake for everything like transformation and all that. Then they connected the tool to Firebolt, because apparently their query is very fast. I don't think you could use Firebolt entirely as an alternative to Snowflake or BigQuery, for example. I guess it's like everything else. If you're doing a project and you have the time and you're confident enough with everything and you want to try it, then do it. If you're not sure about the time and everything, try not to use new tools. You can always do another perfect or even continue on the same one. Because otherwise, you’ll just overcomplicate yourself. Even though it's an alternative, even though it’s still a data warehouse, you still have to learn it.
Alexey
If I correctly understand what Splunk is – I know what data observability is in general. Let's say you have a streaming pipeline, there is some data going through your pipeline, or you have a batch pipeline. You have some data there. What these tools give you is the possibility to monitor data quality. Every day, you can see how many records there are, how many records with no values in certain columns, and then they give you some nice dashboards and all that. We covered data quality a bit with DBT. We have that there. And Splunk is a more advanced data quality tool, I would say. There is, for example, a tool called Great Expectations, which is also a data quality tool. Another interesting one is Soda SQL.
Victoria
Actually, in Great Expectations, there's a package for DBT, if you’re also interested.
Alexey
You can check them out. They're open source. There is also a tool called Whylogs, which is also good. It's more focused on machine learning, because for machine learning, data pipelines are a big part of machine learning pipelines. So they also do data pipelines. They, for example, support Spark as well. So there are quite a bunch of tools, but we do not cover these tools. We only touch upon data quality a little bit in DBT. Maybe it's a bit advanced, but at work, when you will have a real data pipeline, having these quality checks is a good idea.",Data Engineering Zoomcamp,2022,
422297,"Yes, it was not mentioned. But what was mentioned in the homework was that you have to use the Docker image that I prepared. This was actually done on purpose, because it's possible to just run model1.bin without Docker and get the question and I wanted you to execute it from Docker. This model2.bin was not available anywhere but in the Docker image and this is why the instruction was asking to base your solution on that image.",Machine Learning Zoomcamp,2021,
558889,"Alexey
I guess the question is why we need to do evaluation on test and validation sets?
Dmitry
I think it's not very obvious what the question is.
Yeah. But I'll try to answer. Maybe I'm answering the completely wrong question. Sorry, it's difficult to understand. For validation, we talked about, for example, when you don't know whether you need regularization for the r-value, what kind of regularization parameter you need, what kind of features are useful, what kind of features are not useful, whether you need to fill the missing values with zero, whether you need to fill it with mean, whether you need to fill it with something else, etc. So you have all these questions that you need to answer, and you answer them using the validation dataset? You do this three-way split: train, validation, test. You test the hypothesis on the validation dataset. But then you don't want to throw it away – you still want to use it for training your final model. That's why you combine your train set and your validation dataset into one (like we saw in the last question from today's homework) and you use all this data that is available to train the final model. Then you do the final evaluation on the test dataset. I don't know if I actually answered the question you asked or a question I invented.",Machine Learning Zoomcamp,2021,
