answer_id,answer,course,year,attachments_files
767296,"Alexey
Probably more than you want to put in. I mean, if you have time, why not? But we will not be able to support you. I struggle to come up with an estimate. Does anyone here on this call have an estimate?
Victoria
I think the hardest part is – you shouldn't be a data engineer if you’re taking this course. You shouldn't have the knowledge that we're teaching. And if you don't have the knowledge, you're trying to learn it, and then everything is showing you something else. On top of that, you want to learn something new on your own that you won't have support for. That's going to be really hard. If you really want AWS because they use AWS at your work, they're going to help you with that one. It's probably not going to be worth it. It's going to be very, very stressful. I would add at least two hours on top of the normal hours.",Data Engineering Zoomcamp,2023,
573165,"Yes, I can. There is actually an entire module about that. You probably mean something specific that you didn't understand. Maybe ask about that in Slack.",Machine Learning Zoomcamp,2022,
571892,"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment.",Data Engineering Zoomcamp,2023,
988549,"Again, you’ll probably hate me soon for saying this, but the answer is “it depends”. Maybe it's zero, maybe it's one, maybe it's two. You never know. You just need to start interviewing and in parallel to that, get projects done. Maybe you’ll get lucky and get hired from the first interview. Probably not, but you will already start learning what companies need. Then, at the same time, you try to implement this and you see, “Okay. This is what companies care about. Let me use some of the technologies they want to see that I’ve used.” You can look at the job descriptions to figure out what is important. You can talk to them when you have interviews and you can ask them, “Hey, what kind of technologies should I use for my portfolio projects to be a good fit for this position?” for example. 
It never hurts to ask. And keep doing this. Maybe you will get a job on the third project, maybe to be on project zero, when you haven't even started doing this. I think two or three should be enough, but yeah. When I got my first job in data science, I had been working back then as a freelancer already in data science. I had some projects from past clients that I showed. But in that interview for the job I got, most of the time was spent talking about my Master’s thesis, which was about processing Wikipedia data. I was working with mathematics there and the interviewer for that job was really interested in this. So we spent most of the time talking about that project. So maybe a good answer to this question would be to have one project that is relevant for the company you interview, and then you will just spend most of the time of the interview discussing this project.",Machine Learning Zoomcamp,2022,
384381,"Alexey
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria.",Data Engineering Zoomcamp,2023,
159619,"RMSE and all these metrics are good. I am not an expert in that. I would suggest going to our YouTube channel, where we recently had a talk just a few weeks ago called Probabilistic Demand - Forecasting at Scale by Hagop Dippel. Check it out. He also talks about metrics there and you will see what exactly to use – what kind of metrics to use to evaluate your models for this specific case.",Machine Learning Zoomcamp,2022,
270092,"I'm very sorry to hear about your mental health problems and I want to remind you that posting in social media is not required. You don't have to do this. In fact, many of the students from the previous iteration did not post anything on social media and still were able graduate from the course with a certificate. Since they did the projects, they ended up quite high on the leaderboard, which allowed them to be on the page with top 100 names. If this is what you're after – if you want to end up on that page – just keep on working, don't post on social media, and you'll be fine. 
Don't worry about the points because, again, nobody knows which of the hashes is you (only you know) and these points are virtual. So don't… it's not required to post on social media. If you don't feel like doing this, then don't. But I think it will be valuable for you. Maybe, after some time, when it becomes easier for you, I do recommend taking a look at social media and posting there. Not right now, but later it will be very valuable for your career.",Machine Learning Zoomcamp,2022,
967449,"When you have a very good score on validation, but a very low score on the test, it means your model became lucky. Remember, we had this explanation at the very first module – we had a lesson about model selection. Sometimes, the model can be lucky and it doesn't necessarily translate well to the test. It happens. What can also happen is that sometimes, validation and test datasets are different. For example, if you speed split your dataset by time – so you have a dataset for one year, and then for training, you use everything from January till September, then validation is October, November, and test is December. So you evaluate your model in validation and October, November looks great – but then we know that in December, it's Christmas time and many models that you trained during the normal months are different. For them, this December could be a surprise. So maybe you have something like that in the test data. It's normal. For these cases, you just need to think about how exactly you can build a validation dataset and training dataset in such a way that they are similar. For example, if you have a lot of data, then maybe you can use the previous year for validation or for testing. Something like this. It's all problem-specific. Usually it's an example of overfitting.",Machine Learning Zoomcamp,2022,
876269,"I don't understand the question, to be honest. We don't have the generation part. The generation part is what was done already for us by the New York Taxi Limousine Company. Storage – yeah, they also store it. They host the data. And yeah, we cover ingestion, transform, and serving with the dashboard.",Data Engineering Zoomcamp,2023,
179159,I would not recommend any certificates. Just focus on projects.,Data Engineering Zoomcamp,2023,
205988,"I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?",Machine Learning Zoomcamp,2022,
74030,"This is a very practical course and because of the nature of the course, it is very tool-oriented. But, again, we do not just show the tools, but also explain the main concepts. These concepts are transferable between tools. For example, we cover Prefect, but when you start working, or when you’re at work, you encounter a different orchestrator like Dagter or Airflow or something else, you will already know the concepts. Therefore, it will not be too difficult for you. But yeah, the focus is more on tools rather than other things. We don't really cover system design here.",Data Engineering Zoomcamp,2023,
939646,"Just Office Hours. Like this particular one, not all of them will actually happen live. Sometimes I cannot be there, for example, so we do it asynchronously. But hopefully, you still get to ask your questions and I answer these questions. This is what we’re doing now.",Data Engineering Zoomcamp,2023,
277703,"This question is so commonly asked that I think I should put it in the FAQ. I use a tool called Drawboard. I actually don't have it on this computer. This is a generic laptop. I don't have a touchscreen. But for Drawboard, you need to have a touchscreen. You actually need to have a pen. I have another tablet, which is Microsoft Surface tablet. This is also Windows and this is actually what I used for recording like 99% of the videos for this course. On this tablet, I have a pen. I just open Drawboard and I start drawing there. So it's a combination of tablet plus Drawboard.",Machine Learning Zoomcamp,2022,
356063,"Yeah. [chuckles] It depends. Use validation to find out what the best number of convolutional layers or dense layers is. There is no “correct” answer to these questions. It’s all case-dependent. For this one, it probably makes sense to use existing architectures – to take things like Inception or ResNet, or Efficient Net – all these models that somebody else trained and just use that. If you really want to experiment with this, use validation to find out what works best.",Machine Learning Zoomcamp,2022,
333669,"Yeah, of course. This is why we are all here, right? You just go to ML Zoomcamp and you follow that – that will be your curriculum. And I'm sharing it now with you. You're welcome.",Machine Learning Zoomcamp,2022,
587525,"Yes. I will remind you that homework assignments are not required to get the certificate. You need to pass two projects out of three. If you do that, you will get a certificate. The important part for the project is to know how to deploy a model. So that’s what you will have to catch up on if you want to pass your project. If you don't catch up right now, you can spend the time we allocated for the midterm project towards doing this. Then you can work on the capstone 1 project and capstone 2 project and then submit these two projects and then pass the course and then get the certificate. You don't have to do everything in this course. For example, even if you don't learn TensorFlow and if you don't learn Kubernetes, you can still do two projects and then pass the course. But what you need to learn (what you have to learn) is the deployment sections – without them, you will not be able to finish the course. So chapter 5, definitely. 7 is also helpful – and 7 builds on 5. In my opinion, 7 is actually easier. Once you know the foundation from 5, taking 7 and then applying this is actually easier. You will need to do less work, if you use Bento compared to Flask. You can use Bento for your project, and I do suggest using Bento for the project. Then we will also have a chapter about serverless deployment and then Kubernetes. It's up to you whether you want to take them or not. I do recommend taking them, but use your own judgment to understand if you have the energy to actually do this. I know some people sign up for this course just to do the Kubernetes module, and there's nothing wrong with that. But if module five was difficult for you, maybe Kubernetes would be even more difficult. So maybe you can focus on something a bit simpler. Or… there is no harm in trying, right? I would still suggest trying Kubernetes. You will have quite some time for the Kubernetes homework. According to our schedule, it will actually be scheduled over the holidays, so you will have quite a lot of time to finish it. Let me check. I think for Kubernetes it was last year that it took a bit more time. We actually left the submission form open for quite some time, so the people who needed to catch up did this over the holidays. Don't worry, you will be able to catch up. But the important thing is to focus on the deployment modules. It will help you with the project.",Machine Learning Zoomcamp,2022,
508359,"Jeff
That's a good question. We don't have a publicly available feature roadmap, probably just because we iterate really fast on things. Every week, you'll see there's about a new release. In this course, and I'll point this out here again for folks, we pinned everything. We pinned our versions in all the work to try to make sure that when people come here in a year, hopefully everything still works well. There’s a much higher probability of that. It’s a good idea to pin your versions a lot of times for your packages, but there's a lot of iteration taking place. We have new views in the last couple of weeks. We have other new features. So if you want to see what's happening and what's the latest, just don't pin it – just do an upgrade – just do a pip install of Su_prefect. Same thing with other packages. Then you’ll see what's happening. We do have release announcements and release notes every week when we release at least once a week.",Data Engineering Zoomcamp,2023,
346778,"I like a thing called Cookiecutter Data Science. There’s a good directory structure here. You can use it. In the projects I do, the structure is pretty similar. So do check this out. Also Kedro project structure they have a good structure. You can check it out too. Kedro is a library for creating machine learning pipelines. But I don't think there is a common standard. There are good standards like this Cookiecutter one, but I don't think there is a single one that everyone uses.",Machine Learning Zoomcamp,2022,
238428,"Well, you can just write thank you – that's already good. Also, I want to thank you for helping each other – that's also great. Please keep doing this. This is really awesome. Then there are also many other things you can help in the community. For example, I need some help with preparing for podcasts. Or… There are many things. Maybe if you're interested in volunteering for some things, please write in Slack. 
If you just want to sponsor in GitHub, there is also a way to support me with like $5 per month. If you go to my profile, there should be a link, Support me on GitHub. You can decide if you want to support me monthly, or just with one-time donations. So that's an option as well. 
Another thing – I don't know where you work and what your company is doing, but if you work at a company that might be interested in hiring interns, maybe you can help us get in touch. Last year, we had a great experience with a company called Delphi. We collaborated (partnered) with them and they took two graduates from the Machine Learning Zoomcamp as interns. They're really happy with the result. The students are also happy, of course. So if you can talk to your employer and suggest considering this, that would also be awesome. Or if you have some other ideas, please share them.",Machine Learning Zoomcamp,2022,
398725,"Salary wise, I don't think it matters. There are all types of salaries. Regarding the difference – there is an article in DataTalks.Club, which is called Roles in a Data Team that describes all the possible roles and what exactly people in the team do. There is the data scientist, data engineer, and machine learning engineer. You can go through this article and see the main differences. But maybe I can just quickly tell you. The data scientist is somebody who works on modeling. This is the first four modules, and then trees, and then deep learning. If you focus on that and you know a bit of deployment, then you're good to go for data science. A machine learning engineer focuses more on engineering. An ML engineer should know some machine learning, not to the same extent as a data scientist, but they should still know some machine learning. And they focus more on deployment. Usually, the way it happens is that both data scientists and machine learning engineers work together on training the model, and then they both work on deployment. But in the first case, the data scientist is kind of the expert on modeling, while the machine learning engineer is the expert on deployment. But typically, both of them should work together. Then, when it comes to a data engineer, this is a person who prepares the data for them – or maybe not for them, but together with them. Let's say, without a data scientist in this process, it will take a lot more time. It's best when both data engineers and data scientists work on this together and then the data engineer explains what is needed for the machine learning project. So please go through this article and hopefully, it will answer all your questions.",Machine Learning Zoomcamp,2022,
117389,"Correct, they are the same.",Machine Learning Zoomcamp,2022,
384680,"When I was freelancing, it was through a website that’s similar to Upwork. You can try that. Or Fiver or something like this. But, depending on where you live, you can just use LinkedIn for that and if somebody wants to invite you for an interview, you can ask them, “Hey, do you consider freelancers?” Some of the recruiters who reach out to you will say, “Yes, we do consider freelancers.” And then you can just start doing this. I don't think I can give you a better recommendation, because I'm not a freelancer myself. 
Maybe what you can do is go to DataTalks.Club site’s podcasts, where we have two podcast episodes about freelancing. The first one is Freelancing and Consulting with Data Engineering and then the other one is Freelancing in Machine Learning. In both cases, the guests talk about finding your first client, how they started freelancing, what they do. They know more about this than me.",Machine Learning Zoomcamp,2022,
719623,"Hmm. Probably not. I don't think so. Maybe it is, I don't know. Let's see in 10 years.",Machine Learning Zoomcamp,2022,
292286,"Alexey
Actually, we didn't cover AutoML here in this course, so maybe a little bit of background. Tim, you probably can tell us a few words about what AutoML is for those who don't know.
Tim
AutoML is automatically training and generating a whole bunch of machine learning models and then seeing which one is the best. It's kind of brute force machine learning. 
Alexey
Here, for example, in module six, when we were talking about XGBoost – remember the approach we took? It's called manual tuning. We started with tuning the learning rate ETA, then we tuned the depth parameter, then we tuned to some other parameter (I don't remember which one). Then at the end, we arrived at the solution that is good. But with AutoML, you don't need to worry about this. So you give AutoML a data frame, or numpy.array – you give it some criteria, then you throw it at the AutoML, you go drink some tea, and then you come back five minutes or five hours later and then you have the perfect model. Right?
Tim
Yep. Yeah, BentoML doesn't really... We rely on the ML training frameworks to do the training part. Once you have the model itself, that's when you come in and you say “BentoML.save”. You can integrate it pretty easily with a lot of different training frameworks, as long as you can save the model at the end. MLflow is a tool that generates a lot of models and experiments and once you find the best one, essentially, you save that one with BentoML and that's the one that you deploy to your service.
Alexey
And speaking of that (short shameless plug). We have a course called MLOps Zoomcamp. In this course, AutoML wasn't really a topic, but we used it for illustrating some of the concepts from experiment tracking. In this module, we talked about experiment tracking. Here, we used Hyperopt, which is a library for finding the best XGBoost model. Then we got the best model and we saved it with MLflow.",Machine Learning Zoomcamp,2022,
604270,"Alexey
I think Victoria covers a little bit of this, right? 
Victoria
Briefly, yes. Ankush, I believe there was an intro in week three. You start a little bit with data warehouse, and then what we do with DBT is use that schema and we talk a little bit about data modeling design. We talk specifically about the star schema and Kimball methodology.
Alexey
We wanted to make this course more practical. This is a more theoretical topic and I think there are good resources where you can pick this up. We did not aim to replace these resources. We wanted to give you something that is very hands on.
Victoria
I think it's still in the document with the questions. I wonder if we could do that again. We had prepared a list last year with some books and stuff like that. We could definitely share that as well to make sure you have it. 
Alexey
I think the list was Awesome Data Engineering, I will look it up. 
Victoria
Ah, yes! We have Awesome Data Engineering. It's bookmarked in the channel. There’s an amazing list of resources. We have a lot. 
Alexey
The idea there was to put it to GitHub and do something like Awesome Data Engineering GitHub repo. There are maybe 10 already. Maybe it should be a more unique name. I actually don't see Kimball and other stuff. 
Victoria
No, but I can add that part. 
Alexey
Please do. We can even have a separate section which we can call Database Design or something and put it there.",Data Engineering Zoomcamp,2023,
256019,"Alexey
I'll try to Google it so we have an example. If I just put it into Google, then we should have illustrations. I personally find it not the easiest thing to interpret. [image 1]
Kalise 
We'll just kind of go over the high level. The intention of the Radar chart – as you can see, there's different rings of circles and it is because Prefect does not require a DAG anymore (Prefect 2). What that means is you can think of each of the dots as the nodes, or your tasks, or your sub-flows. Think of them as nodes and edges and how they're connected. As it expands out, it's going down, with the data dependencies, for example. A task could come back through the middle and call an inside ring that has already been called again. That is really why it’s circular. Now, like Jeff said, we also now have a timeline chart in Prefect Cloud as well, if you were to update your Prefect version. That was released as well. It's a timeline view.
Jeff
It's pretty new. It's pretty cool. We have a picture in the release and I put a picture in Slack today. It’s even more updated with the nodes and edges. Now it has edges too. It's linking from one to the other in time, so that's even spiffier than that now. But it kind of gives you more of what you might think of as a fairly DAG-like view for cases where it makes sense to do that. Yeah, it's a little tricky. We've had some good feedback today on this. It's not intuitive, necessarily, to understand the Radar View chart. It's not something that we're used to seeing as a DAG, because things can be more than just DAGs. We wanted more than one way to show that everywhere. We're working on having kind of multiple different views for folks – some are basically easily understandable, and some that are for more advanced use cases.",Data Engineering Zoomcamp,2023,995264_qa5_de_zoomcamp_2023__office_hours_2_pic1.jpg
25619,"If you need it, we can organize a session for that. Why not? I think we can do something like that. We have some materials. We can probably see how to best organize that for the interview. But Google is your friend – I'm sure you can find a lot of information on Google.",Machine Learning Zoomcamp,2022,
231294,You can ask that in Slack. You can send a direct message to me and we'll figure this out.,Machine Learning Zoomcamp,2022,
992099,"I don't know what that means. We have three courses, which are all Zoomcamps. You can just find the links in the course project. So yes and no, I guess?",Machine Learning Zoomcamp,2022,
55842,"Yes, you can do many things locally. You can use Postgres and run all your queries there. It will not be the same as using BigQuery. But still, you will practice SQL, you will practice analytical queries, and that's what you need.",Data Engineering Zoomcamp,2023,
113420,I actually do not.,Machine Learning Zoomcamp,2022,
501117,"Alexey
Yeah, there are some issues with the new Terraform videos. I don't know. We will probably finish them before the end of this course. I'm not going to promise anything right now. Let's see. Now the first week is over, so it's kind of late anyways. We'll try.",Data Engineering Zoomcamp,2023,
733122,"Tim
It's kind of both. We try to make the deployment of your ML model as easy as possible, whether that's for the data scientists or the engineers. I think we have a lot more detailed features for the engineering and Ops side that maybe a data scientist may not care about immediately. I think it's both. I would say it's half and half.
Alexey 
There’s an interesting thing. This course is based on a book and when I was writing this book, I had a software engineer who wanted to switch to machine learning engineering in mind. Then we ran the course, and in the forum there was this field poll, where I asked everyone who signed up for the course to specify what they're currently doing. Funny thing was, most of the people (most of the students) were data scientists. It was a surprise for me. I was expecting software engineers to join this because this was the target audience, but the reality was different. I guess maybe for you, something similar happened, right?
Tim 
Yeah, I think so. I'm guessing we see probably more engineers than your ML Zoomcamp does. An engineer like me is not going to know much about training. And I think that's one of the questions down below. I think it's good to know the concepts of training and that's why I participate in ML Zoomcamps like this. But as you scale the team, I think you learn to specialize a little bit more. It's just like a small company versus a big company – if you're a small company you wear a lot of hats and then the bigger you get, the more people you get to hire, and you wear fewer and fewer hats. That's just the nature of being either a small or a big company, it just depends on what you like.
Alexey 
Maybe I'll also add to this question. Even if you have a team where you have ML engineers, backend engineers, other engineers, data engineers – there could still be cases when they're busy with something else because they have other priorities. In our case, it was a big monolith and they were very busy decoupling it and creating microservices because it was very important. But I was annoying them with deploying the model and then they simply did not have time for that. Sometimes it may happen that even though you have people on the team, they cannot necessarily jump on this problem right away. Being able to deploy a model is helpful. It doesn't have to be super resilient, super available, super reliable, but if it's deployed (if it can be used) then it's already a good thing. And tools like Bento help a lot with that. They help data scientists to just do a good enough version that doesn't break and when engineers become available, then they can take care of tuning it.
Tim 
Right. Even when the team gets big enough, engineers will hand the deployment over to DevOps as well. As you get bigger, you get more and more separation and specialization.",Machine Learning Zoomcamp,2022,
146382,"We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.",Data Engineering Zoomcamp,2023,
45881,"I don't have a standard or reference example. Check out the projects from the past year. I think that will be a good answer and just pick the format you like and follow it. Another source of projects could be our project of the week. If you don't know, for example, this is something we'll have this week. So there is a plan, broken by what you need to do on each of the days and then you need to follow this plan. At the end, you will have a project. There is actually a section there with all the projects. If you take part in these projects, you can add links here. Then you can just go back to some of the projects and look at them. 
This is my project. I don't think this is a good example, but maybe another source of inspiration – a far better source of inspiration – would be projects from other students, from the previous cohort. There were real gems. This year, too – I saw really good projects. So go through them, find the ones you like, and maybe if you have time, you can create a template for that. You can say, “Okay, this is the reference example. I saw this in this project. I saw this in this project. I combined it.” And reference the examples. People will like it and people will appreciate it. You can also use it for your projects and then share it with the community. That's a good idea.",Machine Learning Zoomcamp,2022,
779904,"Yeah, just the link. You cannot upload a zip file to the form.",Data Engineering Zoomcamp,2023,
774098,"Yes, you can.",Data Engineering Zoomcamp,2023,
844757,"Jeff
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud.",Data Engineering Zoomcamp,2023,
125466,"Pretty much any of these areas. I don't know which one is best. What does “best” mean here? Remote jobs exist for any of these areas. Just pick up what you like, build a portfolio in that area, and then start looking for a job?",Machine Learning Zoomcamp,2022,
175233,"Again, it really depends on your problem. But you probably want to start with rule-based modeling. The very first approach you can take is to develop a simple rule. Then you can actually learn this rule. Maybe you can train a decision tree on this data and then you can see what kind of rules the tree learned. Then you can just use that as your first model. But then, over time, it's growing. Actually, I think I'm just describing data modeling here. [chuckles] I just realized that training a decision tree and then using it kind of becomes data modeling. 
But honestly, for some systems, it makes sense to have rule-based modeling, because sometimes you want to have flexibility of overriding some of the model’s decisions. You can even have both. You have a machine learning model, and then you have rules and they kind of coexist. This is actually what we have at OLX for moderation. Moderation is the process of deciding if a listing (this is an online marketplace) and let's say you want to sell your phone. You go to OLX and you want to sell it. You fill in the title, the description, you upload some pictures, and then you hit “publish”. The listing is not published immediately. It goes through the moderation process. We look at this listing, and we conclude that it's safe to publish. There are a few checks. The first check might be “Is it a duplicate of another listing?” If it's yes, it's not published. If it's not a duplicate, then we do another check. Actually, these checks happen in parallel. Another check could be like, “Is there a gun or some other prohibited item?” Or “Is it not safe for work content?” 
There are a bunch of checks. Most of these checks are machine learning models. But also, a lot of these checks are rule-based models. I will not tell you these rules, but they could be “If something then not published,” for example, or “If something, then show it to moderators.” These rules alone, they're very flexible and moderators can just go to our moderation platform and add these rules manually. This is very helpful, because retraining a model takes some time. You don't always have this time, especially in cases like moderation systems. 
You want to have the flexibility of adding rules and reacting to things that are happening almost immediately – as they happen. Rules allow for this flexibility. These two things coexist. In summary, you can start with a rule-based model then you do data modeling – and, actually, one does not exclude the other. You can use these rules as features to your models and then you can use both in your production systems.",Machine Learning Zoomcamp,2022,
601463,"That, again, depends. If a company wants to have a cover letter, send it. I don't read cover letters, to be honest, even though I’m a hiring manager – I don't read cover letters. I usually look at the CV. Quite often, actually, I try to not even look at the CV. The CV screening is done by a recruiter. She looks at the CV and she probably also looks at the cover letter (I don't know if she actually does). When I do the technical interview, I try not to bias myself and I don't look at the CV. I already trust the recruiter that she did the screening, so the candidate already satisfies the requirements, at least in terms of the keyword match. But I don't want to bias myself too much, that's why I look at the CV maybe after the interview. 
Sometimes, for example, I have a story where I wanted to apply for a computer vision position, which was in the real estate domain. They were working with some real estate images. I didn't have experience in computer vision. In my cover letter, I said “I don't have experience in computer vision, but worked on a project that is related to real estate and I can talk more about that in the interview.” They said, “Okay, come in.” I then went to the interview and I asked them if they invited me because of the cover letter and they said yes. So sometimes it makes sense. But for many cases – I personally don't care much about them.",Machine Learning Zoomcamp,2022,
520235,"I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.",Machine Learning Zoomcamp,2022,
196538,"I will show you. In the Data Engineer Zoomcamp, we have the cohorts folder, and then we have the 2023 folder, where you can see the homework. Actually, there are two homeworks. I think we need to change that. I'll just set a reminder for myself. This is the first homework, and then there is another one here. There are two homework assignments for week one. One is SQL. You go through these questions, and then you submit the answers using this form. And then you do the same for that TerraForm homework. There is a form, you submit the answer here. It's just one question. This is how you do this. After the deadline, we grade the homeworks and you will see the leaderboard.",Data Engineering Zoomcamp,2023,
454908,"Well, I’m glad you asked. We have a data engineering Zoomcamp, which starts in January and we have the MLOps Zoomcamp, which starts in May. The only thing that is missing here to become a full stack data scientist is the business understanding part. Remember, when we talked about the CRISP DM process, the first part was business understanding. This is business domain knowledge and things like that. So the only thing that is missing to become a full stack data scientist is picking up this part. For that, I don't know what the best way to learn it is, apart from just joining a company and then talking to stakeholders (to the users of your model) and trying to understand more and more from them. I do recommend doing this, but let's say if you’re not working yet and you want to be a generalist, taking all these three courses is fine. You don't have to do everything there. For example, in the data engineering course, maybe data warehousing is not as important for data scientists as for data engineers, so maybe you can skip that part. But chances are that, as a data scientist, you will need to work with data warehouses too. So you might as well just watch the whole thing.",Machine Learning Zoomcamp,2022,
303198,"We do not. But if you want, or know somebody who would be interested, let's talk.",Data Engineering Zoomcamp,2023,
601991,"Alexey
I don't think I understand the question. But I'll tell you a story. Last year, when we ran our Data Engineering Zoomcamp, we used Airflow. Airflow was quite problematic. In Airflow, it was very difficult to set up locally. We thought it would be easy. It was in Docker – it had a Docker compose file. We thought, “All you need to run is just Docker Compose Hub, and everything will work fine.” It was very naive of us to assume that. It gave many people a lot of problems. Meanwhile, in MLOps Zoomcamp, we used Prefect from the start. We didn't consider using Airflow. And it was a lot smarter. That's why we thought that for teaching the course, Prefect will be easier to get started, because it's a more lightweight thing. 
Prefect is a nice tool. It's a really cool tool. I'm a big fan of Prefect. But also Airflow is probably more popular. That's why we don't remove Airflow from the videos altogether. They are optional and we still encourage you to try them, too. But if you first walked through the Prefect video, I think it will be a lot easier for you to start with Airflow. It's more lightweight, the code is less verbose there, and it's easier to get started. Once you get up to speed with Prefect, then everything will be easier.
Somebody shared a report in the data engineering course channel. The report was from the AI infrastructure Alliance, I think. They also explain why people like Prefect more than Airflow, so maybe find it.",Data Engineering Zoomcamp,2023,
71621,"I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.",Data Engineering Zoomcamp,2023,
666870,"In deep learning, if you saw the lectures already, you might have noticed that we don't use cross-validation there, simply because it's too time consuming. We set aside a validation dataset and we use this to guide us – to select the best parameter. That's the usual approach I follow, because with proper cross-validation, with creating three folds, it’s simply too time-consuming. That's why I follow a simpler approach. I guess that is the alternative.",Machine Learning Zoomcamp,2022,
863599,"Yes, that's why we have the environment preparation video",Data Engineering Zoomcamp,2023,
844404,"Jeff
With deployment, it’s server again – back to the server. We have a server, when you want to do a pip install Prefect and Prefect Orion start – you have the server running locally. You communicate that with that server through the command line, and then information goes to the server. You can put the deployment information on the server. It's the same thing with Prefect Cloud, except it's hosted. It's our database that's getting that information. In both cases, there's a database backing things. Locally, it's a SQLite database by default. If you want to, you can use Postgres for bigger, higher use operations with multiple people. We have a Postgres database on Google Cloud (GCP) that is backing our stuff on the cloud, too. That's what we're using behind the scenes too. Those are just different servers in different places, as Kalise mentioned before, ours can do things like authorizations so only certain people have access to data, and you can share/collaborate on Prefect Cloud. Basically, the information lives on the server there and when it's time to go and run that, you can schedule it as you've seen from the homework and elsewhere in the course. The agent, somewhere, will be pulling and saying, “Hey, alright. Yeah, I got something ready to run here. There's something that's waiting. I'm going to run it.” And the agent will kick off a process either on your local computer or in some infrastructure you specify, like Docker, and it'll make sure that has the flow code available there. Question 4 on the homework – lots of questions there – we asked you to put the code on GitHub, because GitHub is super popular. You're gonna be able to collaborate with people on GitHub. The code is being found on a GitHub repo, and then the processes are being started in that infrastructure and then it all runs beautifully. You never have any errors and everything works.",Data Engineering Zoomcamp,2023,
641425,"The best way is just to test it. Take a linear regression, look at all these t-statistics, f-statistic, standard errors, and then play with the regularization parameter, and then see how it changes. Then you will see how it affects it and this effect will be different for different datasets. The answer is – it depends. Just go and test it yourself. To be honest, I don't remember exactly what the statistics show to give you an answer. But this is how I would approach this if I wanted to know it.",Machine Learning Zoomcamp,2022,
465132,Okay. Yeah,Machine Learning Zoomcamp,2022,
448783,It's totally fine. It's your GitHub repo – you can do whatever you want. You can delete it if you want.,Machine Learning Zoomcamp,2022,
474349,"Yes, I think so. There was a talk about graph analytics. We had two, actually. Getting Started with Network Analytics in Python from Eric and then Modeling the Human Brain by Jessie. So these two. It’s funny that YouTube found it even though it actually doesn't mention the word “graph” in the title, nor in the description. But Jessie talks a lot about graphs. It's actually her research area. I think she graduated already. And she has a PhD now. She was talking about her PhD project. We also had a Book of the Week about graphs. Practitioners Guide to Graph Data. Via this link you can see all the questions and answers that we asked. You can check this book out. This book is more about graph databases, rather than graph learning.",Machine Learning Zoomcamp,2022,
227886,"I don't know what AS is – applied scientist? Yes and no. I would, again, put emphasis on projects. Course homework is okay too. When looking at someone’s portfolio and I see that this is a homework project, I would ask myself, “How much of this was guided? How much of this did the candidate actually do themselves?” With projects it’s different. Projects are usually more independent. Thus, with projects I will have a lot less questions for you. Homework assignments are fine too, but they are more meant to make sure that you understand the material. They are good for you, but… maybe it makes sense to include them in your portfolio as well. But do projects – that's much better.",Machine Learning Zoomcamp,2022,
139253,"I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.",Machine Learning Zoomcamp,2022,
660130,"Jeff
Cleaning up resources on your computer is something that we don't have a direct thing that we do with Prefect, I don't believe. But you could have a script that is scheduled by a Prefect to run and clean out anything that's sitting someplace after a certain amount of time, for example. That's one option if you're saving a lot of results to your local file. If you want to see more about results, and just how they work, I encourage you to check out the data here. 
You can see information about persisting results and caching – this is one thing that came up on the homework. I know that because earlier in the class, we used a little bit of caching with Kalise. Sometimes, they need people to try to run something in Docker and wouldn't be able to access the cache because it was on your local machine. There are ways to clear that cache. You can refresh that now with a new version of Prefect itself. And you can read about that in the tasks. There are a number of strategies to do that and there's some information in the FAQ and in Slack about that. Refreshing the cache starts with 2.7.8, so make sure you have Prefect 2.7.8 or newer installed.",Data Engineering Zoomcamp,2023,
659857,"Yes, they are. Most of the videos are still the same. There is no point in re-recording them",Data Engineering Zoomcamp,2023,
389970,"Oh, you did not join the course late (Week 1). Late would be like week 6. You're not the first one who says that they’re struggling with Docker. Windows and Mac OS for Docker are often problematic. Linux is the easiest way. If you’re still struggling with it, go with a GCP VM.",Data Engineering Zoomcamp,2023,
557268,"BigQuery is a data warehouse and it's optimized. It's usually faster. AWS Athena is a data lake. You will see the difference in week 2 – there is a video that explains what a data lake is. Then in week 3, you will see what the data warehouse is. But AWS Athena is more like a data lake. You can still run all these queries, but maybe they will be slower and I also think they will be cheaper. For analytical queries that – for queries where you want to get results quickly, I usually use BigQuery. Again, like these are two different clouds, two different technologies. The counterpart of BigQuery would be Redshift. But I think Redshift is slower than BigQuery as well.",Data Engineering Zoomcamp,2023,
453360,"Mention, yes, but we will not actually cover it. If you're interested in CI/CD, we have another course which is the MLOps course where we'll cover it. The course will start again in May. This is the “best practices” one. In best practices, we have things like best coding practices, as well as infrastructures, code, and CI/CD. You can check it out. Here, the focus is on one of the things we cover in the course. So just taking these modules by themselves alone, I'm not sure how useful that is. But if you take the course, it should be useful. However, we don't cover CI/CD in this course (DE Zoomcamp).",Data Engineering Zoomcamp,2023,
116002,"Alexey
In an actual production environment, usually you have some processes that generate data, you capture this data and you put them in the data lake. Then the transformations you do take the data that you already have from the data lake and put it again in a data lake or in a data warehouse. In this case, the data lake is GCS and the data warehouse is BigQuery. This is what it typically looks like.",Data Engineering Zoomcamp,2023,
2183,"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine.",Data Engineering Zoomcamp,2023,
788641,"Jeff
Yeah, I got the video recorded already. You just can't see it. We will release it after the deadline.",Data Engineering Zoomcamp,2023,
828995,"Tim
I think the way that it works is that you can always join the Slack community and ask. That's usually the best way to get in touch with one of us. Joining the community is always helpful. Anybody there can point you in the right direction. Then, depending on what you want to contribute, we'll kind of walk you through how to structure it. A lot of the time, people will need certain features that we don't have and we'll go ahead and fork the repository and open PRs themselves. That's typically the fastest way to contribute. If you need something, then you build it yourself and then open the PR, and then we'll review it and probably have a few comments and then commit it 
Alexey
A word of caution – sometimes just opening a PR without talking to any of the maintainers first may lead to frustration and a lack of understanding of why a PR was not accepted, like “Hey! I spent two weeks of my time contributing and they don't want to accept my project. I'm not going to contribute to open source anymore in my life.” This has happened to me. That was frustrating. What I learned from that experience is that, first, it's best to talk to the maintainers and ask how exactly they want this feature to be implemented. The way you might want to implement may not be the same way the maintainers see this feature being implemented. So it's worth discussing first. 
Tim
That's definitely true, Alexey. I think most of the features out there, we've probably heard of, and we already have some thoughts on how we want to do it. Also, it's likely that we've seen a lot more edge cases as well. That's why it's probably a good idea to run your approach past us before you start coding it. That’s if you want to contribute. If you don't want to contribute and you just want to solve the problem for yourself, you can do it however you want.",Machine Learning Zoomcamp,2022,
535717,I don't know what inclined here means. But I think I already gave a recommendation – I think these two machine learning by Andrew Ng and this other one are quite orthogonal. They focus on different things.,Machine Learning Zoomcamp,2022,
215519,"Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.",Data Engineering Zoomcamp,2023,
359392,"Alexey
You pick a project where you want to contribute and usually, they have some contributing guide. If you don't know which project to pick, we have a thing called Open Source Spotlight in our YouTube channel. Here, we invite different open source authors to talk about their tools. The tools are very different. They're not all data engineering-related or machine learning engineering-related. They're all different. You can perhaps go through the tools and if you like any of these tools, usually one of the questions I ask the author is “How can people contribute?” In each video, you will find out how you can contribute to this specific tool.",Data Engineering Zoomcamp,2023,
133104,"Imagine that you're running an ecommerce company – simple ecommerce. Our CEO comes and says, “Our users have big problems discovering items. How can we help them?” Then you need to understand “Okay, what does it mean ‘they have problems discovering items’?” You need to understand what problems they have and how we can help solve these problems. Once you understand the problems, then you can generate possible solutions. One example is coming up with a good search. Another solution could be coming up with good recommender systems. It also depends on a business goal. What do we want to optimize? Do we want to make sure that our users buy more or so that it's actually easier for them to find what they're looking for when they are just exploring? This is all input that comes from the business people – the stakeholders in the company – and the users. Taking all this input into account and thinking, “Okay, now I know what I need to work on to solve this problem. What can it look like?” And the solution could be designing or coming up with a recommender system that is on the item page that shows similar items. Or, for example, showing what people buy in addition to this item. So that's one example. 
Another example. Let's say we have a video hosting coming – like YouTube. All of a sudden, people start uploading some content that we don't want to have on the platform. We want to get rid of this, right? Business comes to us, the data scientists or software engineers, and says, “Can we somehow make sure this doesn't happen?” And then we can think, “Okay, what are the ways that we can make the bad content go away?” It could be introducing a content moderation platform, for example, where we apply some different machine learning algorithms to a video in order to understand that this video should not be on the platform. Instead of a video, it can be anything. 
So this process of listening to the stakeholders, understanding what the problem is, and then up to the point where you have an idea of how this could be implemented – this is what I call “converting a business problem into a machine learning problem.” I hope it makes sense now.",Machine Learning Zoomcamp,2022,
93187,"Ankush
Are you in a company which has a lot of DevOps (DataOps)? Then none – then you don't need much. But if you are working in a startup, I would say some DevOps knowledge (Terraform, Docker, how to fix Docker issues, how to go inside and see what's going on, logs) would be helpful, in general. Even if you are a backend developer or a software developer, this knowledge would be valuable in any case. If you're working in a very big company where there are thousands of developers with this kind of experience, then maybe you can rely on them more.

Alexey  
I think of companies like Zalando, or for example, OLX where I work, there is already a data platform. If I need to schedule a SQL query, I don't need to do much. I just write the SQL query, commit it to Git and then it just works. I don't need to do any Docker, Terraform, or anything. I'm not a data engineer, but this is how it works.",Data Engineering Zoomcamp,2023,
684662,"Maybe. I haven't seen the updated machine learning specialization. I don't know what that is and what kind of content there is. I took the course back in 2012 (I think) when it was still using Okta. I do recommend taking this course. You still need to do interviews. Let's say you're looking for a job and your goal is to get a job – you start going to the interviews and you see what questions they ask. If these questions are about theory, then I think the best way to learn this theory – the theory that wasn't covered in this course, such as various bias trade-offs, gradient descent, we didn't cover many things – if you want to learn these things, then I don't have better recommendations than Andrew Ng’s course, at least the old one. But I think the updated one should be even better, right? That's why they updated it.",Machine Learning Zoomcamp,2022,
944432,"In this course, we just made it simpler for you – we use BigQuery because it's a part of Google Cloud Platform. When you register, you get free credits, so you don't need to think about this. In practice, I guess what’s really important here is that the concepts stay the same across different warehouses, whether it’s Redshift, BigQuery, Snowflake, Firebolt, or other stuff. The concepts are similar. 
BigQuery is quite popular, Snowflake is quite popular, so if you come into a company that uses a particular warehouse, just stick to that. If you're starting a new team and you need to decide, I don't have a good solution for you. You will need to really think about what you need, what kind of features there are available and then evaluate. The answer is – it depends. 
How to decide what to use in this course? In this course, just go with BigQuery. Play with BigQuery and that should be okay. That should be enough.",Data Engineering Zoomcamp,2023,
774098,"Yes, you can.",Data Engineering Zoomcamp,2023,
450239,"Alexey
YouTube channel, right?
Jeff
I'm teaching a two day course on Prefect next week in New York. If anyone's around New York – it's an in person course – you can come and join that. We have a couple in a few other places, too. Coming to London, Washington DC, and San Francisco are on the books. We don't yet have a remote one, but maybe we'll do something in the future on that. Let us know if that would be of interest to you. You can just reach out on Slack. That's the short answer. We don't have a book, although we've talked about working on it. The thing is, we are rapidly improving things, making changes. Just today in Slack chat, I was sharing with people that we have a new Timeline view. Since I recorded these videos, like 3-4 weeks ago, and now we have some new visualizations – we have all kinds of new features. We're still having lots and lots of improvements and updates and exciting things happening, so we need a little bit more standardization there to do something like a book. But that's the short answer. If you want to come join me, we can spend a couple of days together working on it.
Alexey
So I know that you also have people in Berlin, so any chance you will have workshops in Berlin too?
Jeff
I think there's definitely a chance there. Anna, who you see in Slack is in Berlin and we've had a couple people asking about that. So if that's something that's of interest to people, do definitely reach out.",Data Engineering Zoomcamp,2023,
477292,It is. Linux is the recommended operational system. But MacOs and Windows will also work.,Data Engineering Zoomcamp,2023,
616286,"Well, I think it's better to learn Python simply because it's more popular. There are still some problems where R is more suited to solve, like time series, some statistical stuff, but in general, Python is pretty versatile. When it comes to deployment, it's much easier. I would focus on Python and then if your work requires you to learn R, then do that. If nobody specifically asks you to learn R, then go with Python. It's not like if you don't know R, people will not hire you. They will still hire you – you will just need to learn R at work. It's not a bad language, it’s a good language. If you need to learn R because the company uses it, just learn it.",Machine Learning Zoomcamp,2022,
449113,"Victoria
I don't have a good article where it would explain this that I know. In my experience, this depends a lot on the company and the person setting up. I've worked in companies where we had completely separate accounts for each stage. We would have something like one for development, one completely separate account for something like QA, and one completely different for prod, and then we would have something like a cloning production into our QA environment and our dev environment, for example. 
But normally, the architectures I always see work with DBT are – you would have one warehouse where you'd have multiple data sets (as it’s called in BigQuery). In others, like Postgres, Redshift, and Snowflake, it's called schemas. Or you'd have one warehouse with separate databases. Then you could use something that all of the data warehouses tend to have, which is some way of doing tasks, where you can copy. For example, doing something like copy clients, where you can move the data from production to dev. 
Another topic that is super related to this question, in my opinion, is CI/CD. We didn't go through CI. I would recommend this blog as well. Here, what you could also do is all merge. You would have normally open a pull request and you would have in there something like a CI check that would make sure that the code that you're trying to merge works. Then you could run something like a DBT Cloud or merge, like a shove in here or on a pull request. That is also another way where all of these stages would communicate.",Data Engineering Zoomcamp,2023,
547515,"Let's say two days. So Wednesday, November 23, 2022 at 11 PM Berlin time. That should be enough, hopefully. If it's not, let me know.",Machine Learning Zoomcamp,2022,
612379,"I personally would not, but that’s just me. I usually prefer… I want to understand what's going on there. If it's just a prebuilt thing and I put some data in and then something comes out and I don't really know what's going on there – I don't trust this thing. But that's just me. Again, maybe the important bit here is –you set up your validation framework and whatever blackbox AutoML solution you use gives you a good score on this validation framework, then you're good, right? If you don't worry about how exactly it's done there. Just use it. 
It’s probably a good thing not to worry about these things, especially if you're a startup and you want to move fast – you just want to get something that is already available for you. You just want to start using it and then see the benefits of using it. Maybe just go for this. But I usually like to have a bit of control of what's exactly inside. That's why, for me, I usually prefer to have a Docker image that I deploy to the cloud (to AWS Sagemaker, for example). It just gives me peace of mind, I guess.",Machine Learning Zoomcamp,2022,
575944,"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right.",Machine Learning Zoomcamp,2022,
775971,"You mean the input size, I assume. If it's a pre-trained model, it's easy, because you just take whatever is available there. You also take into account speed and accuracy requirements – and you will probably have them – you will have some idea of how good your model is or how good your model should be. If that's okay for the model, for example, if you want to make it faster, is it okay to sacrifice some of the accuracy? So you need to take all the business requirements into account to answer your question. The answer, again, is “It depends.”",Machine Learning Zoomcamp,2022,
996268,"Alexey
Yes. Well, maybe. Nothing concrete. If you have some ideas, again, please share them with us. I know that many people asked for a course in deep learning. I unfortunately don't know deep learning that well to make the course about that. But maybe there are some other areas that you have ideas about – share them with us. Again, for deep learning – I think maybe we will eventually do this. I just need to find people who know it a lot better than me and I will just help with the process. So nothing concrete, but if you have ideas – share them.",Data Engineering Zoomcamp,2023,
470692,"Alexey
I have never used Pulumi. Again, we use Terraform because it's just one of the tools. The principles apply, Infrastructures as Code applies. It doesn't matter if you use CloudFormation, Terraform, Pulumi, or something else – the principle is still the same. The syntax is better. Pulumi might be better than Terraform when it comes to syntax. I think everything is better than Terraform when it comes to syntax. I don't know why it's so popular because the syntax, in my opinion, is terrible. But yeah, I never used Pulumi. If you like it, you can use it for your project.",Data Engineering Zoomcamp,2023,
132084,"I'm not sure what you mean. Do you want to take a look at all the questions? Or you're interested in the answers too? I think there is a way to export the questions because it's the same Slido link for all the sessions. I'm also working on actually turning all my answers into text. There will be a table from AirTable with all the answers. Maybe I'll share it later. It's not finished yet. But yeah, there will be all the answers I gave in text form. As you can imagine, of course, it's very time consuming. It takes a bit of time, but I will share this thing with you soon.",Machine Learning Zoomcamp,2022,
571677,"Ankush
[corrupted sound] talk about pricing when it comes to BigQuery. I don't think we're gonna compare it with other data warehouse solutions outside Snowflake because Snowflake costing is a bit more complicated. I don't think so. I think we’re going to stick to BigQuery for now. And just remember, the less you pay, the better it is. But the human cost is always more than machines. There is always a bit of a compromise. [chuckle] To answer it simply – no, we're not going to do it.",Data Engineering Zoomcamp,2023,
961339,"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. 
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project.",Machine Learning Zoomcamp,2022,
957387,"Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.",Machine Learning Zoomcamp,2022,
963384,"Alexey
Let me just copy this and paste it to Google and you'll find a way to answer your question. Is actually asking for the ELT tools. Not helpful, I guess. But yeah, Airbyte is definitely one of them. There are others too.",Data Engineering Zoomcamp,2023,
922526,"You can follow the same setup here. Apart from creating a virtual machine and doing port forwarding, everything else that you do in this video also applies to a local Linux environment. Maybe you will also need to install Google Cloud SDK, because on a virtual machine, you already have it – you don't need to install it. Locally, you will need to install it. Apart from that, you can just follow the same stuff for a Linux computer. For Windows or Mac, I don't think we have a guideline.",Data Engineering Zoomcamp,2023,
419697,"I mean, if there is already a model in Hugging Face for your problem, that's good for you. But I guess that's only like 1% of problems that you solve in the industry. Data scientists wouldn't be working at companies if that was the case. [chuckles] We still need to get data and we still need to train these models. And then if you think about it, somebody actually put these models on Hugging Face, right? I suspect that it was done by data scientists. Right? [chuckles] I don't know any other model hub apart from that. Maybe Kaggle. I would say that’s also a good source for models – more for data, but in data, you have a dataset and you can also train a model there. There are many notebooks that show how to do this.",Machine Learning Zoomcamp,2022,
332487,"For pretty much every case, I guess. But yeah, you definitely need machine learning for recommender systems. Maybe for a start – if you're just starting with a recommendation system for your platform – there are simple things that you could do, like seeing what the most popular thing in a certain age group is, or what the most popular thing in a certain category is, and then the command in that. But, quite quickly, you will realize that you need something more substantial, let's say, and that there is no way to do it without machine learning. So you will have to use machine learning. The answer is for any – for every case.",Machine Learning Zoomcamp,2022,
95935,"I don't know – scripts? It would be nice to have an example. Usually, your longer jobs would require a container. It also depends on where you will run these jobs later. A typical scenario, for example, is using something like Kubernetes or AWS batch for executing these jobs. These environments require a Docker container. There, basically the rule is – if you want to deploy anything there, you have to put it in a Docker container. Another alternative could be, for example, if you use Prefect, there are two options. One thing you can do is just do all the work in Prefect, so Prefect will execute this thing. Or another option is to use Prefect as a simple orchestrator and delegate the execution to something like, again, Kubernetes, AWS Batch, Spark, and so on. In this case, if you use Prefect and the agents have all the required libraries, you can just use Prefect for executing things. You don't necessarily need to put these things in a Docker container. But if you're going to use Prefect only for orchestrating and the actual jobs will run in some external environment where a container is expected, then yeah, you Dockerize everything. If you have some examples in mind that you want to talk about, let's do this in Slack.",Data Engineering Zoomcamp,2023,
235894,"I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.",Machine Learning Zoomcamp,2022,
551078,"You'll have to come up with a project idea yourself. We will help you, of course. We will share some datasets with you, but will need to go through these datasets and you will need to find a project yourself. Then if you're not sure if this is a good project or not, you can just, again, ask that in Slack. You can say “Okay, I found this project. Will this dataset be a good dataset for the project?” I can already tell you that datasets like Iris, Titanic, Buying Quality, MNIST – these datasets that you see in every tutorial – are not good for the project. Try to find something a little bit more unique.",Machine Learning Zoomcamp,2022,
597788,"We don't touch databases here. It's “semi-end-to-end,” if you will. [chuckles] If we think about the process for data projects – first, we have “business understanding” and “data understanding”. This course focuses more on this part:

Partly data preparation, and partly evaluation – maybe more deployment. There’s a focus on modeling and deployment. I would say that the focus is more on Data Preparation, Modeling and Deployment.

For example, Data Understanding and Data Preparation would be data engineering.

And then Evaluation and Deployment would be MLOps.

And then business understanding is more related to product management. There is also a thing called AI product management or ML product management, where they talk more about how exactly the process should look like and so on.",Machine Learning Zoomcamp,2022,
362231,"Here, what we can do is – we have a process for collecting data. Let's say we run a website where we sell cars and we can see that inflation is there because the prices that people set for selling the cars are growing. What we can do is, every month we can just use this data that we have for retraining our model. Perhaps we can – I don't know if it makes sense or if people do this – perhaps for the prices in the past, we can retroactively adjust this based on inflation. 
Let's say that if we know that this year in Europe, the inflation is 9% or something like this, or we know that for cars specifically the inflation is this number, then perhaps we can adjust it. But I don't know if people actually do this.",Machine Learning Zoomcamp,2022,
677950,"Jeff
Again, Prefect is DAGless, so you don't have to have a DAG. It gives you flexibility that if you want your things to run in a directed acyclic graph, you can structure things like that. But Python is much more flexible and workflows can be more flexible with Prefect. Maybe it's an Airflow kind of question – I'm not sure. You can schedule things to do what you want. With the recurring rules schedule, our schedule, you’ll have a good bit of flexibility there. It seems extremely, extremely flexible. I encourage you to check out current rules and see if there's something that works there for what you need. Worst case, you could have two different deployments and have them running on different schedules. That’s pretty lightweight, still.",Data Engineering Zoomcamp,2023,
432249,"You're probably in week two already, which is when we look at histograms. There, just follow your intuition. The answer is – enough to get an understanding of the shape of the distribution. This answer is pretty vague, I know. But when you start experimenting with this, you will kind of know when it's too many or too few. When it's too few, you basically have a few giant buckets and it's very hard to tell for you what the distribution is. But then, when you get too granular, then maybe it's just too much. Usually, for me, around 30-50 is a good rule of thumb to go with. Again, it depends. Just plot and then let your aesthetic feelings guide you. There is no right or wrong answer, I think. Someone wrote a comment that there are actually some rules for binning called Freedman–Diaconis. That's the first time I’ve heard of this, but you could probably check it out.",Machine Learning Zoomcamp,2022,
982826,"No. They shouldn't. They should give roughly the same weights. At the end, the minimum of your loss function is the same, but gradient descent and normal equation arrive at the solution differently. And then at the end, the solution should be the same.",Machine Learning Zoomcamp,2022,
67552,"I mean, I don't know why you asked me that, because what do you expect me to answer? [chuckles] Of course, take this course that you're already taking.",Machine Learning Zoomcamp,2022,
293716,"Alexey
I think this is related to the question I answered a few questions ago about this wrong column type. In week 5, we actually used Spark to set the schema. In this case, when we process CSV files like that with schema in Spark and then we use it in BigQuery, everything should be fine. Here, the main idea is not using Spark but specifying the schema. If you specify the schema (if you force the schema) then all the files you create will have the same schema and all the columns will have the same format. Then reading this data in BigQuery will be fine because it will not be confused that in one file, one column has one type, in another file, the same column has a different type. Because of that, there are data type errors. But if we force the schema, all the columns will have the same type and it should be fine.",Data Engineering Zoomcamp,2023,
425209,Maybe tell us more in Slack what exactly you mean by that and what you want to see there? What kind of content?,Data Engineering Zoomcamp,2023,
658180,"For me, as a Russian citizen, it was actually surprising to learn that the World Cup started, because I simply didn't know that until last week, when people all of a sudden started watching football. I see that Ukraine isn't playing there – they have other problems right now, I suspect. I would root for them if they played. I guess Poland. I know that they won a match recently against Saudi Arabia – so, good job. I didn't watch it though. But if you asked me which team I would root for, then maybe Poland.",Machine Learning Zoomcamp,2022,
419700,"Alexey
I don't know when this question was asked, but we did extend it.",Data Engineering Zoomcamp,2023,
760226,"This is such a broad question, I don't even know how to answer that. You need to come up with a question and then try to answer this question with data. So only your imagination is the limit here. Right now, maybe my imagination about this is not very good [chuckles]. I cannot just go ahead and generate a lot of ideas of what you can do. But maybe just open this dataset, take a look at this, stare at it for a couple of minutes, and then you will get a question. Then try to answer this question with data.",Machine Learning Zoomcamp,2022,
296378,"I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.",Machine Learning Zoomcamp,2022,
388792,"One thing I would suggest is to check out Office Hours from last year (2021) if you go to the midterm project, there are these Office Hours. I think it's week nine, where I show how to use the count vectorizer for doing one hot encoding. With count vectorizer, you can apply some filtering. For example, you can say that “I'm only interested in categories that appear at least in 100 observations.” Then, instead of looking at all possible values, you look at only the frequent ones. So go through this document. 
I think there are a lot of different ways you can add filtering here. It could be minimal frequency, it could be something else. It's actually kind of misusing count vectorizer a bit, because count vectorizer is supposed to be used with text features, and not with usual categorical features. But you kind of hack it, in a way, if you say, “Okay, turn these categories into text, and then you train the count vectorizer on them.” So yeah, go through this thing. You can also find the video from that, where I do it live. That's very useful. Again, if you have a huge load of different categories – different values – then the count vectorizer will still require some memory to train. 
There is an alternative. It's called a hashing vectorizer. This one. It does not require training. So you can say, “Okay, I only want to have only like 10,000 features and not more than that.” And it will actually take a word, and it will compute a hash of this word, or value by category, and it will randomly put in one of the columns of this vectorizer. Not randomly – it will compute hash and then put in one of them. So it will be deterministic, of course. Sorry, I didn't choose the right words. 
This is a good way to save memory if you have a lot of categories. So, hashing vectorizer – it works in the same way as count vectorizer, except you don't need to do fit. Actually, you can actually just go through this and read it yourself. It explains everything that you need to know when you compare this one versus count vectorizer.",Machine Learning Zoomcamp,2022,
415195,"If you have time, then why not? But as I said, this course requires some commitment from many people lately. Doing two courses in parallel might be ambitious, but if you have time, then why not?",Data Engineering Zoomcamp,2023,
569646,"The way the decision tree computes “probabilities” i.e. scores is that it looks at the leaf and in the leaf, it sees “What is the proportion of the customers, let's say, who defaulted?” And then this is the score. But the score does not necessarily represent a good probability from the theoretical point of view. From a practical one, it's fine. But from the theoretical point of view, there are some issues, let's say. Then please refer to this calibration stuff I referred to earlier, because I'm not ready to talk in more details about this right now. For simple projects, I think you will do fine with just using the output of this. Sometimes, as I said, your business requirements might require collaboration – that the output is consistent and that it's real probabilities. In terms of “real probability” remember that the definition of probability from probability theory is “How likely a thing is to happen?” “If I take this client, how likely is this client to leave?” The output of random forest might not be exactly this likelihood. It might be a score that you can use to understand the relative risk of this client to default or not. But it's not exactly a probability in the sense from the probability theory point of view.",Machine Learning Zoomcamp,2022,
49875,"Percentage-wise, it's hard because a lot of people signed up, but very few finished. I think we had slightly less than a hundred graduates. But this course is tough time-wise as well. It's okay to take your time for some of the weeks. You might be a bit behind, but maybe what you should do is focus on learning and not on getting a certificate, especially if you have work, if you have family, if you have other commitments. In the feedback form, some people said that it was difficult for them to actually finish the course in addition to other commitments. But what some of them did was take the course and finished it at their own pace and thus still successfully completed the course. But they are not part of the statistics that I shared.",Data Engineering Zoomcamp,2023,
705894,"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too.",Machine Learning Zoomcamp,2022,
483240,"Welcome to the club. Every new library is difficult. You can replace the statement with “I am struggling with X,” and it will apply to pretty much any other library. The good news is that you can somehow find a solution to this problem and the solution is practice. You get stuck – try to get unstuck by Googling, by understanding what you need, and then try to formulate what the problem you’re facing right now is. What do you want to do with pandas, but don't know how? Then try to formulate it and put it into Google and you will find a solution. This is how you learn libraries. This is how I learned libraries. 
There is usually documentation, tutorials – try to go through these tutorials – but first, it starts with formulating what the problem you have is and you want to solve, and then trying to find the solution to this problem. Usually the solution is a piece of documentation, or a video, or a tutorial, or a project that somebody did. Then you try to make sense from the information you find and this is how you learn – by practicing. There are probably videos about pandas – tutorials that you can just watch to learn a lot of that. But you still need to put it into practice. If you don't, then you will simply forget what you learned in the courses.",Machine Learning Zoomcamp,2022,
329041,"I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.",Machine Learning Zoomcamp,2022,
267176,I do not know this course. Maybe you could share the link in Slack and we can discuss it.,Data Engineering Zoomcamp,2023,
634664,"Alexey
I think it depends. If it's an external table, then yes. If it's internal, I don't really know how it works, but I think in this case (I might be wrong) the data is already stored internally in BigQuery and you can do partitioning, clustering on that to have fewer scans and faster query time.",Data Engineering Zoomcamp,2023,
204640,"Alexey
You finish your projects and then after that, you get to review three projects of your peers. There will be a set of criteria. It should be here in the project section, if you search for “Peer review criteria”. You will need to follow this criteria to evaluate. That's how it works, roughly.",Data Engineering Zoomcamp,2023,
998582,"We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.",Data Engineering Zoomcamp,2023,
957483,"Victoria
Going back to Spark versus DBT, it's really up to you. You can use whatever you want. The possible pipeline phase, if you wanted to use DBT, will look similar to how your project looked until week four. So you would use whatever scheduler you want. Something that you could use is Airflow – or for you could use Prefect – to load the data that you chose to the backups and then move them to BigQuery. Once you have them in BigQuery, you would use DBT to do the transformation like you’re using Web4 and then you would use any analytics tool of your choice. We saw a database in Looker studio.",Data Engineering Zoomcamp,2023,
745985,"Sorry, I think they need to know more to help you. But maybe what will actually help you is going to week one, TerraForm GCP and we have the windows.md file that explains how to install Google Cloud SDK and how to install TerraForm. If this doesn't work, then please let us know in Slack. I've already seen a few threads that talk about problems on Windows. I'm quite certain you will find a solution there. If not, ask your question and don't forget to check frequently asked questions first.",Data Engineering Zoomcamp,2023,
697512,"No, it's not in our plans",Data Engineering Zoomcamp,2023,
851834,No.,Data Engineering Zoomcamp,2023,
724503,"Alexey
It’s up to you. If you really want to get a certificate, then you can maybe put more time into watching the videos. If you don't care about the certificate, just take the course at your own pace. When you finish it, you finish it. There is no right or wrong answer. Just do it the way you feel doing it and focus on learning. With the certificate, there is actually a second chance to get it. For example, if you start taking the course now and you're not on time with the first project, it's not a big deal because there is another project attempt and you can use that. Before that comes around, you probably will finish watching all the videos.",Data Engineering Zoomcamp,2023,
394817,"No. It was supposed to be today, but I changed it a little bit. If you go to the updated deadline calendar, you will see that the capstone project starts next week (December 5, 2022). For the evaluation, we give you a lot of time. During this time, depending on when you take your vacation, you will have some time to evaluate the project and then the capstone project starts next week. I think what happened is that when we added Bento ML here, it kind of messed up the whole schedule. The idea was to finish everything by Christmas but, unfortunately, we won't be able to finish everything by Christmas which probably means that if you do the midterm projects 1 and 2, you can get your certificate somewhere here or maybe at the end January. I don't know. Let's see. It will probably be easier to just give certificates at the end of January. Let's say if you finish everything by New Year’s, then you're free to stop taking the course. I know it was very difficult and time consuming. You might as well just finish everything and start the data engineering course or enjoy your life, or do many other things. We'll also have an article, which starts pretty soon. During the next Office Hours, I should probably talk more about that.",Machine Learning Zoomcamp,2022,
417542,"Yes, there will be peer review. For both projects attempts, there will be a peer review phase.",Data Engineering Zoomcamp,2023,
432385,"Yeah, they're similar. Not all of them will ask you to deploy stuff, but for data science, it's often necessary to do simple exploratory data analysis then train the model and write some conclusions.",Machine Learning Zoomcamp,2022,
783196,"It will be a Q&A like this one. You ask your questions and we answer. And that's pretty much it. Typically, it happens live. Sometimes it does not happen live. But the idea is to have a more or less live interaction.",Data Engineering Zoomcamp,2023,
321580,"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp).",Machine Learning Zoomcamp,2022,
643931,"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out.",Machine Learning Zoomcamp,2022,
693504,"Yeah, it is true. If not severe? Yes, it does depend on the model. For example, for linear regression that we implemented ourselves in week 2, you saw that it does affect it. Our model simply could not work unless we added regularization. But when we added regularization, we fixed this. Collinearity was no longer an issue. So it is true and not true at the same time – it depends on the model. 
For tree-based models that we are covering right now in this week's materials, collinearity can also affect how we train our team models. If one feature and another feature are exact replicas of the same, then the tree would randomly select one of them. At the end, the model will probably be the same, more or less, but there are nuances. Sometimes it doesn't play well. 
For example, for random forest, when we select only a part of the feature set, the column that is a duplicate of another column – this feature simply will have more chances to get in a random set, which will affect the quality of the model, but probably not significantly. So I wouldn't say it's a big problem, because we have regularization, because we have models like trees that aren't really affected that strongly. But again, I don't think it's a good idea to have collinear features that correlate. Simply because – why do you need them? Your model will be simpler if you don't include them, so don't include them.",Machine Learning Zoomcamp,2022,
92006,"Indeed, it depends. It really depends on your data. Also, when it comes to RMSE, it depends on your target. If the values of your target are large, then your RMSE will also be large. So it really depends. For classification, usually the metrics we use are 80% (or something along those lines). So they are in percent, and are kind of not absolute, honestly. They don't depend on what you have in your data. But when it comes to RMSE, it depends on the values in your target variable. Here, it can be anything. There is no industry standard. 
Actually, when it comes to other evaluation metrics that we will study in this week – like accuracy, precision, recall – again, there is no good industry standard for this. In some cases, 90% is bad accuracy and in some cases, it's good accuracy. Precision of 90% is good sometimes, in some cases it’s bad. It really depends on the project, on what kind of data you have, how good your features are, and things like this. I would say that if your RMSE or any other performance metric is too good – for example, we will cover AUC as a metric – if it's 95%, then it's suspicious, usually. 95% or more. If I see a very good number, or RMSE that is close to zero, then it should raise suspicion and you should go and check and investigate and figure out why the performance is so good. Quite often, it's because you have some sort of data problems – data leakage, for example, or things like this. Usually, very good performance is an indicator of something being wrong.",Machine Learning Zoomcamp,2022,
554560,"Enough to complete the first homework. If you can't complete the first homework, you should probably take a SQL course. Or maybe you can quickly take it now and finish the course. I know students who did that. If it's not too difficult, or not too unfamiliar regarding what we do in the SQL refresher video and for the homework, then you're fine.",Data Engineering Zoomcamp,2023,
688397,"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either.",Machine Learning Zoomcamp,2022,
251763,"Alexey
I wouldn’t call DBT necessarily... It is kind of an ETL tool, but more like a transform tool. It doesn't do orchestration. Matillion – I have no idea what that is. What would be interesting is to compare Prefect to things like Airflow, Dagter, Flyte and other similar things.
Jeff
Yeah, I think people will learn about DBT here in week 4. You're gonna get to see how DBT works and get more experienced with that. Things like Airflow are potentially interesting to compare to. Kalise, you probably give that spiel more often than I do, if you want to do it.
Kalise
Yeah. Concerning Prefect, our founder was actually a main contributor to Airflow. It was really the vision of Prefect as an improvement of Airflow on workflow orchestration. Having data passing between tasks and being Pythonic as a first-class citizen feature is really important. I really suggest the article, Why Not Airflow? It was written quite a few years ago, but it actually touches on a lot of points of what Prefect does versus Airflow as well. In terms of just a lot of ETL tools, workflow orchestration can do more than just ETL as a whole. If all you are really doing is ETL, or transformations, or something very particular, definitely use the tool that's right for you.
Alexey
I think that there was a blog post from your colleague recently, from Anna, about something similar. Was there?",Data Engineering Zoomcamp,2023,
466311,"Yeah. Last year, we needed at least 16 GB of RAM to run Airflow. Here, we don't use Airflow, so probably 8 GB of RAM should be enough. But I will still go with a VM – I would still take 16 GB of RAM. It will just be better. Let's say you have a laptop with 8 GB of RAM and you already have Chrome running there and VSL and other things, there will really be no space for running stuff.",Data Engineering Zoomcamp,2023,
173594,"Alexey
I guess you refer to week 1, right? You do it in the same way as you do other things. There is one nuance,  one thing that you need to keep in mind, is that your service needs to wait till other things are ready. Perhaps you will need to add a bit of code there for that. If anyone knows a good example, please let us know and share it in Slack.",Data Engineering Zoomcamp,2023,
191259,"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not.",Machine Learning Zoomcamp,2022,
396387,"I think that once you start using them. You also pay a little bit for requests – every time you make a request, you pay a little bit. That’s to my knowledge. I don't really know how it works in GCP. I usually use AWS. In AWS, every time you send a request, you have to pay a little bit. But I think you will get charged money, like fractions of cents, when you start using it.",Data Engineering Zoomcamp,2023,
816559,"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically.",Data Engineering Zoomcamp,2023,
909686,"Yeah. If it works for you, use it. I don't mind.",Machine Learning Zoomcamp,2022,
248002,"Maybe you remember in module one, we talked about CRISP DM. This is a very good way to follow a project. There, you need to start with a goal, so “What kind of goal do you have in mind?” Then you need to think about how you will measure the success of this project. And then, based on that, you go through the rest of the steps. You're probably looking for a more detailed answer here. We can discuss again, but it all depends on your specific project. What do you already have in place? What do you want to do? 
It's all different, but the structure of how you approach the project and how you approach problems is very similar to CRISP DM. Usually, you start with the goal and then you find data that you need for this, then you build the pipelines, you take care of the data engineering part, and then you train your models. Then you evaluate this model, you deploy it. Maybe I missed a step here. But this is how you do this.",Machine Learning Zoomcamp,2022,
586842,"We will, or rather a script will review it.",Data Engineering Zoomcamp,2023,
401034,"Contributions are welcome. If you know how to do this, then please create a guide and then share it with us. You can also create a pull request and include the link in the course repo.",Data Engineering Zoomcamp,2023,
963543,"Jeff
This is getting at different servers. When we start out in the course, you're using the Prefect Orion server. It has a bunch of blocks ready to go with it – they’re built in and it knows about this. But then there are bunches of modules – we have over 40 different collections now on our integrations. There are all kinds of different things out there. If someone has a piece of software and they want it to integrate, they make a module for it, you can install it with pip, but we need to somehow let the server know that it has some blocks in it that could be used by Prefect. So that's what Prefect block register does, as the name of the module suggests. If you connect to a different server, like Prefect Cloud, for example, or a different workspace there, the server doesn't know that you have those blocks available. It doesn't have anything on that server. Those blocks live on that server, so you need to somehow let it know. That's what registering those blocks can do with the block types.",Data Engineering Zoomcamp,2023,
808520,"It will be helpful, but you can learn this in the course as well.",Data Engineering Zoomcamp,2023,
935225,"Yeah. Here, the important thing is to use some validation framework. So set aside some data for testing – for validation – and then use it to select features. There are many things you can actually use for selecting features. If you go to our DataTalks.club YouTube channel, there’s a talk called Feature Selection in Machine Learning with Python. In this talk, Soledad explains what kind of feature selection models there are – algorithms. So please check it out. And I think if you just Google “feature selection,” you will find a lot of articles about this. 
Correlation is a good first step and we will see this in week three, when we look at feature importance. Sometimes you can just throw away unimportant features and that should be large enough. But the best way here is to use cross-validation or just validation and see if removing the feature doesn’t actually doesn't change the performance of your model.",Machine Learning Zoomcamp,2022,
644627,"It is possible to use the competition as your capstone project. I actually would like to encourage that. So please do that. There’s one thing, though. In the competition, you only train a model – you do not deploy the model. That is something you will need to do extra for the capstone project. You can spend a bit of time taking part in the competition, then you will have a model, and then you can just do the rest of the stuff like deployment of that model for your capstone project. That is totally fine. Also, the competition will last for two months. I wanted to open it from the first of December, but somehow I didn't see how to keep it closed and open it on the first of December on Kaggle, so it got opened earlier. The official date would be the first of December and then from that, it's two months. So maybe during these two months you will be able to find a bit of time. Please, go ahead and use it as your capstone project.",Machine Learning Zoomcamp,2022,
884815,Not at all.,Data Engineering Zoomcamp,2023,
254764,"What I personally like is Kaggle. It's not a course, yes I know. But there, you can see a lot of different kernels, notebooks that you can follow and try to learn. I don't think this answers your question. I'll give two recommendations. The first recommendation is to go to our MLOps Zoomcamp. Then in cohorts 2021, in the midterm project, we had these three office hours. During these office hours, I showed some extra stuff. In office hours 8, I showed how to deal with texts. So, if for your project, you're going to use text, then check this notebook and check out the corresponding video on how you can quickly incorporate text in your machine learning models. This is a relatively simple approach. There is also a good book called “
Introduction to Information Retrieval. This is a very nice book. It's also available for free. It's relatively old – from 2006 – but it's a very good book. It's quite practical. It talks about search, but many of these things are applicable to machine learning. For example, “vector space classification” is for text classification and Naive Bayes. So it’s not always relevant for NLP, but some of the things are. For example, “index construction/compression” is more relevant if you want to learn about search. But some of the things like “How do you break down strings into tokens, into words? What are the methods there?” I think it's a good book for that. There is also a thing called Taming Text. But I think the examples are in Java, if I'm not mistaken. So maybe it's a bit old. Kaggle is probably good. If you saw, I recently asked the question in general, “What kind of course do you want me to work on?” Maybe you can write what exactly you want to know about NLP. I know some stuff about classical NLP, information NLP, and search. But I don't know much about “modern NLP” which is what I call all these transformers, GPT-3, BERT – this kind of stuff. All this, I don't know anything about, to be honest, so I won't be able to actually do a course about that. But please go to the thread and write what you have in mind.",Machine Learning Zoomcamp,2022,
956067,"For each use case, indeed, you need to think about what kind of business problem you’re solving and then based on that, come up with a metric. In most cases, precision and recall and F1 score are useful. Accuracy – most of them are good. Maybe what you can do is go to Kaggle where there are a lot of competitions. You can go through these competitions and see what kind of evaluation metric they use. 
First you can try to understand what the problem is about and then you can see what kind of evaluation metric they use for this competition. This way, over time, you will build your intuition regarding what kind of metric to use in which situation? It's not always great. I think Kaggle has some limitations, at least it used to have them – it has a very limited set of metrics. Sometimes, competition organizers would use a metric that didn't make much sense. But it doesn't happen very often. I think right now, the platform supports very customized metrics for each use case. So check out Kaggle.",Machine Learning Zoomcamp,2022,
673446,"Basically, you only have categorical variables as features and your target variable is continuous. Yeah, there are no problems with that. Just train a linear regression, use one hot encoding, and the job is done – more or less. [chuckles]",Machine Learning Zoomcamp,2022,
374283,"I have no idea what Pop!OS is. The reason I am using Windows is because I like it. I have been using Ubuntu since 2010, I think. I used another Linux OS even earlier. But I switched to Windows recently (last year) when recording this course, actually (ML Zoomcamp course) because I needed to edit videos and editing videos on Linux is super annoying – I don't know if you have ever attempted to do this. Ubuntu or any Linux is just not the operational system people use for editing videos or any other thing – pictures, also forget about this. So I thought “Okay, how about I try windows?” I gave it a try and a funny thing happened. 
There is this thing called WSL – Windows Subsystem for Linux – which works exactly like my Ubuntu. And I thought, “If I can have Ubuntu on my Windows, then why do I need Ubuntu?” [chuckles] You see what I mean? On Windows, I could edit videos, I could do many, many other things. So some of the software I use right now, for example, Loom for recording videos, or I use a tool called Krisp for noise reduction, and many, many other tools. They simply don't work on Linux, because the developers don't care about Linux users – they care about Windows users and they care about MacOS users. 
Now I get the best of both worlds. I can use tools that work on Windows, but don't work on Linux and I can still use Linux and do all the development stuff I've been doing on Ubuntu since 2010. And it's been quite great. I really enjoy Windows. I have Windows terminal, which I can use to either do things in Windows or in Ubuntu. And I can always just SSH to a remote computer and have my Ubuntu terminal there. I am not coming back to Ubuntu anytime soon, at least “full” Ubuntu, let’s say.",Machine Learning Zoomcamp,2022,
938493,I don't know what this question means. Everything we study here in this course is important for industry.,Machine Learning Zoomcamp,2022,
375685,"Jeff
We have not had a request for that, I don't believe. Not any of that I know of in our catalog of requests. I will note that there is interest in IBM Cloud Storage. But the best move here – you could probably take something that's very similar to GCP and add that as a collection, potentially. Our collections, again, are all here and you can look at contributing your own if you'd like, so then we would have that available for other people to use. I encourage you to check that out. You can also just make your own custom block and use that. That’s kind of an easier step. There's a section on custom blocks in the docs. You can check that out here and use those as a guide. That can be something that's used. But we haven't had a lot of requests for that, so it's not something I expected soon.",Data Engineering Zoomcamp,2023,
753397,"Colab is an option, yes. You can use that. Colab is a little bit unpredictable when it comes to… working. [chuckles] Sometimes it just stops working and then you have to restart and start training the model from scratch. Overall, it's good. It works. It's just that they might turn off your instance after one hour. It's not like there’s a rule that after one hour, your instance will die. It usually happens sporadically. If you want to have some guarantees with Colab, you probably have to pay. 
If you just want to play around with Colab with the free plan, there is another downside. I think if you want to save something in Colab, and your session dies, I'm not sure if you can recover the files. You need to save them in your Google drive somewhere, or I don't know how it works. So there is a bit of overhead when it comes to the free version of Colab. If you're happy to do this, by all means do it. I actually don't know how much the paid version costs. I think 30 hours will be enough – and remember that it's per month. Next month, you will get more hours.",Machine Learning Zoomcamp,2022,
657869,"No, all the tools are free. That's actually why we use GCP here, because they can give free credits.",Data Engineering Zoomcamp,2023,
336232,"No, you cannot.",Data Engineering Zoomcamp,2023,
829939,"I don't think it's a good idea to use the same datasets. I would go with something different. But if you really want to do this, I don't think I should say “no”. Please write in Slack about this – why you want to use this particular dataset and how it’s different from the homework we do. If it looks fine – yeah, why not? But don't restrict yourself to just these datasets. Try to find something, go to Kaggle, explore what’s there and maybe you will find something better.",Machine Learning Zoomcamp,2022,
473667,"It is, yes.",Data Engineering Zoomcamp,2023,
231208,"Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.",Machine Learning Zoomcamp,2022,
271461,"Yeah, it's okay. You don't have to use deep learning for the capstone project. As I said, if you just use the material for the first seven modules for your capstone project, you'll be fine. You don't have to use deep learning.",Machine Learning Zoomcamp,2022,
347379,"Victoria
The first thing to note is that you can use DBT with Spark. There's an adapter for Spark that you can use. You probably saw it when you created your project in DBT cloud and then for Core, you need to install a different adapter – instead of DBT BigQuery, you do DBT Spark. This means that the limits that DBT has for the transformation are really the limits of the data platform that you're connecting it to. The question is not whether it's more Spark versus BigQuery, rather than DBT versus Spark. 
The other question is: Is Spark with DBT or not? That one can also sync with any of these platforms friends. Why we chose to include both. These are alternative ways of transforming and, as a data engineer, you're going to find both setups in a company. There are companies that you may work at that have all transformations that are all Spark, while other companies have all of the transformations in BigQuery. The same – or just Redshift or just Snowflake. 
Some companies have both BigQuery and Spark, for example. The main difference really will depend on the use case of your data. Spark is usually used for higher loads of data, or a need for streaming as well. That's where normally one would use Spark. It also depends on the data engineering team. And then BigQuery is going to perform better in big data sets, but still is more than Spark in batch processing. I would still recommend reading more about both.",Data Engineering Zoomcamp,2023,
315988,"Victoria
I think this one was solved this already. [inaudible] I know that was in one of those threads. But yes, this is this is my account. I am here under settings on the projects. You're going to see that I actually have the same account as you'll have. It just the videos are one year old and DBT changed a lot since then. You're going to see that when I go through one of my projects, I have this project SAP directory, where you can write the directory. If you don't write anything, then your project that you’re going to develop, is going to start in here. The same when you're writing, it’s going to start always in the root. 
This is my repository – it has nothing but week 4. I have this other one here, If instead you have something like the full repository of the data engineering Zoomcamp, then I can put the project’s directory here and my DBT project will start after all of this. That is where it's going to go and look for my code. If you're using the terminal, then it's as simple as locating yourself where your project yml is. This is the same for here. 
Here is my project yml, under this directory. You locate yourself with either C/D, if you're using Linux or Mac, or dir, if you're using Windows and you execute DBT from there. It's going to go and look through their project yml and then execute from there.",Data Engineering Zoomcamp,2023,
188583,"Alexey
I think 16 gigs of RAM should be sufficient. But again, this year, we are not doing Airflow. Airflow was the most problematic week when it came to local machines. When we tried to run Airflow in Docker, my computer was basically heating up and was going to fly away on the fence. It was too much for it. In the end, I did most of the stuff in a remote virtual machine. This is what you should do as well – use a remote virtual machine because you have this $300 credit. But 16 gigs should be enough, 8 gigs will not be enough. 
Michael
Just in case they didn't catch that. If you don't have enough RAM or CPU, you can just start from the beginning in a virtual machine in GCP – you've got more than enough credits to make it through the course and be running that machine from the start.
Alexey
That's the best thing. Then if everyone has the same platform, same operating system (which is Ubuntu) it just makes it a lot easier to help you. You just say, “Okay, I'm using this virtual machine. It's Ubuntu. This is the problem I have. And then anyone can reproduce this problem, hopefully. Meanwhile, if you’re running this on Windows or MacOS, then everyone has a slightly different version of Windows and sometimes it's just impossible to produce.",Data Engineering Zoomcamp,2023,
126558,"Naive Bayes is a really good model, but I find it very similar to logistic regression. In the end, when you compare the performance of Naive Bayes with logistic regression, it's very similar. It's a very nice model, but since we're covering logistic regression here anyway, I didn't see a lot of sense in including Naive Bayes as well. It has a very nice and elegant theory. You can check it out. It's really beautiful. But in the end, for all practical purposes, it's very similar to logistic regression.",Machine Learning Zoomcamp,2022,
624319,"I do not. I was actually thinking about doing this, but it will probably be quite expensive. If any of you are interested in this, maybe write to me in Slack and then I can think about how to organize that. But so far, there is no “official” program for that.",Machine Learning Zoomcamp,2022,
436415,"Instead of answering this question right now, I will refer to this article called Roles in a Data Team. I wrote it some time ago, but you're basically asking for this information. What the people in the teams are, how they work together and things like this. When it comes to the setup I have at OLX, it's a little bit too complex to explain in the remaining 10 minutes, so maybe I will not do it right now. But please do check out this article and if you have questions, I'll be happy to answer them.",Machine Learning Zoomcamp,2022,
595987,"Yes, I can. I already showed you an article, Roles in a Data Team. Just go through this article, where I cover all these roles. Of course, the opinion of what these roles do varies in the industry. This is my own experience, how I see these roles interact in many companies, specifically in Europe. But I think the ideas are more or less the same across different companies.",Data Engineering Zoomcamp,2023,
826163,"That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.",Machine Learning Zoomcamp,2022,
527586,"If we use just Flask, then there is a warning that “Flask is not a production WSGI server.” So what you need to check is what WSGI is and read about this. I don't think I'll have the time or the knowledge to explain it here right now. But the thing with Flask is, it's not a WSGI server, but Gunicorn is. Well, Flask kind of is, but it's like a toy server, let's say – just for development. It will work on your computer only for testing, but it will not scale. It will not be able to process multiple requests coming in parallel. Gunicorn is much more scalable, parallelizable and just better to use. That's the main reason why Gunicorn is needed. For more details, maybe just check out this article – could be a good one. It explains what WSGI is, why we need this application framework (which is Flask) and a WSGI server, which is the actual server.",Machine Learning Zoomcamp,2022,
426713,It was this above average thing. That was the y_train variable.,Machine Learning Zoomcamp,2022,
485718,"This question was probably asked before the solution was published. Just in case, I will show you where the solutions are. Go to cohorts, 2023 cohort, then the week 1 homework and we have the solution here. We already recorded the solution, so just go through this and you'll see the answers. It’s the same for the other homework assignments and solutions. For the Terraform part, there is no solution. It's just simply applying everything you saw in the lectures. For workflow orchestration, we will also have the solution here after the deadline.",Data Engineering Zoomcamp,2023,
102379,"If I look at the course content outline, we are here right now, which is deployment. Then we will have BentoML, which is deployment. Then we have neural networks, which is more about how models work. Then the rest of the content is about deployment. As you see, 3 of the models (plus one optional) are about deployment. There are other tools that you will learn about and how to apply them. The procedure is very similar – you train a model and then deploy it. That's more or less it. 
What we covered in week 5, are fundamentals – how to organize your virtual environments, what Docker is, how to create a web service, etc. You don't have to use Flask. Many people say that “Flask is no longer cool – FastAPI is cool. We should use FastAPI.” Please, go ahead and use FastAPI if you want, but the process is roughly the same no matter which tool you use. If you want to learn more about that, also check out our MLOps Zoomcamp where we go into more detail about how it should actually be organized. We also talk about different tools we need to use for training and so on. The current course is the foundation and then MLOps builds on top of that and goes more into all the engineering aspects of machine learning.",Machine Learning Zoomcamp,2022,
549426,"Jeff
I think we answered this in Slack and a few other places too. We just want to do a few things. You could just go right in and ingest it as a CSV file, but we wanted to actually change it into a parquet file, do some transformation there and just go through a little example of cleaning up something. But you could go and read something directly in. That's fine. There are tools that are becoming more popular for doing ingestion in particular. Things like Airbyte and Fivetran and other places, especially if you have lots and lots of data in a big organization where you need to have ingests – that can be helpful.
Alexey
In the live chat, there is a comment that says “Getting a Data Engineering Job - Jeff Katz.” This is actually one of our videos. Let me show you how to find it. Go to our YouTube channel and then you just put “Getting a Data Engineering Job - Jeff Katz “ in the search and then this is the link. Check it out. It's also pretty useful.",Data Engineering Zoomcamp,2023,
962799,"I mentioned cross-validation. Another thing I ask when I interview people is about projects. Usually, I ask, “Why did you do this project?” If I get a good answer to this question then it's a very good sign. Here, what I mean to say is – explaining why you did certain things is really helpful compared to a validation framework. So communication is the skill I mean when I say “explaining things. And I guess coding in general is the third one.",Machine Learning Zoomcamp,2022,
776038,"Yes, there is a thing called the validation dataset that you can use to guide your decision. That's the best thing you can do. Another thing you should take into consideration is time. Let's say that we have a lot of categorical variables, then fitting a decision will take more time than a logistic regression. Then applying it will also take more time. So this is something you should also consider in your experiments. But in the end, you probably want to use the validation set to guide you.",Machine Learning Zoomcamp,2022,
602965,"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]",Data Engineering Zoomcamp,2023,
61077,"Yeah, I honestly think that there is not much of a difference between this. In one case, if we don't put limits on the bias term, we say that we don't want to penalize it – it can be as large as possible or not. But in the case of Andrew Ng (in his machine learning lectures) they say “Okay, bias term can be anything. We only want to control weights for things.” Practically, I don't think there is a significant difference, but it becomes a little bit more difficult to implement it. 
When you add these to diagonal lines, for our case implementation is super easy. You just take this identity matrix and multiply it by something. But then in case we don't want to penalize for the bias term, it becomes a little bit more involved. Since I didn't notice any practical difference in these two approaches, I decided not to spend time explaining that it's also possible to do it this way. I hope that answers the question.",Machine Learning Zoomcamp,2022,
604191,"Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.",Machine Learning Zoomcamp,2022,
558912,"The most dangerous one… I can think of two. One is going for the most “exciting” and complex model, when a simple non-machine learning solution will work. For example, you can just do something like group_by and calculate_mean for each of the things. Let's say you want to predict the price of a car. What you can do is run a group_by_query from your data, and you can look at the model, make, and year, do group_by and then use this as a prediction. That's very simple. It doesn't need any machine learning at all. And it's already a good baseline. Then the logistic regression model that we covered is another good baseline that will probably improve over the previous baseline. Maybe that's enough for a start. You don't need a complex XGBoost model (maybe XGBoost is not that complex compared to deep learning). But it's very tempting to go with the fancy new solution and Skip the boring ones – the group_by one, linear regression, decision tree – skip them and go to the more “fun” ones. So that's something that data scientists sometimes do. 
Then another one is chasing new tools – again, trying new tools instead of focusing on the business problem. I think it's common across all engineers, not just data scientists, but also software engineers, frontend engineers, and so on. I would again, go to our YouTube channel, where there is a talk by Elena, How Your Machine Learning Project will Fail, where Elena talks about the many different reasons of how things can go wrong in a machine learning project. Then this one is somewhat similar, but here Doug talks about search projects, not just machine learning projects, but specifically search. The one with Elena is more general. So check that out. I think that covers the most “terrible” one.",Machine Learning Zoomcamp,2022,
8144,"Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.",Data Engineering Zoomcamp,2023,
129019,"I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.",Data Engineering Zoomcamp,2023,
487541,"I can't make this decision for you. I don't know enough of your background to answer this question. It can be both, too. Why not? Probably, the more relevant one would be the MLOps Zoomcamp. Here, I am assuming that you are more interested in ML engineering for this course. And if you're interested more in ML engineering, then it's definitely MLOps Zoomcamp. In the end, it's really up to you. There is no right or wrong answer, so I would suggest doing both. [chuckles] My opinion is a little bit biased here. Also, you only have 24 hours in your day.",Machine Learning Zoomcamp,2022,
169534,"You do exploratory data analysis – you look at the data, you see if anything is wrong there, and then you write some code for preparing this dataset. There is no silver bullet for that. You just need to look at the data to see if there are things that are wrong, and then see how you can programmatically fix these things with your code. For example, for extra large numbers, there are multiple things you can do. You can just throw away the observation completely, or you can do capping. Capping is when you say, “If this value is larger than X, you keep it at X.” There are many ways you can do it, but I think these two are the most typical ones.",Machine Learning Zoomcamp,2022,
591677,"It depends. In some problems, you can already build a good model with 100 examples. In other cases, 10,000 might not be enough.",Machine Learning Zoomcamp,2022,
610246,"Yes, you don't need to do weekly homework assignments for the certificate.",Data Engineering Zoomcamp,2023,
60829,"Six and the first one kind of has two parts. So I guess, seven.",Data Engineering Zoomcamp,2023,
829845,"Quite often. I don't use them personally. We use them for running A/B tests. Let's take a churn prediction example – churn detection. We have one model for detecting churn and then we roll out a new model. Our ultimate goal is increasing revenue. So “What is our revenue in one month?” From these experiments, we know that our model has this precision and has this recall, it has this AUC, it has blah, blah, blah, all these metrics. But we don't always know how exactly these numbers translate into real revenue at the end – how much money the company will have. What we can do is take these two models and then send some of the customers to the old model (variant A), and some other customers to the new model (variant B). Then you can measure what the revenue will be after one month, let's say. That would be a hypothesis test. In this case, I think, it's a t-test that checks that the means are significantly different between two groups. And then p-value will tell you if you should reject this hypothesis or not – whether this new model is actually better, or whether they are the same. But I don't sit with pen and paper and calculate all these p-values. There is a statistical engine that does that for us. Of course, t-tests don't always work. There are some assumptions with this, like there’s normal distributions and these statistical engines usually select the best test for a particular situation. Maybe I will recommend for you to check our YouTube channel, where we have three talks about A/B tests. I was thinking about this one – Setting up an A/B testing Framework from Agnes. Agnes is my ex-colleague – we used to work together at OLX. She describes the framework we used at OLX for running A/B tests. This conversation with Jakob is about how his company runs A/B tests and this one with Sadiq shows how to set up the infra for doing this. It was a few weeks ago, so many of you probably have seen it. So I was referring to the one with Agnes. And actually, this one about Growthbook is another interesting one. They don't use traditional statistical tests – they use Bayesian tests. It's an open source library for doing these Bayesian A/B Test. So you can check that out too. Yeah, check this out. You'll find a lot of useful stuff. So the answer to your question is ‘yes,’ that happens often. Sometimes these questions actually come up during the interview, but not for ML engineers. I think data analysts and data scientists need to do these kinds of things more often than ML engineers.",Machine Learning Zoomcamp,2022,
139004,"Yeah, I think log transformation is suitable for right-skewed distributions. The same we had in the course. Because for the left-skewed, I don't actually know how it will work. You can just apply it and see what happens. I think that's the best approach. Maybe the answer is “it depends”. You can just try it and see what happens. Then based on what you observe, you can decide what to do next. If you see something that remotely reminds you of a bell-shaped curve, then you're on the right track? I hope I answered this question. 
I don't know about any better transformation methods available for skewed data. I think logarithmic transformation is a pretty standard one when it comes to pricing data, that's why we did include this in the course. If you work with prices, it's very likely that you will have to use this one. But when it comes to others, maybe it's less common. There are things like the Box Cox transformation – there are quite a few of them. But maybe they are not as widespread as this one.",Machine Learning Zoomcamp,2022,
50203,"I wouldn't call it an issue, actually. It's more like a feature than a bug of RMSE. Let's say you have a model that predicts price and you want to know, on average, how wrong your model is in dollars. That's the purpose of RMSE. Or if you're predicting the age of somebody, then you want to know, on average, how wrong your model is in years. It gives us this understanding of how wrong the model is, on average, on the same scale as our target variable, while R-Squared and MAPE do not give us that. They serve different purposes. 
I never actually used R-Squared in practice. For me, it's always misleading. I cannot really interpret it quite well. But MAPE is a good one – it has its own problems and I think we talked about that in the last Office Hours. It's good if you look at multiple metrics at the same time – you look at RMSE, you look at MAPE, you perhaps look at mean absolute error (MAE) and then, based on that, you make some decisions whether to use this model or not.",Machine Learning Zoomcamp,2022,
115190,"Yes, more or less. Some parts will require Prefect, but I think you will still find a way of getting the answers without Prefect. I believe so. So technically, yes – it's possible. But then again, we do not officially support Airflow in this iteration. Other students who use Airflow for homework for week 2 can help you and most likely will help you. We also have some frequently asked questions from the previous iteration. Do this if you know what you're doing. If you're just learning right now and you don't feel very adventurous, then stick to Prefect.",Data Engineering Zoomcamp,2023,
590047,"Tim 
I think “ML engineer” is a fairly new title. I do think it implies that they should work the whole pipeline. [chuckles] I am not sure that the industry is going to end up with these individual people who can do the whole thing and be really good at it. That doesn't seem like a scalable approach to me. I think eventually you'll find that the tools will get better, or good enough, where people can specialize and not have to know every part of the pipeline. Not know every part of the pipeline in a really deep way, but I think it's helpful for everyone to know the pipeline at a high and medium level to be able to collaborate better. Collaboration, especially in ML, is difficult because there's so many people involved.
Alexey 
What I see in practice is that often data pipelines are built by data engineers and then there is a bit of feature pre-processing, feature engineering – all that – and usually, the data scientists take care of that. And then ML engineers focus more on deployment and then scaling and then DevOps, MLOps people support more on the infrastructure side, like “How can we make sure that Kubernetes is running, that all the logs are saved, that we can monitor the whole thing?” At least this is what I see around me in the company where I work. I talked to a few other companies and they have a similar setup. Sometimes ML engineers work in detail on training pipelines as well, but I think it's more common that data scientists do this. At least from what I see. I haven’t interviewed many companies – I just talked to a bunch of people here in Berlin when I went to have lunch. I asked, “Okay, tell me about how your organization works. What kind of roles do you have? What kind of responsibilities?” And this is what they tell me.
Tim 
I think as the industry matures, the specializations will become clearer. 15 years ago in software development, DevOps didn't even exist and software engineers were supposed to be doing the builds and the testing – everything. Then we realized, “Okay, we need somebody who kind of bridges Ops and development.” That became DevOps. And then the software engineers, as we progressed in data, were doing all the transformations and the pipelining and everything else. But then we said, “Oh, actually, we need data engineers.” So that became another thing. I think as the industry matures, the specializations will become more clear. But there will always be the early people in the industry who benefit and companies who benefit from people who work the whole pipeline.
Alexey
I think there was a term “full stack data scientist,” right? A person who can do it all. I don't know if it’s still a thing. I think a couple of years ago it was, but now we have ML engineers. Before, it wasn't like a very prominent role but now we see more and more ML engineers.",Machine Learning Zoomcamp,2022,
539366,"Why do you think it will be irrelevant? Do you think we'll just identify all churned users, bribe them with discounts, and live happily ever after? At some point, other users will consider leaving too, so we actually need to run this model regularly. We will need to update this model regularly – retrain – because the model will probably make mistakes. So it's probably a never-ending process here. We will always need to keep an eye on this model – probably in an automated way. Maybe we have a process that automatically retrains it every half a year or something. But I don't think just deploying this model and using it will solve the churn problem. Maybe I misunderstood your question. I don't know.",Machine Learning Zoomcamp,2022,
962918,"Tim
This is a really great question and I don't have a good answer for it, I think, because the industry is evolving so quickly. I've definitely seen data scientists being in charge of deploying models, probably where that data scientist is one of the only engineering resources, or the engineering resources that are there are too busy. If you're a data scientist and if you build a model, the way that you scale the business value of that model is having that model deployed as an API. If somebody has to do it and there are no resources anywhere else, then yes – the data scientists can do that and we try to make that as easy as possible. In larger teams, I think, typically not. You'll probably have at least some type of DevOps or ML engineer that will help you deploy your model. But knowing how the model deployment pipeline works, I think, is pretty important. That way, you can work better with your engineers. I think it's always nice to know at least the high and medium level of how it works and how to do it end-to-end in fairly simple scenarios. But if it becomes more complicated, I think it does take somebody with a little bit more specialized knowledge.",Machine Learning Zoomcamp,2022,
429152,"Jeff
Yeah, I can talk about this a little bit. I actually was just reading about this yesterday. I had a book in front of me and I was like, “Oh, I want to read it right from the book. Maybe I'll copy/paste it when I go grab it.” I like the Fundamentals of Data Engineering book by Reis and Housley that just came out recently. I recommend that. It is a great book that gives you a lot of background in data engineering. It has a good description of Parquet, in particular, and why it's useful. It can be compressed a lot more than a CSV file. CSV is just kind of this not very standardized way of doing things and they take up a lot of space, even if you do compress them. Parquet is a columnar format and it's very useful for more efficiently storing data, and then you can compress it, oftentimes with Snappy compression and make it even smaller. So it's a fast way to read and write data and not take up tons of space. There are other file storage methods that are even more efficient for reading fast and writing fast. But it's very good for a balance of ways to store things and it has some metadata about the columns in it that goes along with it. That's just really useful for a lot of things like putting it into a database, for example. So it's a winner. I feel like it's pretty much the standard way, if you're just going to store something in a file persisted to disk and just gonna be sitting on a disk. Parquet has become the most popular way to do that with data. But other people can feel free to jump in and add on or disagree.
Alexey
Which chapter was it? Sorry. 
Jeff
It's in the appendix at the end.
Michael
I would also add that with Parquet, you can also do partitioning to decide where the files are written to. When you read the files back in, it's not going to read all of them like you would with a CSV. Also, Apache Delta Lake is built with Parquet files. It just uses extra JSON files to track the changes, so you can just take object storage like S3 or a GCP bucket, and instead of making a full database, it just writes the Parquet files, but then you can interact with them like it's a database, which I think is just really powerful.
Jeff
Parquet is cool. Use Parquet. [Michael agrees]
Alexey
We had one issue though, with Parquet. Michael, you should know about that, because we came across this issue again recently. You have to be careful with the schemas there, because once you say that a particular column is flawed, and then if in another Parquet file, it's not flawed, but it's integer – then you might run into problems. So you have to be careful with the schema. Meanwhile, in CSV, it's just text and then you kind of convert to the proper type as you read, not as you write.
Michael
Yes, so I would say that you need to be very explicit with your schema definitions when saving your files. If anyone is interested, the NYC data – I know they updated the Parquet and there are many files with many issues in there, which is great to practice your data wrangling (if you want some punishment [chuckles]).",Data Engineering Zoomcamp,2023,
336604,"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever.",Machine Learning Zoomcamp,2022,
677072,"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. 
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. 
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end.",Machine Learning Zoomcamp,2022,
343579,"I guess, use validation.",Machine Learning Zoomcamp,2022,
323555,Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.,Data Engineering Zoomcamp,2023,
620364,"It depends, really. I use Pandas for pretty large data sets. But, it depends on how large your machine is. With pandas, you can always take your data and chunk it into multiple pieces. There is the pandas read_csv iterator – you can iterate over the rows of your CSV file or whatever. Yeah, just use pandas. Usually, in practice, more often than not, we use a database. Yes. For example, for the setup we have at OLX, we do have a database – I would call it a data lake, because at the end it's just a bunch of parquet files in S3. 
If what I say makes no sense for you, maybe check out our data engineering course, eventually (after this one is over) where we explain what a data lake is, what a data warehouse is, and things like this. 
At OLX, we have some sort of database (some sort of storage) and we can run some queries on top of this storage. Then, the result is often saved as a CSV file or as a parquet file. Then what I do often, for example, is just fetch this file, download it on the machine where my jobs are executed, and read it with pandas, and then apply the function that I need (like a model that predicts to this pandas data frame) and then save the result somewhere. 
It's also possible to do it with a database. Actually there is a very nice library that I will show you right now. We will have a workshop called Effective Machine Learning Inside Your Database (September 21st 2022) There are tools that allow you to run machine learning inside your database. If you're watching/reading this past the 21st of September 2022, there will be a recording of this workshop on our channel. This shows how to actually apply machine learning in a database without leaving the database at all. So there are many, many options that you can do here. Again, there is no right or wrong answer – it depends on your case – but pandas is totally fine. 
There's also the common one – Spark. Spark is also used quite often. Again, maybe after this course, you can check our data engineering course. There, we show how to use Spark in one of the videos there. I am the instructor for the Spark module. In one of the videos there, I show how to apply a machine learning model to a Spark data frame. But maybe not right now – after the course – because right now you already have a lot on your plate, so don't get distracted. It's important to keep focus. If you intend to go through the course one-by-one, maybe it's better to focus on the course. Make a “to watch” list to come back to after finishing the course.",Machine Learning Zoomcamp,2022,
747722,"Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.",Data Engineering Zoomcamp,2023,
723980,"Jeff
There, really, the sky's the limit in terms of the ability to paralyze work as you go. We have integrations with Prefect and Dask and the ability to use Kubernetes to scale things out. Absolutely. Check out the collections catalog to see what we have right now. You can also make your own custom block if you want to, from a custom collection, if you want to contribute back to everyone else. So, Ray and Dask are two task runners that we have – two libraries that are popular, that we integrate with, where you can spread out your operations over multiple cores and over multiple machines. Those can be popular for those kinds of use cases.
We saw how to use a Docker block. We also have a Kubernetes block. So that is something you can pull. We use the infrastructure of the Docker container. You can use Kubernetes Job. That’s kind of the easiest way to go with things. That's totally available right here, and then things will kick off in your cluster.",Data Engineering Zoomcamp,2023,
432336,"No. From what I understood, from what Ankush said, you don't need to know Java.",Data Engineering Zoomcamp,2023,
843792,"I mean, I can give you my idea on what data analysts do, but it doesn't mean that everyone follows this idea. You should probably actually start with job postings, and then think, “Okay, these are the companies I want to work for. What kind of skills do they require?” And then go from there. That would probably be more useful than just my opinion. That's, “in my humble opinion”. But I'll try to answer it anyway. Data analysts should know SQL very well. That's their main tool – SQL. By SQL, I don't mean just select star from a table, you should also know “joins,” you should know “group by,” “having statement,” “Window functions”. If you know window functions, that's great. Every time I need to use a window function, I need to Google it, but analysts usually know them much better than data scientists, because they need to do SQL a lot more often. Another tool that analysts need is Python or R. I think Python makes more sense these days. But again, it depends on the company. In some companies, the analysts use R. There, it's the usual. Let's take Python –you need to know tools like pandas, some visualization libraries like Seaborn, matplotlib. I don't remember the name of the interactive library, but if you Google, you will find it (In the comments, people suggest Plotly, which is an interactive plotting tool in Python. It’s quite cool. I’ve seen some data analysts at OLX use it). So doing simple analytics in Python and then plotting the results is useful. What I also see our data analysts at OLX do (we call them product analysts) is also use Tableau a lot – Tableau or any other dashboarding tool. Again, it depends on the company. Many data analysts spent quite a lot of time building dashboards in Tableau. Sometimes, it can also be an ad hoc analysis in Excel, or first in SQL and then putting data in Excel and then saving it over. SQL, the most important thing, then Python and data analytics tools in Python such as pandas, and then dashboarding. That probably covers like 90% of the work. But also, it's important for data analysts to be able to communicate. I hope that's helpful. It's a bit off-topic for this course and I am not an analyst. I work with analysts quite often, but I'm not an analyst, so please treat this with a grain of salt and maybe talk to real data analysts about that.",Machine Learning Zoomcamp,2022,
348726,"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it.",Data Engineering Zoomcamp,2023,
943509,"Okay, I can. All the course videos you will need are here, Data Engineering Zoomcamp. And all the live videos, all the homework, everything that is specifically related to this cohort, will be in the Data Engineering Zoomcamp 2023. But you don't actually need to use a playlist because all the videos are linked here. But if it's more convenient for you to use a playlist, then this is the playlist to use. And this one is just materials for live streams, homeworks, and so on.",Data Engineering Zoomcamp,2023,
127825,"Alexey
Yeah. They are kind of like folders and there are files in these folders. They can be stored in S3 or HDFS or Google Cloud Storage. Indeed, we save the data before transformation and if we want, we can also save the data after transformation in a data lake. As to why we save transformed data in a data lake rather than just a data warehouse – sometimes it's just cheaper. In a data warehouse like BigQuery or Snowflake, you need to pay a lot for storing the data there and for accessing the data. If you just save the data in a data lake, your indexing – the way how you can speed up the queries – is limited, so you will probably need to read all the files in a folder, but usually it's cheaper, especially if you are doing something like a full scan. When you need to access all the data for a day or inside a partition, then data lakes are usually much cheaper. Usually, that's why we save transformed data in the data lake – mostly because of that.",Data Engineering Zoomcamp,2023,
481139,"I would ask myself, “Why is my model that large?” Is it because it has like 5000 trees in my XGBoost model? Or 10,000? And if the answer is yes, I will try to cut the number of trees. Usually, in situations when I find myself having very large trees, it is when my learning rate is very low, so each three is only slightly improving the previous three. Here, what you can do is just increase the learning rate by three times (for example) and then reduce the number of trees by three times. Then you will have roughly the same performance or maybe a bit worse, but then your model will be one gigabyte instead of three. 
Another thing you can do is reduce the number of the depth of the model. And you can play with this more – so maybe increasing the learning rate and making the tree shallower. That could also help. That's typically how I would do it. Otherwise, if this is a linear model – the only reason your linear regression or logistic regression model could be that huge is because you have a lot of categorical variables. You have a weight for each of them, so maybe you can reduce the number of variables you have by using the techniques we talked about half an hour ago. Maybe you can talk about your specific case in Slack and we can figure out what the best way for you is.",Machine Learning Zoomcamp,2022,
528051,"It really depends on your background. If you're already a software engineer, then maybe data engineering will be easier because you don't need to spend a lot of time studying machine learning. But it also depends on what you like more. It depends, as I said – Let's say if you have a PhD in physics, maybe data engineering will be more difficult for you than data science. But if you like data engineering more than data science, just go for it and do data engineering. If you like it more then it will not matter for you if it's harder or not.",Machine Learning Zoomcamp,2022,
461585,"Jeff
The Prefect agent is what kicks off your workflows. It says here, “Go use the infrastructure as specified in the deployment.” So it's constantly there, it's pulling for work. The deployment is this concept that has all the information that is needed in order to actually run a workflow. It will have information such as a way to find your code on your own infrastructure, so your flow code has to be available somehow. Again, it's not stored on Prefects servers if using Prefect Cloud, it's not stored in Orion – it is stored on your infrastructure. 
And then in the deployment, you’re also going to have your storage, you're gonna have your infrastructure, and you're going to have just some other metadata that could be needed there. But the deployment gets put into a work queue or now a “worker pool,” as we're calling them. And the agent pulls that queue, looking for work. So when there's a scheduled flow run, then an agent picks that up and says, “Alright, now go run the code that’s specified in the storage in this infrastructure that’s specified.” 
The Prefect Profile is a nice way to just flip back and forth between different workspaces, if you're working locally. With a Prefect server, that's great if you're hosting that. Then you can also use the profile to switch to the cloud workspace. And if you have multiple different cloud workspaces, you can specify those with a Prefect Profile. I'm just gonna go to the docs here and search for profiles, which is under Settings. This is where you want to go. You can check out information about Prefect Profiles, and how to use those, where to switch back and forth. There’s profile files down there, so a number of things. The two most common things I use are Prefect Profile LS Prefect, in order to list the profiles and Prefect Profile use in the name of the profile I want to use to switch back and forth between the profiles.",Data Engineering Zoomcamp,2023,
611343,"This comes up pretty often. First of all, I will recommend going to our YouTube channel where you will see this talk from Kishan. Actually, in this talk, I already saw a book I was about to recommend. This book is called Forecasting Principles and Practice. This is the book you want to read if you want to learn about time series. It's in R, but that's fine. If you're doing time series, you probably want to stick to R anyway. But many things work in Python as well – for example, this exponential smoothing. You can implement it in Python yourself. This is like the easiest, in my opinion, the most simple method for doing time series forecasting. You can implement this and it's actually quite fun to implement and tweak it to see how it works. So if you have some time and want to learn more about time series, try to implement this exponential smoothing.",Machine Learning Zoomcamp,2022,
917151,"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now.",Machine Learning Zoomcamp,2022,
503843,"This negation makes it a bit difficult for me to understand the question, “exploratory data analysis should not be performed before the split.” Technically, again, coming back to what I said, you set aside your test data and you pretend it's not there. You just forget about it. The only reason why you want to use your test dataset is to test your model, which happens once in a blue moon – very infrequently. Then you can set this aside, you pretend it was never there – you don't know about its existence – and then you can do whatever you want with the remaining data. You can do exploratory data analysis on the train and validation combined, you can do exploratory data analysis only on train, you can do pretty much everything. 
Typically, I think the right approach would be: you set aside data for test, you forget about it, then you take what is left, you split it, you do exploratory data analysis on train. This would be the right textbook approach, I would say. Then if you need to handle null values, you handle null values. You explore your full train dataset and you see, indeed, there are some null values. This means that for your preprocessing step, you need to do the same thing for the test dataset. Basically, everything you do to prepare your data for your full train – you need to reproduce it to repeat it on test as well.",Machine Learning Zoomcamp,2022,
833736,"I think it's confusing because there is no video 1.6 about setting up the environment. It's just a page. There was no video – that's why it wasn't removed, it just never existed. But if you actually want to find this Docker video, then you go here and here is the video. This video is not from this course – it is from a different course. But the steps you need to do here are very similar to what we need to do in this course. 
You can just go through this course and this video shows how you configure a remote machine on AWS. I hope that answers your question. It was never a part of this course. Maybe if somebody wants to contribute – maybe not everyone likes watching these videos – so if you want, you can just maybe go through this video and contribute a step-by-step guide for this. If you're interested – if somebody might need this.",Machine Learning Zoomcamp,2022,
321199,"By “increasing the AUC range,” you mean that the gap between X_train and X_val becomes wider. I would not worry about this too much, as long as your score on validation is good and that you're certain that your model is not overfitting. Then it's fine. Oftentimes, I don't even look at the training score. So that's fine. As long as your relation is good and you trust your validation, then it's fine. If you're uncertain about your validation, what you can do is run cross-validation. Then if you have five scores instead of one, and you can see the standard deviation of this course, you can be more certain that the model is behaving or not behaving well. Pay more attention to validation rather than train.",Machine Learning Zoomcamp,2022,
926441,"This event is a live event. So, yes, there were live events and there will be others, if this is what you meant. If you meant something else, please let me know what exactly you have in mind.",Machine Learning Zoomcamp,2022,
948185,"What is the difference between prediction and inference? I don't think I know. To me, they both sound synonymous. They're used for both, I guess.",Machine Learning Zoomcamp,2022,
40536,"I'm trying to parse this question. If your VSCode says “no module flask” then what you need to do is look up how you connect your Visual Studio Code to your Python Interpreter. Probably something like this. What you end up doing at the end is – select an interpreter, the one in your virtual environment, and then this interpreter knows about flask and all these things.",Machine Learning Zoomcamp,2022,
856125,"Since we had more signups this year than last year, I hope it will be more than 100. But let's see.",Data Engineering Zoomcamp,2023,
290881,"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you.",Machine Learning Zoomcamp,2022,
141517,"The best resource for practicing Python is your terminal. Go to your terminal, open iPython and start practicing it there – or maybe not iPython but Jupyter Notebook. I guess this is not the answer you were looking for. Maybe try to think about what kind of simple things you can automate and automate them. There must be some things that you do manually every week (for example). Try to write a Python script to automate them. Maybe something similar might be… I don't know, I am running out of ideas right now. 
But if you just sit down for about 20 minutes and think “What kind of things can I automate? What kind of things can I do?” And then try to use Python for that. That's the best resource. Of course, maybe you want me to suggest a website where you can practice Python. Maybe LeetCode could be good. They have different problems. I don't even know how it works these days. You have some problem description and you try to solve it with Python. That could be useful, let's say, if you want to work at a company like Amazon because they will torture you with these kinds of problems during the interview. At the same time, you can also practice Python. 
Another resource could be Code Academy. I think they have some Python courses, so maybe you can check that out. I see that it says “pro” so you will probably need to pay for this one. They might have free ones. Anyway, I think the best way you can practice Python is by doing little projects – writing little scripts.",Machine Learning Zoomcamp,2022,
170074,"Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.",Machine Learning Zoomcamp,2022,
445288,"Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.",Machine Learning Zoomcamp,2022,
311327,"Alexey
Well, let me try to see. Google “data cleaning checklist”. What do we have here? Yeah, maybe you can go through this thing. Seems quite good. There are also tools like, for example, Soda, or Great Expectations. Tools like that. There are quite a few of them. They look at data quality and they have an existing set of checks that you can just use. I think the checks that they have by default, or the checks they have in the libraries, make a good checklist.",Data Engineering Zoomcamp,2023,
125560,"The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.",Data Engineering Zoomcamp,2023,
944536,"For your project, what you can do is just select one of them – you don't have to do two. Another option would be to train two models. Let's say you have one XGBoost model for predicting target one and then you use the same features for predicting target two. Then you have two models. This is a perfectly viable approach. Then another approach would be to use a deep learning model or a neural network. With a neural network, the output that you have could be two numbers – your model will output two numbers, two targets. I guess that's what you referred to when you said “double”. 
By the way, we don't cover a use case like that in the course. We don't show how to create a model with two predictions. But if you do some research, you will find it. With Keras, it's actually relatively easy to do that. You can just Google or use your favorite search engine to find this. I think both of these options are good. It really depends. I don't know what the best way is. 
The upside for having a neural network is that you have just one model. But the downside is, let's say you want to improve the accuracy of predicting one value, but not the other – how do you do this without affecting the other prediction? Sometimes you might want to have this separation and you want to have independent models. I would actually go with two models.",Machine Learning Zoomcamp,2022,
829530,"Yeah, good idea. Thank you.",Data Engineering Zoomcamp,2023,
651256,"Well, in case of classification, if the bias term is negative (for example, in case of churn) then it means that the client is more likely to not churn than churn. It’s similar in linear regression. Again, it depends. If your values are only positive, then negative bias terms can be suspicious. But if you sometimes have negative values and positive values in your data, then nothing is particularly wrong with that. So, it really depends on your problem.",Machine Learning Zoomcamp,2022,
323509,"One epoch consists of multiple iterations. Remember, in the code – if you go to the course and open deep learning and open a notebook. Go to where we start training a model. This thing (batch_size=32) batch size. When we specify batch size, it means that we take our entire dataset, and we chunk this dataset into batches of 32 images. It's actually called step, not iteration. Then each step would be fitting a model on that batch, specifically. And when you go over the entire set of chunks, then you finish your epoch, usually. Usually, an epoch is going over the entire dataset once. It's not strictly correct. You can define epochs differently. You can define epoch as 100 steps or 1000 steps – it’s up to you. But usually, in practice, you say that one epoch is one iteration over the entire dataset. I guess when I say iteration/epoch, to me, it seems like I can use them interchangeably. But the difference between epoch and step is what I just described. Maybe if you find a different definition, please share it in Slack.",Machine Learning Zoomcamp,2022,
879300,"Tim 
Yes, actually. One of our developers was asking about that the other day. I think we'll try to do something like that.
Alexey
Because I think the idea behind Hacktoberfest – it's very similar to the “good first issue” that we saw. In Hacktoberfest, you need to add an issue, but then there is also an idea that you give some sort of guidance to the person who is contributing. You can tell them how they start, what they should do – it's a bit of hand-holding, but that is my understanding. I think to take part you just need to add an issue. I don't know, maybe you need to register somewhere explicitly.",Machine Learning Zoomcamp,2022,
434278,"By “unsubmitted” you mean “unseen” information? One thing you can do is ask the Vectorizer to raise an error. It will see, “Okay, I have not seen this value before. Let's throw an error out.” And this is what it will do. A better thing would be to use Pydantic and BentoML. In BentoML, you can include this. One of the modules this week talks about that. It's called validation. Here, Tim shows how to add a Pydantic class. Pydantic is a special framework in Python that can validate your data. In this one here, we show how to add this validation to the input data. Actually, in the example, he just removes one of the fields from the request and the model still works – it's not throwing any exceptions. But this is not always what you want. In that module, we discuss how to deal with this.",Machine Learning Zoomcamp,2022,
503162,"I'm not sure what exactly you mean here. Probably, you’re talking about container orchestration platforms like Kubernetes. We did not cover it in this course. We do cover Kubernetes in our Machine Learning Zoomcamp. If you want to learn it, check it out.",Data Engineering Zoomcamp,2023,
165997,"The extension of our file was bin simply because we decided to give it this name. It's actually Pickle, but it’s just when saving, I put “.bin” there. You could put anything there. You can put the “.pickle” you can put “.txt” – anything – it doesn't change the content of the file, which is a Pickle file. Other formats include – there's a bunch of different formats. There is Joblib, which is almost Pickle – there is PMML which is a special format for machine learning models. You can use them between different languages. 
For example, you create a model in SciKit Learn, and then you can save it and import it from a Java application. I don't use it, to be honest, because I usually write the training jobs in Python and then the serving scripts are also written in Python. So we don't have this problem. The difference is in the nuances and details. With Pickle – I don't remember if I mentioned that in the videos – there are security issues. For example, if somebody with malicious intent wants to execute some code on your computer, they can insert this malicious code into the Pickle file and when you load the pickle file, the code will be executed. Then potentially, the person (hacker) can overtake your computer. So load Pickle files only from the sources you trust. Other formats have similar pros and cons. PMML does not have this problem. But maybe it does the Py adoption. With Pickle, it's very simple – it's a built-in library in Python. You just import it, you don't need to install anything, – you just do pickle dump and then you have your file.",Machine Learning Zoomcamp,2022,
570436,"I'll show you a trick. You can thank me later. You take this question. You go to your favorite search engine, you put this question to search and you see the answers. You're welcome. [laughs] But honestly, I don't know much about these things. I know a little bit, but I might tell you something that does not make any sense, so I will not even attempt to do that. I'm not really an expert in NLP. Secondly, with Google, you can find a lot of good resources. Maybe I will show you one good resource which is CS224N. This is natural language processing with deep learning, which talks about all these things. Word vectors, recurrent neural networks are RNNs. So go through this thing if you're interested in learning more about that.",Machine Learning Zoomcamp,2022,
586828,"The load part is when the data is already processed and you load it to a data warehouse. We don't touch a data warehouse in week 1. So I guess that may make it extract, or transform, or both. I guess that's the extract and transform part. The transformation we do is not really a transformation. We don't really do anything with the data. But if we did, then yeah, it would transform as well. I don't think it's really important. The important part is that we do something with the data and the data ends up in our database. Actually, we do load it to the database, so it's also load. I confused myself. So… [chuckles] I think it's all the parts. Anyways, let's discuss it in Slack.",Data Engineering Zoomcamp,2023,
644557,"When it comes to scaling, for example, when we talk about AWS lambda, you don't need to worry about that. When lambda gets a lot of traffic, a lot of requests, and one lambda cannot handle them, it automatically starts another lambda function in parallel – another instance of a lambda function. So you don't need to worry about any of that, lambda is doing its own thing in the background. But as a result, what you get is it scales – it can handle more traffic. If you want to maybe have control over this, if you want to scale it yourself up and down, then in Kubernetes, there is the Auto Scaling group. You will see in the Kubernetes homework how to actually do this. Also in module five, we talked about Elastic Beanstalk, it can also automatically scale. When it sees a lot of requests coming in, it creates another instance and there is basically a load balancer. I think it's called Elastic Load Balancer, which forwards the request to multiple instances. When it comes to GPUs – in the course, we do not show how to use GPUs with Kubernetes. But if you Google it, you will see how to add nodes with GPUs to your Kubernetes cluster (to eks) and then, based on that, you can scale – you can create more machines with GPUs. If you need that, of course.
As for batch inference, what we talked about here is what I call “online inference”. In this course, we only covered online inference, and by “online,” I mean we create a web service and it's like an API. You get the request, the service processes this request and replies with a prediction – a response. This is what I call an online web service. There is another one which is called offline and this is what batch inference is, usually. There is also what we saw in Bento. In bento, there is a different kind of batching – it's still online, it just takes all the requests, puts them together in a batch and then it's still a web service. There is another way of serving, which I usually call “batch serving,” or “offline serving”. This is when we have all the data in our, let's say, S3 location or somewhere and then we just load this data with something like pandas, we apply the model, and we save predictions. That's also an option if your use case allows it. If you want to learn more about that, we actually discuss this topic in more detail in the MLOps Zoomcamp. There is a module about deployment, where we talk about three ways of deploying: web service, the one we use in this course (ML Zoomcamp), then there’s streaming, when you have a stream of data and you react to events in that stream. Then the third one is batching. So if you're interested in learning more about deployment, definitely check out MLOps Zoomcamp.",Machine Learning Zoomcamp,2022,
348853,"Yes. For a time series problem, you should never shuffle. For a time series problem, you should do a time-based split. Let's say you have data for one year, you take, for example, the first nine months for training, then two months for validation and one month for test. Or something like that. You need to do a time-based split here and shuffling is wrong. Because if you shuffle, what can happen is that you can accidentally kind of see into the future. In your training dataset, you might have a data point from December – your model will see into the future, and it's not supposed to do this.",Machine Learning Zoomcamp,2022,
920377,"Yeah, you can think of these as two attempts. Not two projects, but two attempts. If for some reason, you cannot take part – you're busy, you have a vacation, something happened, you get sick – and you cannot deliver the first project in time, then you get another chance. This is why we have the second attempt. Or for example, if you fail your project, then you also get another chance. If you don't do the first project and you fail the second attempt, then unfortunately, there is no way to resubmit it. It will be two attempts, but one project.",Data Engineering Zoomcamp,2023,
276568,Maybe you can just go ahead to our repo and create a pull request. That will help. That is certainly useful and you will help immensely if you just go and create a pull request with that and we will just accept it. Great idea. Thank you.,Machine Learning Zoomcamp,2022,
37944,"I believe we have a course that is just about that and that course is called Data Engineering Zoomcamp. So if you want to know what to do with cases when the data is large, check out our Data Engineering course – Data Engineering Zoomcamp. Maybe I’ll answer this quickly. So what to do when the data is too large? There are multiple things you can do. First, you can sample your data – meaning that you just take a part of this data and discard the rest, and you train your model on that. That is fine. Then you can go distributed –you can use distributed computing for accessing all the data, which is more complex. You should have a good reason for doing this because, oftentimes, training a model on a sample is sufficient. But if you know that you will get better accuracy, better performance, if you train on the entire dataset, then you should consider distributed computing – things like Spark and so on that we cover in the data engineering course. 
If data comes in real time – I don't really know what you mean here. But again, if data comes in real time and you want to score it in real time, then this is something we talk about in the data engineering course. We cover it in the streaming lecture and also in our MLOps Zoomcamp. In our MLOps Zoomcamp, we cover streaming, which shows a case when there is a stream of data, and how we apply the model to this stream. This is a course I recommend taking after the ML Zoomcamp – the one we're doing now. So you can finish this course and then go to MLOps Zoomcamp. When it comes to web service – I don't know if I'm answering your question right now, but we will cover this in deployment. This is for applying the model to your data, for predictions. If your training data comes in S3, in real time, then you just need to collect this data, save it somewhere, and then train. This is where Data Engineering Zoomcamp comes in.",Machine Learning Zoomcamp,2022,
806069,"It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.",Machine Learning Zoomcamp,2022,
820366,"I use Anaconda for prototypes. I first create a prototype, just starting my Jupyter Notebook with the base environment from Anaconda. When the prototype is ready, the notebook is ready – I create the virtual environment. Of course, in theory, I should have first started creating the virtual environment, but I don't always know all the packages that I will need in advance – installing them one by one, I’m too lazy for that. So what I do is just use the base environment with all the packages I potentially need. But then, when this kind of proof of concept prototype is finished and I know exactly what I want to do – I want to turn this notebook into a script and this is when I create a Pipenv environment. 
So I start with Anaconda, I do everything for the prototype in Anaconda, and once the prototype is ready, I create a Pipenv environment ( a Pipenv file) and then put things there. Basically, the setup we had in this course is what I follow in practice as well. This is my usual environment manager – it's Anaconda for everything and for specific projects, I create Pipenv files. Again, in theory, maybe the recommended approach would be – when you start a project, you start with a clean virtual environment and then you first install NumPy, pandas, and whatnot (what you need) and then you add package by package when you create a prototype. I find it time consuming and I'm a lazy person, so that's why I cut corners a little bit.",Machine Learning Zoomcamp,2022,
992927,"Well, I'll tell you a story about my book. In the book that I wrote Machine Learning Bookcamp, I thought that it would be a good idea to include a chapter about Kubeflow Serving. So I did that. It was actually at the end. I wanted to write a chapter only about Kubernetes, but then I thought, “Oh, it could be a good idea to also talk about Kubeflow Serving.” So I included that and it took a couple of months for the book to go to production, meaning they turned all these documents with chapters into a book. 
By the time the book was published, Kubeflow Serving no longer existed because it got renamed to K Serve and most of the stuff I was talking about in the book no longer worked. Books on new technologies become pretty… not irrelevant, and maybe not obsolete, but maybe slightly outdated. Therefore, the last part of chapter 10 in my book is outdated now – you can only use it for this old version of Kubeflow Serving. 
It’s the same with even this course. We did this K Serve model and by the time it finished, there was a new version and some things stopped working there, so we needed to update that. I want to thank Max Payne (I know it's not his real name). He was very helpful preparing things and he left a few notes there. So thanks a lot. Because of these notes, now it's actually up to date. The video might not be, but if you look at the notes, it is. I recently checked it and it's still working. Books do become outdated. 
But the rest of the stuff I put in the book is actually not outdated. Kubernetes still works. Flask still works. Maybe the version of Python we used (I think it was 3.7 in the book) I wouldn't use now. I would use 3.9. But most of the concepts are fine. So yeah, some things changed, but not so fast. Some things change fast. There are books, for example, Elements of Statistical Learning, that even though most of the content is pretty old, if not everything, it’s still relevant. But because it's theory – theory maybe doesn't change – fundamentals maybe don't change as fast as new technologies.",Machine Learning Zoomcamp,2022,
820131,"I guess you're talking about how to select the number of epochs. The way I do it is just run it for 100-200 epochs, and I leave it training. Then I add checkpointing. When I come back, I see what's happening. The problem sometimes with more epochs is the discrepancy you have between the training set, where it often overfits to the training set. You might have 100% accuracy on the training data, but then it's let's say, 70% accuracy on validation. That might be problematic. If I compare this to a case... I'll take some notes so I don't get confused. Let's say you have two models. You have Model A, which you trained for, something like 100 epochs and it has 100% accuracy on train and it has 60% accuracy on validation. That's your Model A. Then you have Model B, which you trained for 10 epochs. It has 70% accuracy on train and 60% accuracy on validation. Here, the difference between train and validation performance is much smaller for the second model – for Model B. It's only a 10% difference. But for the first one, it's 40%. I would go with Model B. I would be more sure that it’s not overfitting – it will not have any surprises on the test dataset. So that's the only concern. But if you add some dropout, if you add some data augmentation, then probably by epoch number 100, you will not have 100% error on train. Usually you don't, because of all this extra regularization stuff. Then it could be that the model that you trained for 100 epochs is more robust because it has seen more variations of your data. It's all case-dependent, I would say. But the main thing I would look at is the learning curve and how far apart the curve for the training error and the validation error are.",Machine Learning Zoomcamp,2022,
202611,"If you look at the notebook, yes, we use shuffle=false. For train, the default shuffle is true. Actually, if you think about this, when you go over a dataset once to evaluate it, it probably shouldn't matter in which order you go over it. That's what shuffling is controlling – when you go over a dataset, you want to shuffle it so the order is random. I think the less randomization you have in the validation dataset, the better. I think that's the reason I shuffle=false here. 
You can experiment with this – set it to true and to false – and then see if the scores you get are the same ones or not. But I think for validation, it's important that we try to stay away from randomization, just in case. I don't know what can go wrong here, but maybe something can go wrong. You’d rather be careful. That's the reason, I think.",Machine Learning Zoomcamp,2022,
292637,"Jeff
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI.",Data Engineering Zoomcamp,2023,
623923,"Alexey
I don’t know what you mean by “the next courses”. We have three courses right now: Machine Learning Engineering course (ML Zoomcamp) which is in September. Then we have this one (DE Zoomcamp) in January. And the MLOps Zoomcamp starts in May. We plan to reiterate them every year, for now. These three courses take the entire year to run and it makes it very difficult to think about if we can add another course to the courses we already have – how to find the time and how to do this. Right now, I'm figuring this out –I am often the bottleneck when running these courses, so I'm thinking about how I can just step out of the course and then let it run by itself without me. Then maybe we can focus on other courses. When this is figured out, then maybe there will be new courses from DataTalks.Club. But for now, you can enjoy the courses we already have.",Data Engineering Zoomcamp,2023,
797592,"Yes, it’s definitely okay. We actually had a problem with the script that assigns auto-generated ones. That's why that part was removed from the course instructions. Now you should create your own repo and use that for submitting the work. Sorry for the confusion. That happened before my vacation, so I thought, “Okay, I don't want to spend time debugging and figuring out what's wrong with the script. Let's just stick to the same approach we used previously – everyone creates a project in their own repo.” There are some downsides to that, but I wasn't in the mood of going through debugging. So please – it's okay. Please do that.",Machine Learning Zoomcamp,2022,
911903,"In some domains, maybe the color doesn't matter much – on the shapes matter. If it's that domain, then yes. For example, I think for clothing, it might be the case. Maybe colors don't matter much, because you can have all different colors. But on the other hand, maybe it's actually still important because you want to distinguish the background from the foreground (from the actual item). This is where the color information could be important. If you think it will give you some performance increase in terms of making predictions faster – just use validation to find out if there is any predictive performance drop (drop in accuracy) when you switch to grayscale. If you see that there is no drop, then just stick to that.",Machine Learning Zoomcamp,2022,
763858,"Yes, also because of that. But usually, typically, I'm more interested in probabilities than in hard scores. For example, this model that we just studied – the risk score. Instead of just saying, “Okay, this user (client) is going to default,” it's much, much more useful to know, “What's the probability that this user is going to default? How risky is this customer?” We have a number that’s not just binary – risky or not risky. There are multiple gradations of risk, let's say. It could be completely not risky, somewhat not risky, moderate, somewhat risky, and risky. It could be five gradations or something like this. Then it's more granular for people who make the final decision and decide if they should give the credit or not.",Machine Learning Zoomcamp,2022,
9205,"Not really. But I'm not sure what you mean by that. What kind of services do you need? It's a community. If you need some help from the community, you can ask the community for help. It could be a review of your CV, a review of your project, or just general career advice. For example, you could say, “I am a data analyst. I want to become a data engineer. I'm taking this course. What else can I do to be more attractive to potential employers?” For example, ask a question like that and then we have a career questions channel where you can ask this kind of stuff.",Data Engineering Zoomcamp,2023,
290379,"Alexey
Prefect is an orchestrator. If you need to produce a lot of data, Prefect might not be the best tool. Prefect really shines when it needs to execute tasks in a specific order, one after another. Usually the approach many people use at work, from what I see, is they use an orchestrator for orchestrating, and then you delegate all the compute to something external, like Kubernetes, AWS Batch, or Spark. Spark is an external thing. You submit a Spark job from your Prefect graph – from your Prefect DAG. Of course, you can do many things with Prefect, but when it comes to large datasets, then Prefect executors might simply not be able to handle that. This is when you need to use Spark. One does not exclude the other. You use Prefect for scaling Spark jobs, because maybe after this Spark job, you have another job that is not Spark – and Prefect knows how to execute them and in which order.",Data Engineering Zoomcamp,2023,
74892,"I always confuse the two – is a physician a doctor or somebody who does physics? Because I guess the answer would be completely different. Either way, coming from physics (from STEM) you probably already know how to program. But as a physician, as a doctor, you might not have the same skills, so this is where you need to start. Maybe learn the basics of Python. I think this advice actually applies to anyone who wants to start in data science – get comfortable with the command: things like navigating the file system like CD, LS, CP, MV and commands like that. Get familiar with using Git as well – configuring GitHub, that's pretty useful, too. Pushing code to GitHub, pulling code from GitHub – I think this is a must. After that, you can already take the course (this one). Maybe the chapters about deployment will be too difficult for you, but the rest of the chapters, where it's a more general introduction to machine learning, could be quite good. So just start with that and if it's difficult, we can think about that. But more importantly, you need to improve your Python. But since you're asking it here and it's already week nine, I assume your Python is fine. So just stick to the course and you'll be fine.",Machine Learning Zoomcamp,2022,
84493,"I don't know what you mean by that. If you want to help everyone in Slack, please do this. Nobody will say no. We're a community. We help each other. Please just go ahead and do this.",Data Engineering Zoomcamp,2023,
195641,"Tim 
Awesome question. Honestly, just in the last few days, you guys have all added a lot of value. I know you could feel frustrated sometimes when you find bugs – just little things. I think a few people found that “—reload” wasn't working on Windows systems. Just finding things like that and reporting them is actually a huge help to us. That way, we can find all the different scenarios where it doesn't work. That makes the library so much better. So I think filing bug reports helps and if you want to contribute to the codebase itself, I think there are a couple of different easy ways to get started. Certainly, if you've built a project with BentoML, we're always accepting more projects in our example folder. But there's also the tag in GitHub “good-first-issue” as well. Those are all issues that, if you comment on them and ask to contribute to build those issues, you can. A good first time issue is the good thing for your first time contributing. Then “help- wanted” is just a tag anywhere we see a need, we tag it with that. Although it may be a more advanced feature.",Machine Learning Zoomcamp,2022,
918931,"There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.",Machine Learning Zoomcamp,2022,
816488,"I don't think there is a significant difference between different sub-parts of data science. Usually companies that pay higher salaries are American companies. Let's say if you don't live in the United States – in Europe, in Asia, or in some other parts of the world that are not in the States – if you get hired by an American company, they will pay you more simply because companies are usually richer in the States compared to the rest of the world. 
It's not a rule, of course. If a company is from Switzerland, they probably will pay similar money. But apart from that, I don't think there is any significant difference between different sub-parts of data science. Just pick what you like and focus on this, then you have a good salary. Again, I’m not saying that you have to work for an American company, but if you're after a high salary, then maybe consider American companies that are hiring remotely.",Machine Learning Zoomcamp,2022,
500711,"No, it should not. Just keep working on this project. It should not affect anything. Of course, if you decide to copy the project that somebody did, then it will, of course, affect it. If people who evaluate your project will accidentally find this out, you will receive zero points and you will have to rely on the other two projects to pass the course. You will not be disqualified for the course, but for the project, you will. So don't do that. Apart from that, I don't see any problems. As long as it's not a Titanic or Iris dataset – it's not one of these popular datasets, that's fine.",Machine Learning Zoomcamp,2022,
371090,"It's like 10 weeks or something like that. If you count the second attempt of the project, it will be like 13 weeks. Quite long, but not so long as, let's say our machine learning engineering course.",Data Engineering Zoomcamp,2023,
92112,"I am afraid this is the first time I hear about this. [Alexey Googles CUR feature decomposition] No, I don't really know what it is. Sorry. I can't answer this question.",Machine Learning Zoomcamp,2022,
118163,"In summary, “Yes, you can join late. Not a problem. If you care about the certificate, it will not impact your chances of receiving one. All we care about for that are projects. We will have three projects. If you complete two out of the three projects, then you will receive a certificate.” 
For this part “Would submitting the homework late be a problem?” It will be a problem in the sense that you will not be able to do this. Once we close the form, the form does not accept any submissions, so you will simply not be able to do this. There is little point in this, because once we close the form, we also publish the answers. Theoretically, after this, you can just look the answers up in the solutions and then put it in the form. Then it kind of defeats the purpose. 
Submitting homework assignments after the due date will not be possible. It doesn't mean you will not be able to get the certificate. Let's say if you're joining this course in November, then you can binge watch everything until the end (I know some people did this last year) and then you just focus on projects for the remaining time and then you get the certificate at the end anyway.",Machine Learning Zoomcamp,2022,
154447,"You saw that Docker is one of the criteria for the project. So if you don't create a Docker file, you will not get the points for that criterion. Let's say, if you deploy to PythonAnywhere – I think for PythonAnywhere, you don't need to create a Docker file. Maybe you do… I don't remember. But, indeed, there are environments where you don't need to create a Docker container. You can still do that – you will just not get points for the containerization part. 
I think it's quite important to know how to Dockerize your image, even if for your specific project, you don't need that. We want to make sure you also know how to do this. That's why it's part of the evaluation criteria. Knowing Docker is quite important, I think, in today's world. If you want to work as a data scientist, or an ML engineer or any kind of data engineer, you have to know about containerization. So that's why. 
Also, for Lambda, there are different options. Yes, you don't necessarily need Docker. But you can also do it with Docker. There is an option where you serve a container through Lambda – it's possible. If you want to use AWS Lambda, I would actually suggest following this approach.",Machine Learning Zoomcamp,2022,
366102,"Jeff
As Alexey said many times in the class, you can use whatever tool you want for things. So if you want to accomplish things with Workflows, that's fine. There are different tools out there. I'd say the big difference is that Prefect is agnostic to any cloud. You can easily drop in different storage blocks if you want to, or have just different options. Anyone who's using Python first wants to be able to use lots of different tools. That's a good option there. With Workflows, you're kind of tied to Google's infrastructure at that point, in a pretty significant way. But use a tool that works for you. That’s my advice there.",Data Engineering Zoomcamp,2023,
654600,"Alexey
I want to show you again how to find all the deadlines. You go to ML Zoomcamp – this is our 
course page. Then you go to cohorts, and then you go to 2022, which is this cohort, where you will find the deadline calendar. Click on this link and you will see that these are the deadlines. For the midterm project, the deadline is the 7th of November and then there will also be an evaluation phase and it's due on 14th of November. Then we will learn about neural networks, serverless deep learning, and Kubernetes. That's the plan right now. Things may change in the future. This is not set in stone right now. But this is to give you a rough understanding of where we will be. Most likely, we will stick to this – but things happen.",Machine Learning Zoomcamp,2022,
7040,"I don't have a “favorite” project, but please check out the demos from the students of the previous cohort. And please check the midterm project link that I shared. You can also find it in the midterm page of the course. And maybe just come up with your own favorite. You can also share it in Slack, actually. So maybe you can see, “Okay, I went through all these projects. I really like this one,” and you can just share it with others.",Machine Learning Zoomcamp,2022,
667006,"This question is tricky, because I haven't yet published the homework for streaming. I think it's in progress right now. You can study streaming right now, but you will need to do your homework and submit the homework later, when it's released. But let's say you don't feel like studying data warehousing. Then just don't. I think you should, but let's say that you don't want to study DBT and you just want to do streaming. Feel free to skip DBT Batch and focus on streaming. That's fine. You don't have to do some models that you don't think are relevant for you.",Data Engineering Zoomcamp,2023,
158071,"They can definitely be used on text. I think if you just Google “neural networks for text”, or “deep learning for text”, you will find a lot of material. That’s a good way to start. There is also a Stanford course called Natural Language Processing with Deep Learning. It's very similar to the course I was referring to in the image classification module. But this one is more advanced – it's more difficult – and it focuses more on text. That is also a good resource. You can start learning about that by learning what text embedding is, and things like Word2vec, word embeddings, Word2vec growth and things like that. Then you can slowly progress to more complex models.",Machine Learning Zoomcamp,2022,
942610,"Okay, well, let's take a look at the criteria. Model training, reproducibility, model deployment, dependency… You can see that four out of nine of the criteria are about deployment, right. This one (reproducibility) is also about that partly. Exporting notebook to a script. Hmm. Most of them are actually about productionizing your model, not about the modeling. If you do that, you will get seven points. I don't think that will be sufficient to pass the project. So you will need to work on this as well. But it doesn't have to be perfect. If you just create a Docker file, use BentoML, and you automatically have some points here and there. Just put in some effort and you pass it. Don’t worry. But if you just submit a Jupyter notebook, and that's it – I don't think it will work. You need to do a little bit more than that.
I want to add that this is a machine learning engineering course. This is not a course about data science. This is a course about engineering. If you want to be an engineer and receive a certificate showing that you learned the engineering part at the end, you have to learn the deployment part. There is no way around that.",Machine Learning Zoomcamp,2022,
849572,"That's a great suggestion – I cannot answer it better. I can only confirm that I don't look at them. We have an applicant tracking system and I can actually look at the cover letter there. But I don't – because I don't care much, to be honest.",Machine Learning Zoomcamp,2022,
864659,"Yes, there is. If you go to the project folder you will see the criteria. This criteria is like your checklist.",Data Engineering Zoomcamp,2023,
948389,"I do not understand this question, to be honest. I need an example. Maybe ask in Slack with… I usually don't like screenshots [chuckles] when you post them in Slack, because when I open a screenshot with code on my mobile, I cannot see anything. But maybe this is a case, when you can actually include a screenshot of what you mean and let's discuss it there.",Machine Learning Zoomcamp,2022,
26045,"Yeah, I will not really check that. You are kind of expected to use the backup CSV. I don't know why the number of rows is different. Please use the backup CS",Data Engineering Zoomcamp,2023,
795105,Seaborn.,Machine Learning Zoomcamp,2022,
856840,"Victoria
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. 
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. 
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft.
Ankush
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. 
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery.
Alexey
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever.
Victoria
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] 
Alexey
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. 
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something.",Data Engineering Zoomcamp,2023,
116923,"It's possible. The sky's the limit. It's up to you. How do you sell yourself? I will tell you that I did that. I didn't finish ML Zoomcamp, of course – I finished Coursera courses and I did freelancing then. Before getting my full time job, I freelanced for a couple of years. I was also studying at university and then, when I got my first data science job, I already had a bunch of portfolio projects from freelancing. So it's certainly possible. One caveat, though – the market in 2012 was very different from 2022. But I think the need for freelancers is still there. 
I don't know how to find freelancer contracts now. Maybe it's like the usual ones, like Fiverr, UpWork, things like that. You need to have professional experience somewhere. In my case, I had experience in Java and I learned ML. So my profile was – I was helping companies with machine learning in Java. I already knew Java. Maybe that's an important piece of information that you also need to know. If you don't have professional experience at all, it might be a bit difficult, but you should probably focus on portfolio projects, not only the years of experience. If you can show the potential client that you can solve their problems, they wouldn't care if you have worked before or not. They will see your portfolio and they will understand, “Okay, this person can actually do the thing I need.” 
Maybe one more thing – I took part in a Kaggle competition that was about detecting duplicates. And then, because I was on the leaderboard, a company who had the same problem reached out to me saying “We have the same problem. Can you work with us and help solve it?” They didn't care if I had any professional experience. They only cared about my solution. They already knew from Kaggle that I could solve this problem and that's why they reached out to me. The idea there is – it's not like I was just working on this problem alone and didn't tell anyone. I was actively sharing what I was doing in public. 
That's why I encourage everyone – every single person who is taking this course – to share their progress. Because if you don't, then nobody will know that you actually have the skills. But if you do, for example, you share something about a project you did and this project could be about detecting duplicates, for example. I don't think any of you did a project like that, but just as an example. Then maybe somebody who needs to solve this problem will see this post and will think “Okay, this person actually did the project on that. Let me contact them and see if they would be up to helping us.”",Machine Learning Zoomcamp,2022,
535752,"Kalise
Yeah. Prefect Cloud is open source and there's an entirely free version of it. It is obviously hosted so you don't have to manage the time of updating your UI and everything with new Prefect versions, if you use Prefect Cloud. It also has the ability that you can have collaborators. With the free version, you also get two other collaborators that you can invite to. This means sharing the blocks and reusing blocks within the different flows that you're building with your collaborators. Then there are also additional features such as not just notifications, but automation, which means being able to trigger an action when something happens, as well. For example, say your flow fails, or maybe your agent – the work queue is unhealthy –instead of just getting a notification, you could actually pause the deployment. You could trigger an action with that automation, as well.
Jeff
I’ll just add that there's a fifth homework question. People can use Slack and just in case you haven't been there yet, sometimes Slack is pretty limiting. It’s like, “Oh, too many people are hitting us with notifications for things.” You might have to make your own Slack workspace. There are instructions in the FAQ on that, so just check that out. Originally, I set it up with email, because email is nice and easy enough to deal with Slack. But it's just a cloud feature. So if you want that, you have to have an account on Prefect Cloud, which is free. That's our email server that's running that in the background. There are ways that you can set things up yourself, but it just gets pretty involved. A lot of things like that are just kind of easier with the cloud system.
Alexey
And I guess platform engineers are happy when they don't need to maintain stuff. 
Jeff
[chuckles] Yes. You don't have to employ people to do things. 
Alexey
Yeah. I mean, even if you do have to employ them, they're still happy that they don't need to worry about maintaining yet another server.",Data Engineering Zoomcamp,2023,
546686,"It's an okay technique. I would still train the model on big images at the end as well. But, again, check it with validation and then you will see how different the models are.",Machine Learning Zoomcamp,2022,
362864,"You don't have to. You can if you want. Remember, this is optional. This is something we talked about already. This is optional. If you want to share your progress publicly, you can do this. If you saw the introduction at the beginning when I was talking about the project – I was going through the forum. We have this thing called “Learning in public links.” The score is capped at 14 points. Every day, you share your progress and at the end, you have 14 links. 
By the way, I noticed that some people try to be smart and try to “game” the system by submitting non-relevant links. Please don't do that. I understand the desire to get extra points, but remember that this is actually anonymized. Nobody knows yet that this hash is you. That's one thing. Then the second thing is… why do you want to cheat? It's not clear to me. The person who I saw – they didn't cheat, but they tried to game the script that we have for evaluating. That person will lose all the learning in public links. They will not have any “learning in public” at all. Instead of seven points for each of the previous homeworks, they will get zero, and they will receive zero points for the rest of the course. So please don't do that. I don't see a point in this. I didn't think somebody would even try to do that. But life is full of surprises, right? Please, please don't do that. There is no point in doing this. Why do you want to do this? I don't understand. Yeah, but you've been warned.",Machine Learning Zoomcamp,2022,
573442,"Yes, we will do this. I think I announced it last week. So yes, we will have it. I still have to make announcements on social media. But if you just want to register, you can go to the Data Engineering Zoomcamp on GitHub or just click here, and then you can sign up. You will get a notification closer to the starting date. For the ML Zoomcamp, we have a telegram channel. You can sign up for that too.",Machine Learning Zoomcamp,2022,
83489,"In the homework forum, you will see that it's #dezoomcamp",Data Engineering Zoomcamp,2023,
754019,"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited.",Machine Learning Zoomcamp,2022,
813016,"Zero. You can start applying for jobs right now. You don't have to have a portfolio for that. Maybe you can use your Zoomcamp projects as a portfolio. That's a very good idea. Maybe one-two should be good, but you can find a job without any projects in your portfolio at all. It also can happen that you don't find a job even with 10 projects in your portfolio. There are different situations – everyone has different backgrounds, everyone has different problems. So I don't think there’s a “one size fits all” answer to that. I'd say one or two is helpful to have.",Machine Learning Zoomcamp,2022,
614759,You will not be able to do this because they require a credit card.,Data Engineering Zoomcamp,2023,
949841,"Tim
No, I haven't. 
Alexey
Do you know what it is?
Tim  
No, actually.
Alexey
This is a tool for building machine learning pipelines. We had a workshop about Kedro here on our channel. You can check it out here. The idea behind Kedro – this is for training your models. This is not for serving. This is for splitting… for the projects for the course that you will need to do, you need to come up with a train.py file, but Kedro is a way to modularize (to break into separate chunks) this train.py file such that these chunks are more modular, I guess. You can have a separate function for preparing your dataset, for feature engineering, for doing all these things, and Kedro gives you an opinionated way of actually doing this. They give you a template, you follow this template, and then they run this. It's a cool thing.
Tim
Like a declarative ML pipeline. Got it.
Alexey 
Yeah, exactly. Eventually, at the end, it still runs on one machine. But it just gives you an easy way to split it into logical units, let's say, and also run it. It takes care of the pipeline orchestrator – it takes care of running things in the order unit.
Tim 
Right. I'd imagine there would be one final step once you've trained your model, if you like it – that would be a declarative BentoML deploy step.
Alexey
Exactly. For example, if you use MLflow, you save your model in the registry, and then another step would be to export it to Bento. Or immediately to Bento, skipping MLflow. Then, probably, there could be a conditional – if performance in validation is above certain thresholds, then do that. Otherwise, don't do anything. If you're interested in Kedro, check out this workshop with Merel.",Machine Learning Zoomcamp,2022,
953341,"We haven't evaluated your first homework yet, because you still have some time to submit your homework. After it is done, you will see the evaluation.",Machine Learning Zoomcamp,2022,
151441,"For Section 7, Windows should work. I tested it in, actually tested it in BSL, in Ubuntu. But on Windows, it should also work. If you want, I can test it, but I think that some students already did it and it worked on Windows. Remember that if you do this on Windows, (this is actually in our FAQ) 0.0.0.0. is not the same as local host. If you just run BentoML serve, and then the rest of the command, it will say that you need to open this URL (http://0.0.0.0:3000/) but this URL will not open – you need to replace the zeros with the local host. There is already a section in the FAQ about that. Apart from that, I don't expect to have any problems on Windows, so you can keep using Windows. As long as you have Docker installed, you should not have any problems.",Machine Learning Zoomcamp,2022,
723430,"No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo",Data Engineering Zoomcamp,2023,
461727,"Alexey
You will see links soon. I think we already have the first one, but we just need to put it on GitHub. We will communicate over everything in Telegram and then the automator will post the link from Telegram to Slack. So you will see if you're in Slack.",Data Engineering Zoomcamp,2023,
28519,"I think this is actually two questions. How do you network with people? I think we even have a few podcast episodes about that. I like the one with Juan Pablo, From Math Teacher to Analytics Engineer, where he talks about networking – how to meet people at meetups and so on. So maybe check out this podcast. What you can also Google is a set of ice breaking questions. When you approach people at meetups, you can just ask them these ice breaking questions. I'm not sure if I'm very good at this to give you any good advice. But yeah, please check our podcasts. The one with Juan Pablo is the first one that comes to mind. Maybe we should have a special podcast episode that is for networking. 
Then how to create an eye-catching profile for other people and companies? We also had an episode about that. There are quite a few. I like this one: Standing Out as a Data Scientist with Luke. By the way, we will have another follow up episode with Luke in January, so check it out. Luke says (if I can give you a TLDR for this) you need to pick a niche and do a few projects in this niche. You also need to understand what companies in this niche you are really looking for. This will make you attractive to these companies in this niche.",Machine Learning Zoomcamp,2022,
728460,"Depends. [chuckles] Again, sorry for answering this way. So, what is your end goal? Do you want to be a researcher or do you want to work as a machine learning engineer? Depending on what you answer, the answer to this question will be different. If your answer is “machine learning engineer,” then the answer is you don't stay up to date with recent advances in AI. You don't have to stay up to date, let's say. If you want, just go on Twitter and see what's trending and then read these articles. Or Reddit, r/machinelearning, for example. But if it's too much for you, then just don't stay up to date. Nobody is forcing you to do this. 
To know the basics well enough and for actual practical applications of machine learning, I recommend attending conferences – something like PyData, for example. I think PyData has chapters everywhere, so you can find a PyData conference in the area where you live, and then just attend it. Or watch PyData conferences online. And it's not just PyData. You can go to DataTalks.Club webinars, workshops, and also learn from them. Of course, these are not the only channels that you can find. YouTube is full of information that you can use for getting experience from people from the industry. This is how you stay up to date for machine learning engineering.",Machine Learning Zoomcamp,2022,
611431,"If you look at the criteria, you will see that model performance is not one of the criteria. If your model does not give you great performance, it will not affect the score that you will get at the end. This is intentional. The reason it's like that is because it's very hard to control this. The performance on the dataset of a model is very dataset-specific. For some datasets, this is how it is. In some cases, for example, for click prediction – if you want to predict if somebody will click on an ad or not, the performance of the model is usually bad. You cannot predict with very high probability if a person will click on an ad or not. But it doesn't mean that these models aren't useful. But the performance that you will observe on the validation dataset will be bad. And that's okay. 
This is just a characteristic (a feature) of this particular dataset – of this particular problem. That's why it's not a criterion, but you do need to try to train multiple models, as you see here (in the criteria for model training), and then try to tune their parameters. In the end, maybe you will not have a great model – you will not have great performance for this model – but at least you tried and this is what we evaluate here. We don't evaluate the performance, but whether you tune the parameters.",Machine Learning Zoomcamp,2022,
538275,"I don't particularly enjoy moderating Slack, to be honest. [chuckles] So if you want to share the load and help me, please reach out. I will definitely share the joy of moderating Slack with you. That's a good suggestion. Thank you.",Machine Learning Zoomcamp,2022,
135988,"Yes, it is definitely possible. By all means, please use it. You will have to add extra stuff, of course, like you will need to cover the deployment part. But it's completely up to you where you get data from. If it's a Kaggle competition, it's a Kaggle competition. But since this is your code, you’re not stealing it from anywhere, or rather “copying” not stealing. If you don't, I suppose, use this code for any other course, then it's totally fine.",Machine Learning Zoomcamp,2022,
756671,"I don't know – Docker can be quite hungry for storage. 50-100 GB should be enough, I guess.",Data Engineering Zoomcamp,2023,
47681,"I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.",Data Engineering Zoomcamp,2023,
882964,"Yeah, I can. First of all, you’ll have to do projects here for this course too. You can treat them as real projects – something that you will show to potential hiring managers. When you do this, keep this in mind – think of it as though you're not creating this project for this course. You're not creating this project for your peers to evaluate. Instead, think of this as if you're creating this for a hiring manager – one who will be looking at your project and trying to evaluate it. Keep this in mind when you work on this. Add as much documentation as possible, as much context as possible. 
The hiring manager might not be aware of the course and they might not know some things, so give as much context as they need to figure out what's going on. I don't think this is really the answer to the question you asked. I think it will definitely showcase your experience and skills if you write very good documentation for your projects. Concerning strategy – just find a dataset. Go to Kaggle, where you will see a news feed. For example, here’s a Jupiter Network Global AI challenge. You just open it up and look at what people do here. You will see some data.
[image for reference]
You can see what people work on and see what it’s about. In this example it’s “Predict hourly sales for cash registers across a retail chain.” This looks like a cool project. So how about you get this dataset and work on this? If you run out of ideas, just open Kaggle – you will see what people are working on and you can work on the same thing. It’s as easy as that. You can also spend a bit more time trying to figure out what you like and then find a dataset about that. As I said last time, the best thing is trying to think what is important for you, and then getting a dataset for that – collect the dataset yourself. 
If you have problems with that, just go to Kaggle and pick whatever people are working on. Here are some other projects [scrolls through Kaggle] Hourly energy consumption – looks pretty interesting. Data science job salaries – also pretty interesting. Any of these things can be used for a project. Again, Kaggle is not the only place where you can find data. Just today, we released a dump of our Slack into a special channel – I think it's called “slack dump”. There was an announcement in the general chat. You can also check it out, maybe do some analysis of that, and see what you can find there.",Machine Learning Zoomcamp,2022,896414_qa2_01.jpg
608866,"I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.",Machine Learning Zoomcamp,2022,
565059,"Alexey
I'm not sure what this question means “how to fully utilize”. Well, come up with projects and utilize them. It's really up to you. I don't know how we can help you here. If you want to have some brainstorming sessions, just go to Slack and start a discussion there. I'm pretty sure we and the students will be very happy to take part in these discussions, contribute, and come up with some ideas. 
As for the end-to-end guided mini projects… yeah, sure. If you have some ideas, let us know. But then it also feels like another course. The projects we will have here are not super guided. They are more like, “This is the set of criteria you need to satisfy. Go and work on them.” It's kind of guided, but most of the time, you still do stuff on your own. But this is when you really learn. When you do a guided project, maybe you learn less than when you do a thing on your own. But again, in both these cases, let's start the discussion in Slack and see where it brings us because both these things are really interesting.",Data Engineering Zoomcamp,2023,
205028,"Yes, this is a really good way to build your portfolio. By the way, if you don't know about #project-of-the-week, this is the repo. Well, we only had two iterations here. But here, it's more like a study group, where we focus on building and learning things. For example, we did this one about Streamlit – there is a task for day one, there's the task for day two. You are somehow on your own, but still at the same time with others, who are also going through this. This is, let's say, more independent. At the end of this thing, we will not have 30 people all with the same project – we’ll have 30 people with different projects. It's a different thing. It makes it a bit more interesting and unusual. 
It's definitely a good way to build a portfolio. If you have some ideas on what exactly you want to learn, you can suggest these ideas and we can do this as a #project-of-the-week. So yeah, I do recommend this approach. I think that’s one of the best ways to do projects is when you try to come up with a problem that you want to solve yourself. And you will have to do this for this course too. We will give you some guidelines, these guidelines will be this evaluation matrix that you’ve maybe seen, but you will actually have to find a dataset yourself, you will have to find a problem yourself, and then you will have to implement everything we did in the course yourself. At the end you will have a model that is deployed and could be used. And I think this is really good. This is what you want to have.",Machine Learning Zoomcamp,2022,
268227,"Very, very often. This is probably the model that I use the most at my work. This is something I use pretty much for every project – for some projects. Not only is it a good first baseline, but also sometimes we just deploy this logistic regression, it works and then no further work is needed and we just leave it there. Sometimes these models are then improved with something like XGBoost or something, but logistic regression is like the workhorse of machine learning. It's used in many, many, many stations. 
Actually, in some situations, it's not really possible to use anything else except logistic regression. For example, at the previous company where I worked, which was an advertisement company, and there, it was very, very, very, very important to be able to make predictions very fast. Logistic regression is the best model for that. You cannot beat the speed of logistic regression with any other model. Maybe with a decision tree, but it will not be as good as logistic regression. Logistic regression works really well when you have a lot of features and this was the case in the company where I worked. It's really an important and useful model.",Machine Learning Zoomcamp,2022,
48610,"I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.",Machine Learning Zoomcamp,2022,
571925,"I think some of you asked that question in Slack already. Let me repeat what I answered to that question.  Imagine that you have a very big dataset. For this big dataset, training a model takes a lot of time. You don't want your k to be very large. Because if you have a very large K, it means you will spend a lot, a lot, a lot of time just training this model and then computing this course, and then taking averages. And then because your dataset is big, it will probably not be very different across different folds. Usually, this is the case when your dataset is large. Therefore, in large datasets, it makes sense to use k=2 for k-fold cross validation – so you just do it twice. 
Often, even sticking to the usual train validation split, like 80/20 or 75/25 makes even more sense, because you can just do it once without training it multiple times, because A) you save time and B) it's very likely that you will not see a very large difference across different folds when your dataset is large. You can experiment with this and see that it's actually true and that this course that you get on different folds are quite different. 
Then you can just do the usual train, validation set and use that because it will speed up your experiments. For medium-sized datasets, having k=three is fine. Here, you should think in terms of how much time you are willing to spend on doing your cross validation. You don't want to spend a lot of time here. So maybe k=3 should be sufficient. As your dataset size goes down, then maybe you can increase k. For example, for smaller datasets, k=five, or even k=10 in some cases, makes a lot of sense. That's how I typically choose. The smaller the dataset, the larger the k. And for big datasets, I usually don't use k-fold cross validation at all.",Machine Learning Zoomcamp,2022,
423252,"Michael  
I think that would heavily depend on how much experience you have with Python, SQL, GCP. If I remember correctly, I think I probably did about six hours a week. Also, if you want to have the material really sink in, you'll go through it two or three times. So it really depends on the student and the week. Some will require more time. But I think if you manage your time well, the time commitment can be pretty small. 
Luis
Yeah. Like Michael said, I think it also depends a little bit on the issue of the team that you're talking with. For example, on SQL hours, I am already completely advanced, so it didn't take much time with that. It was really fast. But for Docker, I was completely a beginner, so it was harder. Besides that, there's the leaderboard, and if you want to be on the top of the leaderboard because it matters for you, then it will probably take more than that six hours a week. But I think five to six hours per week is expected.
Alexey  
I saw a similar question asked on Reddit and many people said that they’re putting in up to 10 hours, especially in the first week. The Docker week was the most challenging. Then the Airflow week, again, the problem there was mostly setting up. This is where people spent a lot of time. For example, BigQuery week was relatively easy because it's a managed service – you just go to Google Cloud Platform, and you don't need to set up anything locally. It's easier.
Do you guys remember what the most difficult weeks for you were? Was it the Docker and Airflow weeks or some others? 
Luis
Oh, definitely. The first week was the most difficult. Definitely. Actually, I must admit that I didn't do your week of Kafka. Sorry. I was completely busy with a lot of work and skipped the streaming week. [chuckles] Yeah, it was too hard for me.
Alexey
But it's actually a good point. Many people were asking me things like, “Hey, I don't want to study DBT. I'm interested in Spark.” Yeah, just go ahead and skip it in. We're not forcing you to study DBT. If all you're interested in is streaming, go ahead and watch the streaming lectures. They're already there. Then at the end, you just use the tech you want in your project and you don’t use the tech you don't want. You have this freedom. You have this flexibility. 
Luis
I don't know if someone asked already, but one thing that is important is that the projects you don't need are all the things of all the weeks. Don't think “Oh, I have to have streaming. I have to have Prefect. I have to have…” No. You don't have to do everything. Just some topics.
Ankush
Just remember, if you want to get on the leaderboard, you might need to do every week.",Data Engineering Zoomcamp,2023,
301984,"MLOps is a set of processes and tools for productionizing machine learning, while CRISP-DM is a process. CRISP-DM was invented something like 12 years ago, when MLOps didn't really exist. But surprisingly, it covers it pretty well. 
https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png
It also depends on how exactly you interpret each of the steps here. Here is the evaluation and deployment phase, which are pretty related to MLOps. Evaluation is how exactly you evaluate your model – how you're doing an A/B test or something like this. And deployment is how you productionize your model. Many of these things are quite related to MLOps.",Machine Learning Zoomcamp,2022,633365_qa1_2.jpg
411461,"Again, by “coursES” I’m not sure what you mean – probably Zoomcamps? If we are talking about the ML Zoomcamp – here, in addition to Flask, Docker, and or everything we've covered in week 5, we will also learn about BentoML in week seven. BentoML can actually help with making your web application faster. We will also talk about serverless – I think week 8 or 9 (I don't remember). Serverless can also scale up easily, so the more traffic you have, Amazon AWS will automatically scale it up and make sure that your lambda function can handle all the traffic. 
Other cloud providers have similar things to lambdas. In Google, I think it's Cloud Functions in Azure, it’s Cloud functions – I don't remember the exact names. But I think most of the big cloud providers have some sort of serverless option, so you can use that. Then there’s another option, which we will also cover, which is Kubernetes. In Kubernetes, there is an option to do autoscaling. That can work too. 
In this course, we do not cover autoscaling in the video materials – maybe we will have this as a homework assignment. We'll see. But the materials we will have in this course will give you enough foundation to actually go and read more about this topic and learn how autoscaling in Kubernetes works, and then implement this. But I think the simplest approach would be to first check out BentoML and then serverless could be also a good option.",Machine Learning Zoomcamp,2022,
552219,"The homework is not independent from the lessons. In the lessons, we show you how to do stuff and then the homework is to take the materials from the lessons and apply them independently. So you don't have to follow along with the lessons, but you can. It's up to you. If you feel that you already know this topic, just open the homework and try to do it. 
If you don't know how to solve it, go check the lessons. I'm also sure that if you already know, let's say, something about deep learning, then you can just find a solution online. Not the solution to the homework, of course, but the solution to the problem you have. Don't try to look for the solution to the homework problem online because that's cheating.",Machine Learning Zoomcamp,2022,
69088,"Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.",Data Engineering Zoomcamp,2023,
816962,"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course.",Data Engineering Zoomcamp,2023,
857536,"Image data, text data, time series (to some extent) although it is tabular. So images and text are mostly the ones that come to mind when I talk about non-tabular data. In module eight, we will see what to do with images and with NLP, you can check out a lot of resources on the internet on how to use neural networks, or how to use other things. Because if we use traditional methods, like the count vectorizer that I showed you in the Office Hours, it will still take non-tabular data and turn it into tabular data. But with neural networks, it does a bit more than just that. It's more complicated – more advanced. You can find a lot of examples on the internet.",Machine Learning Zoomcamp,2022,
434436,"For historical reasons. They have been around in this area (in this domain) longer than Python. Python has only relatively recently broken into the data world. Before, it was not used for data. I think it became popular because of IPython and Jupyter Notebooks. You could very quickly experiment with things. For regular software engineers, Python is closer than R and that's why, I guess, they started using Python more and more often. Then good libraries appeared in Python, and slowly Python overtook R. But because it was probably more for engineers, and engineers are not very good at statistics, typically, that's why I guess all the stat packages are still in R. Of course, there is “stats methods”. I don’t remember the name of the package, but I remember that the abbreviation is SM. I think you will be able to find it on Google. Maybe somebody in the chat will write the name of this package.",Machine Learning Zoomcamp,2022,
198863,"Well, from my point of view (I might be mistaken because I'm not a data analyst) data analysts usually do not work with machine learning. They sometimes do, but it's not their main focus. Some analysts might use machine learning as an extra tool for doing what they need to do, like forecast sales, for example. But that's not their main domain expertise, let's say – not their main area. Machine learning is more a data scientist kind of thing. That being said, to answer your question, “What types of projects are expected for data analyst jobs?” They are usually about analytics. You get a dataset and then you need to, let's say, build a report from this dataset to get some business insights. Again, I don't work as a data analyst. That's pretty much using SQL and some data visualization tools to turn a dataset you have into something understandable and digestible. If you build a project like that – you can take any of the datasets we use and if you build some cool visualization for that –that's a good kind of project for a data analyst role.",Machine Learning Zoomcamp,2022,
837654,"Well, that's why we have module one. There, the very first unit (the very first video) the example with cars – just use this example. You can use that to explain what machine learning is. When it comes to neural networks, we have module eight, which is about neural networks. You can use that to explain what a neural network is. But if you want a one sentence explanation – it's a bunch of algorithms that extract patterns from data. I don't know – will it make sense to your stakeholders? If it will, good. If it will not, then you have these other resources that I mentioned.",Machine Learning Zoomcamp,2022,
742225,"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ.",Data Engineering Zoomcamp,2023,
60278,"So for the first part “What should be my approach if I'm juggling between both?” Well, I don't know how much time you have. If you don't have a lot of time, I would suggest focusing on one and then come back to the other one. Actually, I need to make an announcement. As the course team from the Data Engineering Zoomcamp, we actually discussed it, and I'm happy to announce that there will be another iteration of Data Engineering Zoomcamp. Maybe what you can do now is just focus on ML Zoomcamp, and then come back to Data Engineering Zoomcamp when we launch it in January. There will be another announcement, but now you can know it. So, yes, we'll have another iteration in January.",Machine Learning Zoomcamp,2022,
280618,"As of today, January 26th, 2023 – it's likely that we will extend it until the end of the week. But this doesn't mean you should put this off until the last day. If you do this, it's likely you will be late and we're not going to extend it a second time. Try to make it happen earlier. If you can't, we’ll see.",Data Engineering Zoomcamp,2023,
467060,"Yes, there is such a thing. If you go here and scroll down, you will see the Tools section. If you go to week 1, there is a video, Setting up the environment on cloud VM, which explains everything you need to do. Then you can also check out the notes from the students. For example, notes from Alvaro are quite comprehensive. I remember that some students also took a lot of notes about the environment preparation setup, so you can take a look at the notes too.",Data Engineering Zoomcamp,2023,
894995,"Yeah, I don't know. LinkedIn? There is also Angel.co, which I think is a good one too. Check that out. Maybe Glassdoor? The usual places where you look for a job.",Machine Learning Zoomcamp,2022,
814636,"Kalise
Our duck is named Marvin. Jeff, do you want to say why he's blue or why we like ducks? [chuckles]
Jeff
Yeah. “Rubber ducking” is a super common thing when you get stuck. You should do this before you put the question on Slack – you should talk to the rubber duck. Oh! Michael’s got his duck. I got mine. I got Marvin here in the background, let’s bring Marvin over. So just instead of talking to anyone, you can just talk to Marvin – talk to your duck, explain your problem, and as you're explaining it, oftentimes you realize like, “Oh! It doesn't work, because I didn't do that thing that's really obvious!” or whatever. So rubber ducking is a common troubleshooting step for people as they are programming. That's where we got Marvin, our blue duck.
Alexey
And the reason why it's blue? It's just a nice color?
Jeff
It is a nice color. I feel like I should know the answer to that, but I don't know if there is a reason. [chuckles]
Alexey
Because why not blue, right?
Jeff
Why not blue? Yeah. [chuckles] Because then if you use DALL-E or some kind of AI generation to get blue docs, you can get all kinds of weird images.
Alexey
Yeah, I think that's what you do for your preview images, right? For your blog. It's all Stable Diffusion or DALL-E or something. 
Jeff
Yeah, mostly Stable Diffusion.",Data Engineering Zoomcamp,2023,
127128,"Right now, I'm reaching the limit of connections. I used to accept everyone who sends me a request, but now I don't. Maybe add a note of why you want to connect with me. But you can just follow me, so you don't really need to connect with me on LinkedIn. If you want to ask me a question, the best way of doing that would be to go to Slack and ask a question on the public channel. When I see it, I will try to answer it.",Data Engineering Zoomcamp,2023,
514625,"Jeff
The answer is yes. It can be nice to define those blocks in code in Python. You can just run the script and boom, now they show up on your server – your new server, new workspace. We're also working on a way to make it a little easier to copy over all those blocks at once. That's something that, because of this course, I raised with some people and they are working on some things there. We're gonna try to make that as easy as possible. If you're using different workspaces or different servers, those blocks live on the server and you have to let them know about it.",Data Engineering Zoomcamp,2023,
633535,"Alexey
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right.",Data Engineering Zoomcamp,2023,
718531,"A good validation framework will tell you…. that's a tricky one. You just need to try to make sure that it's representative of how the performance of your model will look in real life. So you just need to make it as real-life as possible. I'm afraid the answer here is that it's case-dependent. At the end, what is important is the number you get – the performance evaluation metric you get – from your validation framework. It should reflect how well your model will do on unseen data. Sometimes, just shuffling the data and taking a part of this is enough. 
Most of the time, you will also need to split your data by time. In most cases, you have a timestamp in your dataset, and in this case, it's better to use the timestamp  for splitting the data. Apart from that, maybe just use common sense. Try to develop a bit of domain knowledge about this dataset and ask yourself, “Okay, is this validation dataset representative enough of the problem we're trying to solve?” I don't actually know if there is a better answer to that. Maybe I'll think about this and tell you.",Machine Learning Zoomcamp,2022,
68170,"Alexey
From what I see (And Ankush, maybe you can talk more about this) this is a perfect profile for switching to data engineering. Am I right?
Ankush
Yeah, I think this is a good time to be an engineer. And if you have Python knowledge and some database knowledge, you are already ahead of the curve. It won't be too difficult. It will be challenging, of course. But I hope this course helps you out. And I hope that overall, it would be really worth the effort. Because I think data engineering is a rewarding career for that.",Data Engineering Zoomcamp,2023,
838840,"Yes. And the Office Hours step is probably more for the non-technical questions. If you have a large code snippet, you cannot really ask this during Office Hours.",Data Engineering Zoomcamp,2023,
275766,"You are in the right place. That's why we have the course. Just follow week 1 to week 7 or to week 6, do the project, and you'll be fine.",Data Engineering Zoomcamp,2023,
138373,"An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.",Machine Learning Zoomcamp,2022,
182930,"What you can do in AWS is – there is part with billing. You go to billing and you can set budget alerts. Then you can say, “Okay, my budget is $20 per month and if I reach 75% of that, then send me an alert.” You will get an alert and you will know “Okay, I actually spent too much money on this. Now I'm going to go to the AWS console and figure out where I actually spend my money.” And then you can turn off some services you don't want to use, for example Kinesis. We don't use Kinesis in this course, but we use it in the MLOps Zoomcamp course. There, we use Kinesis and it is relatively expensive. 
Another thing you can do, for example, we cover Kubernetes in this course, and you might accidentally forget to turn it off. Kubernetes is not covered by the free tier. If you want to run Kubernetes, you will have to pay. One thing that happened to me recently is that I killed the Kubernetes cluster, but somehow the notes from this cluster were still running. So they weren't killed – they weren't removed or terminated. Because I had this alert, I got an email saying, “Hey, it seems you spent $80, which is a lot higher than usual.” Then I went to the cost explorer and I saw, “Okay, I am spending money on this issue.” I went there, I noticed the instances that were running them and I killed them. Then I would propose to have multiple alerts like that. 
Maybe have one for $20, another for $50, another for $70, another for $100. When the first alert goes off, you go there and fix it – you want to make sure you stop spending the money. The second alert (the second threshold) will help you to make sure that you don’t keep spending the money. Also, I actually have some credits from AWS and I wanted to give them away for the participation in these projects. We wanted to do it for this one for recommenders. I didn't do this. 
For the next one – the next project of the week will be about Fast API and we should totally give these AWS credits to participants. So maybe keep an eye on that and remind me that I promised you that, in case I forget. Then you will get some credits. Not everyone – some of you will get them. If you just commit some project that is not really useful, you will not get credits. But we will probably decide which projects will get credits. I can give you like $50. Of course, it's not a lot, but you can experiment with different AWS stuff.",Machine Learning Zoomcamp,2022,
757200,"This is very problem-specific. You cannot. If you're evaluating a project and you see that they have this RMSE, there's nothing you can do with this information. There is no criteria to even evaluate this. If you notice, there is no criterion that says, “Okay, the model quality is good or not.” So we don't evaluate this, because all of this is problem-specific. If this is for your project, what you can do is train a baseline. The baseline could be just predicting the average value. 
Let's say you have all the cars – you take the average price for these cars and then you compute the root mean squared error of this prediction. You just simply predict the average for all the cars. You will get some root mean squared error and then you train a simple model. Then this simple model, hopefully, should be an improvement over the baseline and then you can see what the improvement is. It might be, I don't know – 200, 300%. Then, because you already know the baseline value, you can compare the improvement over the baseline. This is how you can understand if it's low or high. If it's worse than the baseline, it's low, of course. But if it's better than the baseline, then it's better. You can see that, for example, XGBoost gives you even better performance than linear regression, and you can have a relative ranking of the models.",Machine Learning Zoomcamp,2022,
975414,"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]",Data Engineering Zoomcamp,2023,
748216,"I would prefer that you don't do this and that you take another dataset instead. If you take the same EDA (exploratory data analysis) and you repeat it in a new project, it's kind of plagiarism. If you don't do this – if you do everything completely from scratch – it's kind of okay. But maybe you want to have a variety of things for your portfolio? So maybe try to get a different dataset, just to get exposure to different problems. It's not a super strict requirement – as long as you're not plagiarizing or self-plagiarizing, you're good.",Machine Learning Zoomcamp,2022,
108694,Just one. It's either the first attempt or the second. You should not try both.,Data Engineering Zoomcamp,2023,
554930,"[chuckles] No, I don't. Just go to Google and you'll find a lot of them. I don't suggest spending time on that. If you want – not because I asked you to do it – because it's likely that you will spend a lot of time figuring out how it works and why the stuff in this article doesn't work on your machine. I've been there. I don't recommend it. 
I would suggest you try to go with Saturn cloud or something similar and avoid spending a week on figuring out how to set up cuDNN and all these things on your machine, especially if it's a Windows machine. It’s terrible. I did set up something like this on Windows as well and it wasn't easy. I don't recommend doing this. It’s better to spend this time on something else. [chuckles] But if you want – if you insist on doing this – the internet is your friend. 
Just go to Google and you will find a lot of resources. The catch there is that not all of them actually work and you'll have to figure out which tutorial works and which doesn't. I think on Ubuntu it’s actually simpler, at least according to some of the students who configured this. Maybe just go to Slack and ask, “Hey, I need help with configuring my local GPU.” Because I know at least two students did this. So if you're brave – do this.",Machine Learning Zoomcamp,2022,
129555,I'm glad you asked. MLZoomcamp.com. All the materials are here and you will find them there.,Machine Learning Zoomcamp,2022,
990453,"Tim
I actually don't. [chuckles] I think there’s some really obscure version of Ubuntu, actually, that is not passing. We tried to regress it on multiple OSs and I think there's an obscure version of Ubuntu in there.
Alexey
[chuckles] I will not go there. You mentioned that the best way to contribute is to just run it and see if it works. Knowing how many different environments students of this course have is already a good contribution because all of us have different versions of Windows, different versions of shells in this OS. With Windows life is always dangerous and full of surprises. [laughs]
Tim
Well, Mac M1 is kind of similar. But yes, Windows can show problems sometimes. I think the best way that I've found to have a consistent environment is just to create a Ubuntu instance in AWS and then just go from there. And then if my environment completely blows up, I can just stop the machine and then start a new one. [chuckles]",Machine Learning Zoomcamp,2022,
349635,"Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.",Data Engineering Zoomcamp,2023,
709491,"To me, these tools focus on completely different areas. You can use all of them, actually. MLflow is about experiment tracking. Kubeflow is about creating model pipelines (if you’re talking about Kubeflow, because there is also a thing called KServe, which used to be a part of Kubeflow but now this is a framework for serving machine learning models. It's very similar to some SeldonCore). That you used for serving models. All three of these tools can actually play well together. What I would suggest you take, if you want to learn more about this and if you're interested in this – there’s another course that we have that you can take after this one, which is the MLOps Zoomcamp, where we talk about MLflow (in module two). Then we talk about allocation and machine learning pipelines. We don't use Kubeflow pipelines here, we use Prefect, but the concepts are pretty similar. We also talk about model deployment, which is kind of similar to SeldonCore and KServe. Also remember that we have a module about KServe in this course. It’s actually an optional module. You don't have to take it and it's not graded if you take it, because it's pretty advanced. I think it's slightly outdated, too. But recently, I was going through this module and all the things that didn't work – I fixed. So it's actually up-to-date, but it might be a bit outdated. You can check this one out too. SeldonCore is pretty similar. The main difference between KServe and BentoML is that for KServe and for SeldonCore, you have to have Kubernetes. Bento works without Kubernetes, but for this, you have to have Kubernetes. So you need to know Kubernetes at least a little bit. That's why we cover Kubernetes and then we cover KServe in addition to that. You can also deploy models with MLflow, but it's not the main focus. I wouldn't use MLflow for model deployment. To me, it's more of an experiment tracking and model registry tool. If you want to learn more about that, please check our MLOps course.",Machine Learning Zoomcamp,2022,
455919,"Alexey
To me, it looks fine. I mean, it really depends on your use case, but for me, this is perfectly fine. There could be reasons for skipping data in GCS and putting it directly to BigQuery. I mean first putting it into BigQuery and then transforming it there. So I think this is how you typically use DBT. I don't think there is any right or wrong way.",Data Engineering Zoomcamp,2023,
514845,"Yes. As I’ve already said during the launch stream, you can start applying for jobs right now. You don't actually have to complete the course to find a job. Maybe you already know enough for you to convince an employer to hire you. By that, I mean you already know how to program, you know the command line, and things like that. It will be enough – it will be possible – and many people from the previous iteration of the course did that. They found a job. Some of them, not even as juniors.",Data Engineering Zoomcamp,2023,
737015,"There is a good book about that called OpenIntro Statistics. You can check that one out. There is also a book called Think Stats. That's also a good one. Check these two books out, especially Think Stats. I think it's free. Both of these books are actually free. As you can see, it's available as a PDF. Just go through this book. 
Think Stats is good because it shows code – there is a lot of code. It just shows you all these things like hypothesis testing, p-value, and everything. To some extent, it's kind of similar to this Zoomcamp. They focus first on a project and then they implement some of the things. Check that one out.",Machine Learning Zoomcamp,2022,
227401,I wouldn't. You can just stop the instance without terminating it and then you can resume it. It's effectively doing the same thing as powering off your machine and then turning it on when you're back and want to work.,Data Engineering Zoomcamp,2023,
878595,"Alexey
I would actually suggest neither. I would actually suggest something like a data lake first. And then from this data lake, you can parse it – you can transform it and you can save it in BigQuery, Postgres, whatever data storage you want.",Data Engineering Zoomcamp,2023,
452911,"Yes, we will share some ideas with you. In this project folder, we have datasets. You can just go through these datasets and find something is interesting – you can just take it and do it. If you really struggle with ideas, of course, ask in Slack and we will be happy to help you.",Data Engineering Zoomcamp,2023,
849555,"I don’t know… throw a coin and the coin will tell you what to do if you don't know. [chuckles] And then if the coin tells you that you should be a data engineer and you feel like “Umm… maybe not.” Then go with the other option. I don't have a better suggestion here. I can tell you to follow your passion and follow your heart… But yeah. [chuckles] I don't know. You’ll eventually need to make these decisions yourself. Just do a project in data engineering, do a project in data science, and then ask yourself what you enjoyed doing more. 
Also talk to people – talk to a few data engineers, people who already work as data engineers, people who already work as data scientists, and ask them what they don't like about their job. And then ask yourself, “Would you enjoy doing things that they dislike? Or would you be okay doing these things as well?” For example, what I don't like in the work of a data scientist is all this parameter tuning and parameter optimization, trying different features – all this stuff I find boring, but some people really love it. If some people tell you, “Eh, I don't like this part,” and you think, “Okay, it's actually not bad for me,” then maybe this is the right path for you. 
In the end, if you join as a data scientist, it doesn't mean that the door for a data engineer is closed for you. As a data scientist, you will get to do a lot of data engineering as well. I, as a data scientist, also do data engineering. I also do machine learning engineering. The same is not always true for data engineers. If you get hired as a data engineer, maybe you will get to do data science, maybe not. It depends on the team. So then, what you can explore is the so-called full-stack data scientist. But again, titles sometimes don't mean much. Don't try to force yourself into one of the predefined boxes. Sometimes you can work ‘officially’ as a software engineer and do both data science and data engineering and enjoy this work.",Machine Learning Zoomcamp,2022,
554774,"Yeah. For example, do you use Flask? In Flask, you can see what kind of data... But wait, why don't you just put the data – the text – in your JSON? Your JSON file will look like this curly bracket and text is a parameter and then the text that you have, you just put it as a value and you send that. That will make your life simpler, I think, because you might also want to add some extra data (some metadata) about the text. So I would actually suggest you stick to JSON and put your text and JSON. But Google is your friend. You can just check for the framework you use – if it's Flask, then check for “Flask text input content type” – something like that. Or the same with Bento. With Bento, it's actually more explicit. If you remember, we defined the type – it could be JSON, it could be a numpy array. They probably also support text, but you will need to check the docs.",Machine Learning Zoomcamp,2022,
335665,"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools.",Machine Learning Zoomcamp,2022,
431252,"I think so? I need a bit more context to answer this one. But yeah, you can mix transformations in any way you want.",Machine Learning Zoomcamp,2022,
473387,"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there.",Machine Learning Zoomcamp,2022,
282490,I do not know. I have not used services from Azure to say that. I don't think they are better or worse. They're probably similar in terms of what they can do. They're just different in terms of how you use them and the interface. Just pick whatever you like more. I don't think there is any significant difference.,Machine Learning Zoomcamp,2022,
187562,"Alexey
I don't know. Maybe I'll put this question to Ankush as well. But check the website. I think this is one of the good examples when you can just use Google to find the answer.",Data Engineering Zoomcamp,2023,
676687,"No, ETA is not called epochs. You will see epochs in the deep learning module. An epoch is typically one iteration over your entire dataset. I will not spend time now explaining what an epoch is. Maybe when it's time, you can check the deep learning video and if still not clear what an epoch is, then we can discuss it. Okay, I hope this explanation makes sense. If it doesn't – again, in the deep learning module, we will talk about a similar concept called “learning rate”. It's actually not similar. It's the same concept. There is actually an entire unit about this called learning rate. You can check that out. Maybe just check this unit separately from the rest and maybe it will help you understand ETA a little bit more.",Machine Learning Zoomcamp,2022,
851438,"Pretty good. Now there is a bit of recession – maybe not a bit, but the job market is also declining a little bit. So maybe there are not as many jobs as a year ago, but it's still possible to find a job. There are still job postings. At OLX (where I work) we’re hiring. Maybe we actually hire less in Germany than before, but still – companies are hiring.",Machine Learning Zoomcamp,2022,
252609,"Yes, of course, by all means – take a break. This course is not meant to squeeze all the life juice out of you. You can take the course at your own pace, too – remember that. Please take things easy and don't worry about missing homework. The world will not stop spinning if you miss a homework assignment. Please be mindful of your own capacity and don't overwork yourself. Also, I should mention again, if you are a little bit behind and you cannot make it to the midterm project, it's not a big deal. It's fine. 
You can catch up later. December and January, (December mostly) will be pretty slow. So you can catch up with everything – all the content – by the end of December and then do a project in parallel to that. Then you will have all of January for another capstone project. Remember, you only need two projects. If you do these two projects as Capstone 1 and Capstone 2, then you will be fine.",Machine Learning Zoomcamp,2022,
709740,"Yeah, that's an interesting question. This is something we do not cover in detail in this course. Again, maybe (if you want) you can refer to this Andrew Ng course for more theory. Here we don't cover a lot of theory – we focus more on practice. But from the theoretical point of view, linear regression is trying to optimize the so-called “loss function” and it's doing it by using some mathematics. A normal equation that we saw is one of the ways of finding the minimum for this loss function. Basically, the weights we get at the end are the best in terms of minimizing this loss function. The weights that we get, they make sure that this loss function is minimal. I don't know if it makes sense to you or not, but there is some mathematical foundation behind this. There is an explanation of why it happens this way. 
If you want to find out more about this, maybe you can check about a thing called “gradient descent,” which explains the loss function – we move down by tweaking our weights in such a way that, when we arrive at the end, the weights we have in our model, they are the best ones with respect to this particular loss function. I think we slightly cover loss functions minimization – I think maybe in neural networks. I don't remember. Here, we focus more on practical aspects. You kind of put some trust into the idea that the model gives you a good enough estimate, and then use it from a practical point of view. But if you want to learn more theory, then I think there is no better course than this one – Machine Learning by Andrew Ng. From my point of view, that was one of the best courses I took when it comes to theoretical machine learning.",Machine Learning Zoomcamp,2022,
733094,"Not as a part of this course. We will not cover NLP here. Maybe if there is a lot of interest in NLP, yes. I am actually thinking about what kind of course I should make, so if you have any suggestions, please reach out. I want to start thinking about the next course, so maybe by the end of this year we'll have some sort of outline. If there is a lot of interest in NLP, it could be NLP. But I don't know NLP that well myself. I know the basics, so how deep do we want to go there? So please tell me what you're interested in regarding NLP, and then we can see if this will happen at some point.",Machine Learning Zoomcamp,2022,
514296,"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. 
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. 
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average.",Machine Learning Zoomcamp,2022,"316882_qa4_02.jpg,316882_qa4_02.png"
542726,"Yeah, you can do this. The sky's the limit. Again, use a validation dataset to find out and then remember about the trade-offs. The trade-offs are speed, I guess, and the complexity of your model. You need to take that into account and then use the validation framework that you set up to make the best decision. So use data to make the decisions.",Machine Learning Zoomcamp,2022,
408065,"Okay, what I will do is I will copy this question and paste it into Google and see what it answers. I don't know, is this good enough? Or do you want to know more about that? I usually go with ENTRYPOINT. In lambda, we use CMD because ENTRYPOINT is already specified in the base Docker image. Then using CMD, you can kind of overwrite, but only partly. CMD is like a part of ENTRYPOINT. As far as I remember, that's the main difference between them. Please look it up. I don't remember too much about it.",Machine Learning Zoomcamp,2022,
567755,"Any project will work as long as it's not Titanic, or Iris or MNIST – or similar popular datasets. It would be best if you collect the data for this project yourself. That's a really, really, really big plus. Most people just find a CSV file on Kaggle and there is nothing wrong with this. Most candidates that have portfolios find a CSV on Kaggle and then they create a Jupyter notebook where they train a logistic regression model and they call it a day. But if you really want to set yourself apart from these people – again, there is nothing wrong with this, that's a good project already – but if you want to go the extra mile, you can, first of all, deploy your model. Then, show how you deployed this – and this is what we will cover in this course. In this course, we will train a model and then deploy it as well. 
If you want to go one more extra mile, then consider collecting the dataset yourself. You can just scrape data from somewhere, or you can use crowdsourcing platforms or you can just take pictures yourself. For example, for the book Machine Learning Bookcamp, there’s a chapter about deep learning and image classification. For this chapter I actually collected the data myself – I curated the data myself, but there are pictures that I took myself as well. I actually put clothes on the floor, and then took my phone and took pictures of the clothes. Then I also asked other people to do the same. This is how we collected the dataset for the book. Now we also use this for the course. 
If you do something like this, it will be insanely amazing. Very few people actually do this – very few people collect the dataset themselves for the portfolio projects. Then, you don’t only do your project end-to-end, because you start with collecting the data, and then you finish with deploying it, but it also shows that you care about this thing. Usually, you will only do this for things that you find interesting or important or something that’s close to you. These types of projects are especially interesting. I know some people who were looking for a flat in Berlin – in Berlin, it's very difficult to rent a flat – so they wrote a scraper that scrapes Berlin flats and then helps to find a flat. I think they had a model for predicting the price and they see how different the price is from prediction and use that to somehow find the flat. 
Here, you have a reason – a goal, a problem you want to solve – and then you collect the data yourself, you build the model, and then you deploy it. And you actually use it to solve your problem. This is amazing – this is as close to a business perspective as possible.",Machine Learning Zoomcamp,2022,
548628,"Yes, I did create a form recently. I should have created it earlier. You will submit your Learning in Public links here. But then you also will submit the name of the leaderboard and the score for Learning in Public. This is how you do this. This is how we will know, otherwise, it's tricky.",Machine Learning Zoomcamp,2022,
227942,"I use Pipenv inside Conda. If I do $ which pipenv – it will be Pipenv inside Anaconda. [Terminal image as reference] There is nothing wrong with this. I use Anaconda as a general purpose Python distribution with the packages I need and then I install everything I need personally on top of that, like XGBoost, Pipenv, TensorFlow – everything that Anaconda does not have by default. What is good about Anaconda is that it's separate from the system Python I have. 
There is a system Python on this computer, even though it's Windows – it's not really a system Python. But on Linux, there would be the system Python, which you don't want to touch. That's why I like Anaconda, because you put it inside your home directory and then in this home directory, you can do whatever you want. And if something happens, you can just remove your Anaconda and then your system Python will stay untouched. That's the good thing about Anaconda.",Machine Learning Zoomcamp,2022,559148_qa4_02.jpg
623668,"From 5 to 20, depending on your experience. If you're already an experienced developer, software engineer, you know Python, you know cloud, it could be less than five. If you don't know any of these things and you never used a command line, then it's 20 hours. If some of these things apply to you, some don't, then probably around 10 hours.",Data Engineering Zoomcamp,2023,
659124,"Well, I'm not really a professional in this area. What typically happens for example, in the Google Cloud Platform, everyone keeps the file in a home directory. This is not version controlled. You basically don't commit this file. This is one approach. I think this is a pretty common one. I mostly use AWS. In AWS, there are ways to authenticate, which does not require putting any credentials in your Git. Once you authenticate, you can just apply Terraform stuff. That's how it's usually done. I think it's not the best answer, honestly. Again, I'm not a professional in this. If any one of you knows what the best approach is, maybe write it in Slack and we can possibly create a video about that. Maybe you can write a blog post, if you want, about that. But yeah, I don't know.",Data Engineering Zoomcamp,2023,
855090,"They may ask for a skill, but it doesn't mean that they really have to have it. You don't have to match the job description 100%. Companies usually realize that, let's say, if you know Prefect but don't know Airflow – if you know GCP but don't know AWS – it's still fine. These skills are transferable. Usually, employers will consider you anyway if you have experience with related technologies.",Data Engineering Zoomcamp,2023,
964300,"No. But if some of you find anything interesting, please share the links in Slack",Data Engineering Zoomcamp,2023,
305156,Yes,Data Engineering Zoomcamp,2023,
528427,"The homework is not meant to give you end-to-end project experience – it's meant to make sure you understand, or reiterate, all the topics we cover in the videos and in the course materials. We try to focus on the most important parts there and also maybe push you a little bit out of what we covered. So you need to do a bit of exploration yourself. Maybe there haven’t been as of yet, but there will be homework assignments where you will actually need to move a little bit outside of what was presented in the course, and then do a bit of Googling yourself. 
Homework assignments are not “real projects” in the sense that you probably shouldn't add this to your portfolio and call it “Hey, I predicted a car price dataset. I used this dataset and predicted a car price. Here's my portfolio project.” I wouldn't do this. But for the projects we will have as part of this course – yes, you can definitely do that and they will be more complete and end-to-end, definitely. Especially compared to what we do in the homework.",Machine Learning Zoomcamp,2022,
380338,"Alexey
It's more or less complete. There is one thing we wanted to update, which are the TerraForm videos. Sejal, unfortunately, got sick. In Europe, there was a big outbreak of the flu. I was also one of them. Sejal caught it a bit later than others. So she's unfortunately now sick. She's recovering. We'll have some videos from her there. But fundamentally, not much will change. There are videos from her from last year, but in those videos, the code is already written – not everyone liked this approach, so we wanted to re-do this. And you will see these videos. Right now we have the old material, but that's the only change we still want to do. Depending on the problems you have right now, we will see. This is what we did last time. We saw the problems many students had and we recorded some videos to answer those problems.
Ankush
I just wanted to add that I'm doing some more videos on Kafka. The current playlist is not complete on Kafka and I will be adding at least three, four more videos on that.
Alexey
Anyone else is working on more videos right now?
Victoria
I was planning to do something like “What changed in the last year for DBT?” For example, now you can do Python there as well. But no update to the project.",Data Engineering Zoomcamp,2023,
953999,"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job.",Machine Learning Zoomcamp,2022,
631234,"You will probably not like this answer, but use validation to find out. Train two models – one model uses color, another uses black and white – you compare them and you will see which one performs better. You can also look at speed, perhaps, because this could be another important factor. I assume that the black and white one will train faster. But that's it. Just experiment and see you for your dataset, which one works better, and stick to it. In general, for any question about machine learning, Like, “Is it better to use X rather than Y?” The answer is always validation. Just try both and see whatever option performs better on the validation.",Machine Learning Zoomcamp,2022,
547470,"Jeff
I would watch the videos and do the work in the course and do the homework",Data Engineering Zoomcamp,2023,
49448,"I think you're referring to the problem we had, which is in the frequently asked questions. The problem that we had there was this one: “ValueError: training data did not have the following fields: age, amount”. I don't know how to solve this problem. I talked to Tim. Tim will talk to the BentoML team and they will figure this out. 
Maybe this is the backend – there will be an issue on GitHub. You can follow this issue and see when it will be solved. Or maybe there is an easy fix, such that you just need to specify the feature name somewhere. I don't know yet, but I asked Tim to help me find the answer. Once we have the answer, we'll share the answer with you.",Machine Learning Zoomcamp,2022,
603548,"Alexey
I think I tried to answer this question in Slack already. Let me try to recall what I said there. Spark requires an infrastructure to run it. This is more like a downside of Spark. Because with DBT, you delegate all the compute – all the transformation happens in a data warehouse. DBT just orchestrates this transformation. With Spark, it takes the data out of the data warehouse, transforms it in the cluster, and then puts it back. So it's a different thing. In the case of DBT, the data never leaves the warehouse. In the case of Spark, it does leave and it processes it. 
There are pros and cons to that. For some cases, Spark is just cheaper, especially if it's not a data warehouse, but a data lake that you’re using. With Spark it’s just a lot easier to access the data lake and do all the transformations there on the data lake, and then save the data back to the data lake. On a large scale, when you have a lot of data, it becomes cheaper. With Spark, you also kind of have more control, because it's Python or Java – whatever you use. This is code – you can test this code, you can… it's just easier to manage this code, because it's code. Meanwhile, in DBT – of course, you have tests in DBT, but they are different. They're not like unit tests. So you have more control, more flexibility – for more complex things, for more complex transformations, perhaps Spark is a better option. 
I don't remember what else I wrote there. I think we should put this in the FAQ somewhere.",Data Engineering Zoomcamp,2023,
121944,"Alexey
No. It's just in one way, you start it manually by clicking a button, and in another (in CLI or in the code)  instead of clicking a button, you're using the CLI or you do it from code. I don't think there is any difference. Or maybe you have some concrete example in mind? If so please, ask in Slack, if you want to clarify.",Data Engineering Zoomcamp,2023,
508500,"Exactly. This would be a really end-to-end project in any sense. So when you start with nothing and you end with a deployed and functioning system with monitoring, that is as end-to-end as possible. But then you can maybe try to make it a bit less end-to-end by getting some dataset that you already found or maybe collecting some dataset yourself? I don't think it will be end-to-end in this sense, but still. 
When you get some data, and then you deploy it, it's already better if you just train a model in Jupyter Notebook and call it a day. So just a Jupyter Notebook is not an end-to-end project, definitely, but going through all the steps in the CRISP DM process is definitely end-to-end.",Machine Learning Zoomcamp,2022,
108455,"Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.",Machine Learning Zoomcamp,2022,
171464,"I don't think I understand the question. What do you mean by “general points”? Do you mean “What are the similarities between them?” For NLP, you can use some of the ideas from computer vision. For example, convolutional neural networks work for NLP as well. But that's pretty much the extent of what I know about that. [chuckles] I don't know much about NLP, to be honest – only the basics, like embeddings. I don't know how to answer that question.",Machine Learning Zoomcamp,2022,
131514,"I don't think I understand the question. Is the question about what we do when we have duplicates in data and we split it and then there are duplicates? Some of the datasets in week one and then one split and then another one, or…? Oh, okay. Yeah, the way we do it is, we generate random numbers. I don't remember, to be honest, how exactly we do this. Let me quickly take a look. If we go to regression, notebook, and split – I think we shuffle data, not draw random numbers. Yeah, we take a range from zero to n exclusive. So n is not included and then we shuffle it. This way, we make sure that the same number does not end up in the different splits. I hope that answers your question.",Machine Learning Zoomcamp,2022,828631_qa3_01.jpg
844684,"There is no silver bullet. Ideally, you need to have enough context to say “This is a good solution. This is not a good solution.” There is no one-size-fits-all tool. There is an ‘okay’ stack that works for many cases. For me, I would use Scikit Learn and XGBoost, for example. Some models from Scikit Learn, like linear or logistic regression, decision tree, random forest – and then I would use gradient boosted tree from XGBoost. Some people would go with LightGBM or CatBoost. Then I would deploy them with something like Flask and Kubernetes or with Lambda. That is usually sufficient for most cases. 
If we’re talking about web surfing, then even more often, I will deploy things with AWS Batch. AWS Batch is a thing where you can just take a Docker container and deploy it – then it will run periodically. Let's say if you want to run it every day, you can do this with AWS batch. The things we cover here, I think are quite applicable to the majority of problems. So you’ll probably cover most of the problems if you follow this course. I don't dare to call it ideal, but I think it's good enough to cover most of your needs.",Machine Learning Zoomcamp,2022,
960717,"Alexey
I will show you. You go to our 2023 cohort, and you look at the deadline calendar. It will tell you during which weeks you do the project.",Data Engineering Zoomcamp,2023,
128203,"How F1 works, you hopefully learned from the homework. When to use it? I think always – when you want to use precision and recall (and you value them equally). When you want to have good precision and you also want to have good recall, then in this case, you need to use F1. In cases when you value precision more than recall, then you need to use a different approach – maybe just plotting things. There's actually a special case of F score called Fβ score (F beta score). Here, depending on beta, you give more weight to precision or recall. But usually the F1 score is good enough, because F1 gives equal weights to both.",Machine Learning Zoomcamp,2022,
517467,"It's both. It is very, very applicable in real life. Usually, most projects start with a simple baseline – without any machine learning – and then you can use the next baseline, which would be linear regression. Then your model can work using linear regression until you think “Okay, now it's time I need to improve it,” and then maybe you can use something like XGBoost or something like this. So it's very applicable and it’s applicable as a simple baseline model. 
Sometimes you just don't need a more complex model. Let's say if you have XGBoost, then it becomes more difficult to serve because it's more computationally demanding, while linear regression is just matrix multiplication. It's very simple. It's very performant. It is a good first model that you always should use and you should always start with.",Machine Learning Zoomcamp,2022,
246116,"Yeah, if it doesn't look like we're using it – we are not using it. Maybe we just want to set it aside and forget about it.",Machine Learning Zoomcamp,2022,
776813,"Alexey
You can. Why not?",Data Engineering Zoomcamp,2023,
328651,"Yep, projects are the best way. Just start working. That's already good enough. You can also think about giving talks and so on.",Data Engineering Zoomcamp,2023,
91154,"Yes, you can keep doing this. You don't really need a cloud VM. A cloud VM just makes things simpler for us in the sense that everyone has the same environment. Then, if you have some problems, we can help. But if everything works locally, just keep using your local environment.",Data Engineering Zoomcamp,2023,
738358,"Batch processing is not only used for ingesting data. That's one of the things. In general, any data transformation is done with batch processing. You can ingest data to your data warehouse, you can transform data that you have in your data warehouse, you can put then data into your data lake, you can get your data from a data lake, transform it to a data warehouse – there are thousands of different options and alternatives. For some cases Prefect is fine. It's lightweight and you can do simple stuff there. But sometimes you just need to process a lot of data and then this is where you would use PySpark. You would not try to do this in Prefect because it will put too much load on your Prefect agents. Usually, in practice, you want to keep them lightweight and delegate all the execution of work to some external computing environments such as PySpark, or Spark in general. Yeah, you can ingest data with orchestration. But when you look at the content of week 5, you'll see that it's much more than that. Prefect can be used to execute PySpark backdrops.",Data Engineering Zoomcamp,2023,
92774,"I don't have enough context to answer this question. I don't know. Ask in Slack, please.",Data Engineering Zoomcamp,2023,
915391,"Yes, by all means, you can. But please make sure you document everything in the readme file. You need to assume that the people who will review your projects do not know anything about object detection, which will probably be the case because we didn't cover it. You will need to describe all of that in the readme. Like: What is object detection? What exactly are you doing? What is the function? What is the metric you use for evaluating? How does it work? Then people will look at your project and understand what's happening there. So you need to put more effort in documentation, but this is actually a good thing because your reviewers will also learn something new. If you feel like you can do that, then by all means, do that.",Machine Learning Zoomcamp,2022,
187413,"There is no good target. It's all case dependent. In some cases, if you have 90% accuracy, this is very suspicious – you probably should be worried and check your data for issues. Sometimes, you can have even 50% accuracy – in some cases it’s good. It also depends on what the skewness of the data is, when you have a lot of examples of one class, but very few for another class – like a class imbalance. It also depends on that. As you remember, “accuracy” is actually a misleading metric. In some cases, you should look at things like precision and recall – and you shouldn't consider looking at accuracy at all.",Machine Learning Zoomcamp,2022,
92070,"Tim 
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there.
Alexey 
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance.
Tim
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento.
Alexey 
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]",Machine Learning Zoomcamp,2022,
194397,"When you create a project and then you create this role – the JSON file – in the Terraform BIOS, you can see it. And this is the JSON file with the credentials. If you go here, to the Terraform set up and take a look at this one. This is application credentials. You basically follow these instructions. This is the service account. You download the JSON file, and then this is the file.",Data Engineering Zoomcamp,2023,
731627,"Yes. The idea here is – for numerical features, you can think of a numerical feature as a score. The output of your model is a score and a feature is also a score. It's just a number between some minimum value and some maximum value. Then you can follow through the same procedure as we did for the classifier (for the output of the classifier) but for the score. In our case, the output for logistic regression is always a number between zero and one. But for the ROC core, it actually doesn't matter what the scale of your variable is. It can be something between zero and one, it can be something between -100 and 100, it can be something between -1000 and 1000, or between 0 and 100 – it does not matter. 
What you can do there is just take this feature and go through the same procedure as we did in the lectures. Try to plot it with different thresholds and then see what happens. You can also have an ROC plot for the feature, not for the output of the model, and then based on that, you can see what the area under this ROC curve is. That will give you an idea of how well this feature separates positive examples from negative examples. This is what this ROC curve actually does. That's roughly the idea behind using the ROC curve for feature importance. You kind of pretend that this is the output of your model – this is a score – and then you do everything we did in the lectures to understand how well this feature separates positive and negative examples.",Machine Learning Zoomcamp,2022,
336086,"Check out the beginning of this video, where I showed how to do this",Machine Learning Zoomcamp,2022,
392684,"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!",Data Engineering Zoomcamp,2023,
262806,"Yes. Well, dummy-encoding is… what's the difference – I don't remember. We don't include one of the columns, right? Initially, when I said that hot-code encoding is recommended instead of diamond coding, I thought you were asking if we should use one-hot encoding from SciKit Learn versus one-hot encoding from Pandas. In Pandas, we have this get_dummies method, which works like one-hot encoding, but the problem with this method is that it will create different… Let's say if you're on your training data you have this variable “make” with only 10 different values, but in validation you have 11 – you will have a different size of your matrix. You will see this covered this week, when we cover one-hot encoding. 
Actually, using one-hot encoding from SciKit Learn is recommended. The way we implement this is through dictionary vectorizer (DictVectorizer) and this is something that makes it easier to deploy models later. This week, you will see how we implemented one-hot encoding and in week five, you will see how we can use this dictionary vectorizer to turn a request that is coming to our web service, and then turn it into a matrix, and then predict whatever we're predicting.",Machine Learning Zoomcamp,2022,
68951,"You're talking about images, so the way I would do this – maybe there is a library for that. I would use a thing called glob. I would use this glob function from Python to get all the files in the directory with images. Then you have a list of files and then you can use something like train/test/split from SciKit Learn to split it into three groups. Each of these groups would be train, validation, and test sets. This is how I would do this. It's not a library, so you will still need to use like four lines of code or something like that. Use glob, and then train/test/split from SciKit Learn and then you'll be able to do this. Of course, that might not be enough. You might also need to write a bunch of extra code for putting this into directories. For example, in this clothing dataset, I already put this dataset into three folders – we have train, and then we have classes here. So you might also need to write some code for distributing all your files into different subfolders. But this is also not a lot of code, like 10 lines maybe 12 maybe 20, give or take. I don't know – but it's not too awful. Maybe some of you will figure this out – you can post it in Slack, so then others can also use it.",Machine Learning Zoomcamp,2022,
90836,"It's basically ready. You don't need to do any data cleaning if it's true or false. You just take it as is and put them inside a model. I think. Of course, it depends on things like if there is missing data. Then maybe you need to do something with this missing data. 
But from what I see here – if it's just ones and zeros – it’s good to go. You can just take this and use it as is. It's actually quite a simple situation from what it seems. Maybe I don't see some complexities there. There could be some, but on the surface it looks rather straightforward.",Machine Learning Zoomcamp,2022,
983512,"Terraform is not for creating containers. It's for creating infrastructure – different services in the cloud. One of these things could be creating a Kubernetes cluster, for example, creating… I don't remember how these services are called in Google Cloud Platform, but there are some other container orchestrators. Then you deploy your Docker containers there.",Data Engineering Zoomcamp,2023,
399514,"If you have very good accuracy on the validation set, but you have very bad accuracy on the test set, you probably overfit. Maybe you should have a model that is less powerful. Then you should also check how exactly you split the data. Is there a difference in distribution between test and validation sets? Maybe this is the even more important thing to check. Does your data look different in the validation and test sets? If your data looks different in the test set from the validation set, then maybe you need to try to design your validation framework in such a way that it's similar to your test dataset. 
You should also ask yourself, “Is my test dataset really representative of unseen data or not?” It's probably that somewhere the model overfit, it may be using some data that is not present in the test dataset or maybe the model just got lucky. So then, based on what you think the issue is, you try to fix that. Sometimes it may even mean that you do your split differently – in such a way that validation, train, and test datasets may have the same distribution (depending on your problem).",Machine Learning Zoomcamp,2022,
514012,"For everything we do in this course, it's possible to run things locally. In some cases, when you need to create a Google Cloud Storage bucket or when you need to create a BigQuery table, you cannot do this locally. But for many things, you can. You can set up your Postgres instance, and we show you how to do this. We showed this in the first week’s video. In the Prefect videos, we also show how to do things locally. Then, you will basically need to skip week 3 because you will not be able to use BigQuery. And then week 4, we show how to use DBT with Postgres. Then with Spark, we show how to run things locally. And for week 6, with Confluent, I think you can still get access to Confluent Cloud, but you can also run things locally – run Kafka locally.",Data Engineering Zoomcamp,2023,
87396,"We will not cover it in this course. There is a good book about this called Time Series Forecasting Principles and Practice. It's a bit oriented on R, but I think you can find some code for Python as well in this book. If you want to learn more about time series and forecasting, this is the book you should check out. Usually things like exponential smoothing are efficient for most cases. We also recently had a webinar about this called Building the Modern Geospatial Data Stack with Ramiro Aznar. 
We actually had multiple – we had this one, called Probabilistic Demand Forecasting at Scale, which is quite a nice talk. And this one, called Feature Engineering for Time Series Forecasting. These two webinars are quite comprehensive, so check them out. But I would start with the book that I showed you. I think in this question “fundamental” means things like exponential smoothing. I think this is quite an easy approach and it works quite well.",Machine Learning Zoomcamp,2022,
25760,"I already removed one of these videos from the playlist and I removed it from the GitHub repo. Right now, all these materials – everything you need orchestration – is in week 2, including the part when you need to do some BigQuery stuff. This was the part in week 3 that was a part about using Airflow and BigQuery. Now it’s in week 2. Everything is already there and you should not see any Airflow stuff. We might have accidentally missed some of this, so if you see some Airflow stuff somewhere, please let us know.",Data Engineering Zoomcamp,2023,
413336,"There is no penalty for joining late. You can join at any point of time. You can actually take the course at your own pace. Nobody will penalize you for that. So take your time. Again, I will refer to this list of frequently asked questions. Please check it out.",Machine Learning Zoomcamp,2022,
427195,"You can. Remember that we have three projects and you need to complete two of them to get a certificate. This means that if you miss this one – if you didn't finish this one – you have to do Capstone 1 and Capstone 2 projects. If you pass them, you will get a certificate. Also, let's say you just joined the course now – you can still catch up.
 You can watch the videos and by the time we have the Capstone 1 project, you can already start doing it. Then we will have a bit of a break (for Christmas, New Year, and so on) and if you don't have any plans for that, you can keep on watching lectures, and then also work on your second project. Then you'll be fine.",Machine Learning Zoomcamp,2022,
550613,"I'm trying to understand the question… So the question is “What to do if you have class imbalance, but instead of two classes, you have three classes?” For that, when you define your train_test_split in SciKit Learn they have a thing called stratify. You can put your target variable here and then what this function will do is use these data points (this target) to make sure that the distribution between train, test, and validation of this variable is the same. I'm not sure if what I say makes any sense to you, but maybe I should give more context. 
Your test, train, and validation should have the same amount of the same distribution for all – for example, if we are talking about binary classification, the amount of zeros and ones should be the same in both train, validation and test. And when we have class imbalance, then sometimes it's difficult to make sure that this happens. Because in a usual case, when we don't use stratification, what can happen is that we have more, let's say, positive examples in the train dataset than in validation – just by chance, because the positive examples are quite rare and stratification makes sure that it doesn't happen. Apart from that, you don't need to do anything else, to my knowledge. Maybe I misunderstood your question, so maybe ask a follow-up question in Slack.",Machine Learning Zoomcamp,2022,
208635,"I structure the interviews that I conduct around projects. Typically, a technical interview that I conduct is usually one hour long. The first five minutes is just a bit of introduction, where I say a few words about myself and then you also say a few words about yourself. 
Then, in the next 25 minutes, we talk about your past project – your past experience. Here, I would ask you to tell me a few words about yourself, select a project, and then ask you to talk me through this project. How did it start? Who was working on this project? What is the main problem that this project was meant to solve? What is the main goal? 
Then we would go into technical details – which kind of model you selected, why, and so on. Here, I wouldn't ask specifically theoretical questions just out of nowhere. All these questions would be based on your experience – on everything you say. Basically, if you said that, “For this project, I used XGBoost,” then I would ask “Why XGBoost? Why not something else? Was it better? What kind of other models did you try? Why this particular choice? How exactly did you evaluate your model? This is when all the topics from this course would start appearing? “What is the difference between XGBoost and Random Forest? Why did you decide to settle on this particular model? Then, based on the materials of this course, you can answer these questions. 
Also, I spend a bit of time asking about your deployment experience, which is also something we cover in this course. This would be the first 25 minutes. The next 20 minutes is about live coding. I ask a problem and then we use Python to solve this problem. And then the last 10 minutes is the time for the candidate to ask questions. 
Maybe I’ll just add a bit of a shameless plug. If you go to my GitHub profile, and then look at the pinned repositories – the first one is data science interviews – you will find a lot of interview questions. For many of these questions, you will find answers in this course, but also in this resource. For example, here’s “What is gradient boosting trees?” and so on. This is quite a useful thing. Also, I was talking about coding for the second half of the interviews I conduct – I might ask some things from here. I typically don't ask SQL. I know many companies do ask this. But I would ask something from this part for the coding part of the interview.
In the course, we focus a lot on projects. There is a lot of emphasis on doing projects in this course. Perhaps this is what you should pay attention to. Then, when you have interviews after finishing this course, you will probably also talk about the projects you did. This is when all these things might start coming up. Sometimes the questions are trivia questions. By “trivia” I mean that somebody just comes across this list of questions (on GitHub), and they go “Let me flip a coin and ask this question.” Then they just ask this question, “How do we check if a variable follows a normal distribution?” And then you need to answer that. 
Maybe for this particular question, by the way, you will not find an answer in the course. Or at least partially – maybe we talk a bit about bell-shaped curves. Then again, it's not the end of the world if you don't know the answer to some of the trivia questions. I think in the end what matters is your projects, your portfolio, and not if you know some encyclopedic material or not. So focus on projects.",Machine Learning Zoomcamp,2022,
19338,"Yeah, I think it is. I don't see a reason why it's not.",Machine Learning Zoomcamp,2022,
601871,"Yes, you can do this, especially for your project. For your project, you can choose any technology you want. It could be Tableau instead of what we've covered.",Data Engineering Zoomcamp,2023,
86693,"Usually, it's not about the model in practice. It’s not about the model you use, or the algorithm you use, but it's more about features. You need to work on creating new features if you want to improve your model. Linear regression, for example, is a powerful model – but sometimes, because it's a linear model, there are some limitations. In many cases, for example, XGBoost will have better predictions, so you can use that. But again, it's usually a lot of manual analysis – exploratory data analysis, understanding which features make sense, and then maybe adding more features, experimenting with different features. 
Here, it's very important to have a good validation framework using which you can test your ideas. You think, “Okay, this feature will be useful.” You add this feature, you implement this feature, then you train a new model, and then you see how well it improves your score on validation. This is the best indicator that you're actually improving something. 
A short, one-line answer to this would be “The best technique is cross-validation or just validation.",Machine Learning Zoomcamp,2022,
393080,"Alexey
We don't explicitly have any limits on the size of the dataset, but 2000 is very low. Maybe you can find something bigger? Or maybe you can find a way to make it bigger – to make it more interesting? Typically, the engineers deal with more data than 2000 records. But I don't think we have any strict limitations on the size. I think we already had this discussion last year. I don't remember if we explicitly put any limits. Just use your own judgment. Imagine that a hiring manager is looking at your project. You probably want to make a good impression, right? And I don't think 2000 records will make a good impression, or a 100-kilobyte dataset will make a good impression. Megabytes? Eh. But if it's in gigabytes, it's already good.",Data Engineering Zoomcamp,2023,
189907,"Jeff
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest.
Alexey
At least six people are interested. 
Jeff
[chuckles] At least six. 
Alexey
Maybe more, but they just didn’t know that they could vote.",Data Engineering Zoomcamp,2023,
272263,"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff.",Data Engineering Zoomcamp,2023,
517141,"Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.",Data Engineering Zoomcamp,2023,
794089,You use validation for that and then you see what works best on validation. Then you use that test dataset to make sure you do not overfit. That's how we do it.,Machine Learning Zoomcamp,2022,
292535,"Yes, of course, it can. The sky's the limit. But I think it will be quite challenging for somebody without experience to get a data science job remotely. Then again, it means remotely within the same country – then it's possible. For example, at OLX (the company where I work) we had both interns and juniors who were hired to work remotely – if they are in the same city or in the same country because we're hiring remotely. No problems with that. But let's say if you're based in one country and you want to work remotely for an American company, or a European company, then it might be tricky. 
Not every company is ready to do that. There are companies who do that. You probably need to find these “remote-first” companies that hire everywhere and then you will need to convince them that you are good enough, even though you don't have experience so that you can work for them. It’s tricky. This is one of the reasons we actively encourage you to do this “learning in public” part. If you do this often, then you will be able to get a job as a fresher, even though you might not have “formal” experience. But because you did all these projects, you shared your learning and maybe some people started following you, then companies might notice you. Then it will be easier for them to hire you. 
I think this is actually what happened last year. I don't promise it will happen this year, too – but last year, there was a company, Delphi, who hired two interns. The company is Berlin-based and one of the interns was in Milan, Italy, and the other intern was in Ukraine. So it wasn't the same country, yet they managed to hire them. So I think it's possible. It was actually possible with this course. I don't know if we will have this kind of setup this year. 
I haven't heard from any company who wants to hire interns this year. If you're working at such a company and you need interns, then please get in touch with me. People who graduate from this course will be really good interns, I am sure. But you can maybe help hire people like this person who asked the question.",Machine Learning Zoomcamp,2022,
373121,"Yeah, we can pat you on your shoulder. [chuckles] There’s the position on the leaderboard. The leaderboard is anonymized so nobody will know that this hash belongs to you, but if you want, at the end we will have a forum where you can share your actual contact information – your actual information about you and you will be the first in this public leaderboard. This is how I can kind of give you a reward for that. But apart from that – eternal glory? Maybe that?",Data Engineering Zoomcamp,2023,
660191,"By “train,” I assume you mean you want to collect your dataset. What you should start with is asking yourself what kind of problem you want to solve. Once you identify the problem, you then need to think about “Where can I get this data? Do I need to collect this data yourself? Or can I scrape it? Or can I maybe buy it or find it somewhere?” So that's what you need to ask yourself. Then with the goal in mind, you go to the internet, you try to find the dataset. Maybe you don't find anything so then you can see if there is a website that has this data. For example, if you want to collect a dataset for house prices, then you go to a website that lists houses and then you can scrape this information from the website, or scrape information from Amazon, or scrape information from some other resource. So then you can scrape this data and in this way, you collect the dataset. 
If the information you are looking for does not exist at all, then you will probably need to collect a dataset yourself. It really depends on the nature of the dataset. You can ask some crowdsourcing platforms, such as Amazon Mechanical Turk or Toloka. There are actually multiple companies like this (that do crowdsourcing). With Toloka, we actually had a video on our channel. It was a workshop with Toloka. I think with Toloka, we probably have another workshop soon, as well. So keep an eye on this. But yeah, so you can use crowdsourcing, or you can just go and collect it yourself. It's a very generic recommendation, because I don't know what exactly you want to do. Maybe you can share more information and we can talk about this in Slack.",Machine Learning Zoomcamp,2022,
18314,"For the course material, we cannot re-record it for other clouds. If you know what you are doing, by all means – use other clouds. For your projects, you can do whatever you want.",Data Engineering Zoomcamp,2023,
982235,"I think the idea here is that you can do this, but you don't necessarily have to do it right now. You can do this at the end. Because if you do destroy it now, of course, the buckets and BigQuery datasets will be gone. When you apply it again in the future, it will be recreated. Indeed, the data will be gone. Don't destroy.",Data Engineering Zoomcamp,2023,
537743,"It's super weird. I got used to this, but the first time I did it, it was super weird. It's really strange. On a scale from 1 to 10? I don't know. 8, maybe. It's much more interactive (or less weird) when there is somebody on the other end of this call. But I know that you're actually listening there. I just don't see you. Which makes it maybe a little bit less weird.",Machine Learning Zoomcamp,2022,
689348,"We're not covering time series data here. I'm not really an expert in that. Maybe I can give you a quick answer from what I would do in this case. Here, you probably want to have the same number of samples (units) for each month. I think in Pandas, you can do this with the resample method. This resample would make sure that you have the same amount of records for each of the dates, for example – for each of the months, days and so on. I think I would go in this direction. 
When it comes to actually predicting, it's a totally different story. The usual approach is like what we cover here for linear regression – they don't necessarily work here. They work with some adjustments. For that, I will, again, invite you to check our channel. In our channel, we covered these topics recently. There was the podcast called Feature Engineering for Time Series Forecasting from Kishan and then Probabilistic Demand Forecasting at Scale. These two talks could be interesting for you if you're into Time Series.",Machine Learning Zoomcamp,2022,
753636,"Yeah, I think this is what I just said. Yes, it is more important. Some datasets are just inherently difficult to deal with, like click prediction, fraud prediction – with this kind of data, you usually cannot have a very good score. If you have a very good score there, then something is probably wrong. Maybe you are overfitting or you have some sort of leakage or whatever. If I see a model for click prediction that is super accurate, it will be very suspicious.",Machine Learning Zoomcamp,2022,
883817,"Michael
Actually, I do have a resource – Real Python. They do have some paid content, but they also have a lot of free content that I look at frequently. Also, if you go to sites like LeetCode, or do the Advent of Code – not necessarily to solve the problems, but looking at others’ solutions – you can see some very good examples of clean code. Type Hinting is something that I’ve picked up a few years back from Advent of Code. I still don't do it all the time, but when you see more experienced people write code, you kind of know what you're aiming for. And I think that gives you a lot of direction.
Alexey
I think in this video, Getting a Data Engineering Job, Jeff also talks about learning good coding practices. I remember that the project he recommended to look at, if you want to learn more about good coding, was Prefect. He said, “Yeah, go check out Prefect. It's a great way to learn how to write good Python code.”I guess it's a good plug, right? [chuckles] 
Then there is also a book called Clean Code. It's about Java. It's a very nice book. It's actually not about Java, it's about clean code, but the examples there are in Java. For me, it was very useful when I was starting coding. But I was a Java developer. What I know is there are examples in this book in Python. Even though the book is about Java, there are GitHub repos where the same concepts are illustrated with Python. You can check that out, too.
Jeff
Yeah, I haven't read this book. But there's also a Clean Code in Python book that is out there. It has good reviews on Amazon. It could be worth checking out.",Data Engineering Zoomcamp,2023,
73776,"I don't find it particularly useful to be honest. For binary, yes, it is definitely used. For multi-class… it becomes difficult to interpret, to be honest. So for multiclass, in my experience, accuracy works the best. The rest become more and more difficult to understand as the number of classes rises.",Machine Learning Zoomcamp,2022,
366332,"Alexey
That’s a very difficult question. I don't know if I should attempt doing any forecasting. Anyone else want to answer that question?
Luis
I can answer because… Well, I don't know if I can, but if I may. I left my previous company in the last week of the year (2022) and I already am in lots of processes, including in the final process to be a data engineer. Today, it's the 16th of January, 2023. The market is completely imbalanced, in the good sense – for us. [chuckles] 
Alexey
So there’s hope that, even though there is a recession and layoffs, it's not difficult to find a job. That's what you're saying?
Luis
Exactly, exactly. Completely. Actually, the pandemic was a good thing for us in IT, because a lot of companies are recruiting remotely, so you don't even need to go to the United States to work for that company, or other similar situations.",Data Engineering Zoomcamp,2023,
590606,"For those who don't know, Streamlit and Gradio are frameworks for creating user interfaces for machine learning. The main difference between them is that Gradio is designed specifically for machine learning, while Streamlit is a general purpose user interface library in Python. You can think of them as Flask and Bento ML. With Flask, you can deploy a machine learning model, but you can also deploy much, much more. While in Bento ML you can only deploy machine learning. Here, it's the same – with Streamlit, you can build a machine learning interface for a machine learning model, but you can also build pretty much anything. With Gradio, it's only for machine learning. At least this is my understanding. I have used Streamlit. I have not extensively used Gradio.",Machine Learning Zoomcamp,2022,
657509,"Alexey
Yeah, it can be applied to every company – to any company.",Data Engineering Zoomcamp,2023,
82367,"I personally prefer Dictionary Vectorizer because, later in module five, we will put it inside a web service and the web service receives a JSON object – usually it's a dictionary. So we can just take whatever we get as a request, put this inside the Dictionary Vectorizer and then get a matrix. But with OneHotEncoder, I find it more difficult to use – you need to do more. Usually, it's just a matter of taste, I would say. I think if I go back to last year’s Office Hours, in one of the videos I talk about OneHotEncoding. I basically show how to use it. I don’t remember which one, so I think I should go through this playlist and see. I think I described it somewhere. But my personal preference is to use Dictionary Vectorizer because it is simpler to use and you can see how much more things you need to do for OneHotEncoder. But again, this is just personal preference. With both of them, at the end, you actually get the same result, so it doesn't matter.",Machine Learning Zoomcamp,2022,
787264,I will need to ask Ankush. Maybe ask that in Slack and then he will answer.,Data Engineering Zoomcamp,2023,
842696,"Which deep learning class are you talking about? Maybe I don't understand. If you're talking about the competition – you can still take part in the competition. There is still more than one month to go. It ends on February 1, 2023. One week before the end, you will not be able to submit. The last time to enter the competition will be one week before February 1, 2023. If you're talking about something else, perhaps this module, then it's always open. You can come back to this material when you have time and just take it.",Machine Learning Zoomcamp,2022,
364660,"Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.",Machine Learning Zoomcamp,2022,
258304,"I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.",Data Engineering Zoomcamp,2023,
712882,"This is maybe a long discussion. For simple projects, lambda is probably the best choice, because you don't need to worry about paying for the time when your servers are up. Kubernetes is usually needed when you have a lot of traffic. For example, when you have 1 million requests per day, then it's time to consider Kubernetes because it could be cheaper than lambda. I guess this is like a rule of thumb – if you have a lot of requests go with Kubernetes, if you don't have a lot of requests, go with lambda. Also, for Kubernetes, you need people who will know Kubernetes well. If you're just a data scientist and in your company, nobody else uses Kubernetes, I don't think it's a good idea to use Kubernetes. Only use Kubernetes if you already have Kubernetes in your company and if there are people who know what they’re doing and know how to operate a Kubernetes cluster. Because as data scientists, as machine learning engineers, that might not be your core responsibility to maintain Kubernetes clusters. You will probably have a lot of other work, so you don't want to give yourself any more work by maintaining the cluster.",Machine Learning Zoomcamp,2022,
186536,"I'm happy to support you if you want to organize that. In DataTalks.Club, we have so many initiatives that it will be very difficult for us to actively organize this. But if you want to do this, I can give you all the support you need. I can promote this, I can send it in the mailing list, I can announce this on LinkedIn. So I will be very happy to organize that, but you will need to think of what kind of project it could be. We can actually think together. If you want, you can start putting some of these ideas in a Google document and then we can discuss this and run it. But somebody needs to start this initiative. 
I, unfortunately, cannot do this – but I will be very happy if we do this together. We can think of some cases, actually. One of the examples somebody shared in these Office Hours was to train a model that could select the best answer from the FAQ. We have a lot of answers here and many people go to Slack to ask a question. Many of these questions are already in this FAQ document. So a good “close to real-life” project could be to build a system that can actually select the best answer from the FAQ for a question in Slack. It could be a nice idea – if you’re interested, let me know and we can see how to organize that. Or maybe you have some other ideas.",Machine Learning Zoomcamp,2022,
273900,"Okay, what is Dataform? Manage data pipelines in BigQuery. The page looks good. [chuckles] But apart from that, I cannot tell you more. Future of Google Vertex AI? Well, Google Vertex AI seems quite cool. It simplifies many things. It has a future, but also, I haven't used it a lot. But I can compare it with SageMaker. It's a similar thing on AWS. With SageMaker, you can start using machine learning quite quickly. But then at some point, it becomes restrictive. There is only a certain way of doing things. I use SageMaker a lot and then, over time, I realized that I like the approach with Flask and Kubernetes a lot more, for example. But yeah, they look quite good and I think it's worth learning them if you want to do things fast. Why not?",Machine Learning Zoomcamp,2022,
39122,"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally.",Data Engineering Zoomcamp,2023,
582174,"Alexey
Yeah, they're very similar. The services you have on AWS, you'll have exactly the same service, but with a different name in GCP and Azure.",Data Engineering Zoomcamp,2023,
650314,"I have not used it, so I don't know.",Machine Learning Zoomcamp,2022,
847229,"Yes, but I cannot guarantee that you will arrive at the same answer as without it. To be on the safe side, it’s better to use the materials from the course. Nothing is stopping you from trying it with Scikit Learn as well. If you think it's more useful for you to do it this way, then by all means do it. Again, homework is just for checking your understanding of the course content. You decide what you do with your time. I would recommend first understanding how to implement linear regression with NumPy. Then in week three, when we start covering SciKit Learn, you can just use it from that point on.",Machine Learning Zoomcamp,2022,
134965,"Alexey
Typically you do not, because in the streaming pipeline, the consumers that you have in streams are reactive. Once there is a message in a queue or in a topic, the consumers consume this message and do something. Then they often put the result to another stream and then there is another consumer that reads from this stream and produces something. The execution is triggered automatically because the consumers are reactive. That's why you don't really need an orchestrator. They just wait for their piece of work to work on (to consume), they consume and then they put the results somewhere. Meanwhile, in batch processing, there should be a way to execute a thing, like a job after a job, or a task after a task, in a particular sequence. That's why we need an orchestrator there. There, it's not reactive. Something needs to execute things in order.",Data Engineering Zoomcamp,2023,
191943,"We have an article about that at DataTalks.Club. If you go to the Articles section, there is an article called Roles in a Data Team. I discuss different roles, including machine learning engineering and MLOps engineering. MLOps is not really a role – yet, at least. Usually, MLOps is a set of tools and best practices for making sure it's easier to productionize machine learning models. Usually machine learning engineers apply these MLOps principles. But sometimes there is a separate role – it can be DevOps, site reliability engineers, or sometimes it's MLOps engineers – whose job is to make sure that the infrastructure for running machine learning projects is there and it's stable, reliable, and so on. For data engineering, the counterpart is DataOps. I think the concepts are pretty similar.",Data Engineering Zoomcamp,2023,
809479,"For the homework assignments, we expect that you do them in Python. Prefect, to my knowledge, does not work in Scala. For example, for Spark, you can use Scala. For Kafka, you can use Scala. If you want to use Scala, use it. For your final project, you can use Scala as well, just add as much documentation as possible for people who will review your projects, because they most likely don't know Scala.",Data Engineering Zoomcamp,2023,
405658,"Jeff
We do have a Spark integration through a couple things. Let's look here in the collections again. We have an integration for you that you could use with Databricks. Spark often runs with Databricks. That's a good option there. Fugue is another tool that could be used. It unifies a number of different ways you could run code, but includes Spark and Dask and Ray also. So that could be a good tool. And if you wanted to write Spark code directly and use it with Prefect, you could integrate with something – we don't have something built out right there, but I think folks do kick off Spark jobs with Prefect. If you want to do things in a couple different ways, there are options there. We don't have a direct Dataproc module in Prefect GCP, or a direct block. But surely, there's an API there, and you can probably kick things back and forth.",Data Engineering Zoomcamp,2023,
903346,"Jeff
We don't have an @subflow decorator, so you wouldn't use that. But if you use some flow-decorated function and call another function with that flow also decorated, that's just kind of the way we use subflows. There is also another pattern where you can run deployments. You can call a function to run deployments called run_deployment from within a flow. That lets you just, “Alright, I want to run a bunch of things here. Maybe I have them in a loop or something.” That's a pretty cool way to go.",Data Engineering Zoomcamp,2023,
322081,"“CoursES,” plural? I don't know which courses you mean. Probably, I assume you mean the ML Zoomcamp? Maybe other Zoomcamps too? Actually, the answer is the same – if you like the company, just apply. Apply to companies that you like and that's it. I don't feel that there should be any specific answer to this question. It’s up to you. 
Look at the companies that are hiring around you in the country where you live or companies that are hiring remotely – look if they have positions that you like and apply for these positions. It’s as simple as that. Maybe you wanted to hear specific company names. But, I don't know.",Machine Learning Zoomcamp,2022,
637447,"Victoria
I think that goes a little bit to the fact that almost none of us except Ankush are data engineers. We're all capable, or we think we're capable, of teaching something that is relevant to data engineers. It's the case where every company has their own world, or roles, and they set it differently. Ankush, you could even share that, at one company you work with DBT, and then at another company that was being done by someone else, right?
Ankush
Yeah, exactly. Completely, completely different there. Data engineering in its own self has become a very gray area, honestly. There is also a lot of DataOps there, there is a lot of MLOps there, ML engineering is also considered somehow data engineering. The only thing left is data science and I'm pretty sure that will also become data engineering sooner or later. [chuckles] But this is very specific to Google or very specific to a company you're working for. 
I’ve worked in a company where data engineers do mostly SQL and DBT. There are companies where data engineers are supposed to do MLOps, DataOps, and data engineering. And obviously, others which just focus on data engineering and classify them as streaming data engineers and batch data engineers. It's really very specific to the company you're talking about.",Data Engineering Zoomcamp,2023,
926617,"I am ashamed to admit that I don't speak German that well. I wouldn't be able to use German for work. Plus I've been in Germany for quite a long time, too. That's why I'm kind of ashamed to admit that. [chuckles] But you can live quite well in Berlin. Here, I’m talking about Berlin – the rest of Germany is different – but in Berlin you can get around without speaking a word of German. I do speak some German but I'm not fluent enough to actually use German at work. Maybe if I had to use German at work I would become more fluent, but so far, most jobs don't require German.",Machine Learning Zoomcamp,2022,
402149,"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens.",Data Engineering Zoomcamp,2023,
361364,"Alexey
I know that in AWS, they have step functions, which is a serverless workflow orchestrator. It’s fine if it works for you.
Jeff
Yeah. Prefect has some serverless options too. Depending on your cloud provider, we support almost all of them. Here's a link to a post that Anna wrote. There are other patterns, too. We are working on something where – right now, you have to put your agent somewhere, so you could put your agent even on ECS, potentially. But we're looking at a managed solution where if someone just doesn't want to hassle with it, we can just deploy things for them using Prefect Cloud. But that's a few months off, probably. 
Alexey
Here, what does this project do? It says streaming. That's something? 
Jeff
Yeah, this one does a number of different things. If I recall, it's using Cloud Functions and it's using GitHub Actions, to do a CI/CD kind of deployment. There are a couple things going on there. But Anna has a whole repository and a walkthrough in that blog post. That's just one example of doing it.
Kalise
Essentially, what it's gonna do is – the same way in the videos, how you can run a Prefect flow just by calling a Python function, you can run that wherever. And if you're connected to cloud, you're going to be able to visualize that. You're not going to have to host the UI, the Orion server. So if you had some sort of event happening and you just wanted to execute your Python function as a flow to get visibility, you can just do it right there. 
Alexey  
I think in GCP, there is also something similar to step functions.",Data Engineering Zoomcamp,2023,
757091,"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular.",Data Engineering Zoomcamp,2023,
647925,"I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.",Machine Learning Zoomcamp,2022,
585024,Everything you do here is individual. You don't form teams.,Data Engineering Zoomcamp,2023,
282072,"Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.",Machine Learning Zoomcamp,2022,
950760,"You are in the right place. I was a Java developer. I became interested in machine learning and this course is quintessence… basically all my experience and everything I needed to go through, to learn through, and I think that this is how I best learned, as a Java developer, as an engineer – by doing projects. This course was actually targeted at software engineers who wanted to become machine learning engineers. So you are in the best place.",Machine Learning Zoomcamp,2022,
389267,"It’s already there. It has been there for quite some time. So if you go here, there is a note – and this is the link.",Data Engineering Zoomcamp,2023,
854404,"Yeah, that's what the lecture was about, wasn't it? I don't go into details about that. The theory is quite complicated there, to be honest. But there is a paper – this one. “Gradient boosting machines, a tutorial” by Alexey Natekin. This article talks about derivatives and the functional space. If you're into this kind of stuff, you can check out how exactly it works. But if you're not, don't worry. You can just skip that and focus on improving your validation score.",Machine Learning Zoomcamp,2022,
85872,"The way I usually do it is I have one Anaconda environment (I don't create a separate Conda environment for all experiments, I just have one Anaconda) [Terminal image as reference] I usually stay in the base environment. All the libraries I need for development are here, so I use them. And when I need to productionize it, then I create an environment with Pipenv. So this is how I usually do this. It doesn't mean that you have to do this the same way or that what I do is the “right” way of doing this. This is just the way I usually do it. 
Some of you might say, “But we need to have an environment from the very beginning.” And you will be right in this case. Ideally, for each project you start, you will start it with creating a fresh virtual environment and you install the packages there. Then you have no surprises when you move from the exploration and training phase to the deployment phase – all your dependencies are the same. But I'm lazy and I just use the environment I have for all the POCs – for all the, let's say, exploration projects. Then, when porting them to production, I create environments.",Machine Learning Zoomcamp,2022,568107_qa4_01.jpg
812034,"Jeff
Yes, absolutely. You can certainly deploy Prefect to those platforms. We also have a Helm chart that you can use. So you can check that out if you'd like. We do have people who run Prefect in lots of different workspaces, so it’s definitely possible.",Data Engineering Zoomcamp,2023,
13558,"Well, a few things I always look at are missing values – that's the first thing. Then I also look at min/max values. Sometimes, the missing values are encoded with minus nine and nine or some other values – very low or very high values. You can see these if you do DataFrame.describe. You can also see if you have some categorical data in numerical columns or strings in numerical columns. Things like that. These things come to my mind immediately. Apart from that, it's just usually just doing exploratory data analysis and seeing if anything feels wrong. If something feels wrong, you just try to dig deeper until you understand what's going on there.",Machine Learning Zoomcamp,2022,
233513,"It's “Elements of Statistical Learning.” This is the book. It's actually free. If you click on the link, you can get the book. It's legally free – it's published by the authors for free. Yeah, it's a great book. For example, we talked about the normal equation. It’s called “least squares”. Least squares is what we call a “normal equation”. They start with that. I will not spend time doing this now. But check out this book. It's quite mathematical, but you might like it.",Machine Learning Zoomcamp,2022,
986222,"We don't cover high bias/high variance concepts. Again, check other theoretical courses from Andrew Ng. What you should do when you have errors is reduce the complexity of your models, for example. It really depends, right? Let's say you have a simple model, and you see that the simple model has this performance on your validation dataset. Then you can test that hypothesis, “Will getting a more complex model solve my problem or not?” Then you just try this more complex model and see what happens. Based on that, you make your decision. 
I don't know if I actually answered your question. Maybe a summary of my answer is “use validation and it will guide you in all these decisions.” And test error is, again, if you use your validation dataset many times, then at the end, you can't necessarily trust it because it might happen that the best model is the best just by pure luck. So that's why you have the test dataset to verify that it's not overfitting.",Machine Learning Zoomcamp,2022,
640149,Yes.,Data Engineering Zoomcamp,2023,
677725,"That's why we discussed Docker in the first week. We actually meant this week as a kind of Docker refresher. In the end, many people didn't have any Docker experience. They took it and it was maybe a bumpy ride, but it was fine. Many people did not have Docker experience and they successfully finished the first week. But it was probably harder than other weeks.",Data Engineering Zoomcamp,2023,
619483,Put your code to GitHub. Let's say I have my code with my homework. Let's say your homework is in this directory. Copy this one and put it in the form. That's how you do this.,Data Engineering Zoomcamp,2023,
62150,I don't have an answer to that. I think all companies use it. It doesn't matter what size the company is. But maybe the fraction of companies that use Prefect is still smaller compared to Airflow.,Data Engineering Zoomcamp,2023,
524962,"You're more than welcome to do this, and your peers will appreciate it. But we capped the number of points you get for that at three projects. So you will not get more than three points for each project. You will get nine points for evaluating the projects, but you will not get 12 points. So it's more than welcome – you will learn a lot by doing this – but you will not get extra points. 
You shouldn't evaluate the projects just to get the points, you should evaluate them because you want to help your peers and you want to learn something new. This is a very good motivation and you will be rewarded with new knowledge. This is how I should have answered that. [chuckles] Not with points.",Machine Learning Zoomcamp,2022,
555375,"You can hypothetically think about all the situations, but what I would do in your case is just set up a validation strategy and then test all your assumptions. Then, based on the performance of the model and the validation set, you can see if it's performing or not. It doesn't matter if there are correlated features, non-correlated features – you see the performance metric, (RMSE, in this case) and if this metric is good, you just go ahead and use this model. I hope that answered this question. 
If you want to have some sort of theoretical justification for how it works, then perhaps you should check some theoretical textbooks, like Elements of Statistical Learning or some other courses that focus on theory. Here, I want to show you how to use these things from a practical point of view and that's why I put so much emphasis on setting up this validation strategy. Because I think this is one of the most important things in machine learning. If you can reliably test your hypothesis, then you can do pretty much everything you want – anything you can imagine – you just go and try it on your validation dataset. And if it works, it works – you can just use it.",Machine Learning Zoomcamp,2022,
248031,"Jeff
No features are missing. Everything's there. Easy. 
Alexey
And more, right? [chuckles]
Jeff
Yeah, and way more. [chuckles] You know, I think there are always new things we can add. It's kind of nice to have multiple tools that are all working hard to compete with each other and up the game for workflow orchestration. I think the winners are everybody who are the users there. So we will keep doing that. As I mentioned earlier, we have more visualization tools in the works and things happening there. You always want to keep making things easier and easier to use. In this course, you got to see a number of things, but we didn't even touch on things like parallelizing, working with Dask and Ray. We have integrations for things like that. You can always just make it an easier process. There's a number of parts that you have to start for any of these services, whether it's Airflow, Dagster, or us. We want to do things like make that easier and easier. There are lots of things there. We have a free Python version just like everybody else, because we want people to go and use it and get value out of it. And that's great. Then for companies that need to have more enterprise features – you want single sign-on, you want to be able to control things when people leave your organization, all that kind of stuff. We got a company behind it. It's a win-win process for everybody and it allows us to get paid and work on Prefect and do things like teach courses like this. And it helps everyone do it. So I think it's good for all data engineering.",Data Engineering Zoomcamp,2023,
33221,"There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.",Machine Learning Zoomcamp,2022,
383875,"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model.",Machine Learning Zoomcamp,2022,
957623,"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer.",Machine Learning Zoomcamp,2022,
546298,"Alexey
Pretty much every week is real world data engineering. I don't think we should spend too much time answering this.",Data Engineering Zoomcamp,2023,
860386,"I'll show you what to do in this case. When you go here to Slack, there is this form. Click on this form, fill it in, and we will invite you manually.",Data Engineering Zoomcamp,2023,
295325,"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is.",Machine Learning Zoomcamp,2022,
526582,"I don't know. Since we covered Keras in the course, I'm kind of biased, because my advice would be to learn Keras. But I do admit that PyTorch is becoming more and more popular. Probably, if you look at the community size, you will find a lot more things in PyTorch than in TensorFlow. Also many new things are first implemented in PyTorch and then ported to TensorFlow. For example, Stable Diffusion was first implemented in PyTorch and then ported to TensorFlow. But in the end, it actually doesn't matter. They're quite similar. If you’re now learning TensorFlow or Keras, and then you join a company where they use PyTorch, you will just switch to PyTorch and the other way around. On a higher level, it doesn't really matter, but if you want to go with popularity, then maybe PyTorch is more popular for modeling purposes. But if we talk about model deployment and model serving, I think PyTorch is still getting there. Because of TensorFlow Serving and there’s also a thing called TensorFlow Extended and all that, it's backed by any data, but to me personally, it feels that TensorFlow is more mature when it comes to model deployment. I might be wrong, but this is my feeling.",Machine Learning Zoomcamp,2022,
337669,"Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.",Data Engineering Zoomcamp,2023,
260208,"I don't know much about this, to be honest. I know that their model for predicting the price of real estate went rogue. So it started predicting some… I won’t say more, because I'm not super sure about what exactly happened there. I wasn't really following. I know that the company lost a lot of money because of a rogue model. I think if you just Google that you will find an explanation.",Machine Learning Zoomcamp,2022,
966799,"It can be. Docker makes it a lot easier to do this. But it doesn't have to be Docker. With lambda, you will need to prepare a ZIP archive that has everything you need for deploying the lambda function. So you will need to put all your dependencies like NumPy, Pillow, and TensorFlow Lite inside a ZIP archive. You package everything in this ZIP archive, you put the ZIP archive to S3, and then you tell lambda, “Okay, this is where my files are. Get them.” You need to be careful when you prepare the ZIP archive. Let's say if you use Mac OS, and you're on the M1 chip then most likely, the ZIP archive you prepare on your Mac will not work on lambda. So you will probably need to do this inside Docker. Then inside Docker, you prepare a ZIP archive, and then you use the ZIP archive to upload this to AWS. If what I'm saying makes no sense to you, and it probably does not if it's the first time you use lambda, you can just, again, use Google, “AWS lambda python prepare a ZIP file with dependencies”. Yeah, I'm sure if you try this, this is what we need. When you do “pip install -r requirements” you put all the things in the requirements, and -t is the folder where it should install all the requirements. I would actually put it into a separate folder called “build,” for example. Then it will install all the dependencies to this build folder, and then you go to build and then you do ZIP to package everything. Then this is your ZIP file. That's what you need to do. I’m glad we talked about that. If it's not clear – if you're stuck – let's talk about this in Slack.",Machine Learning Zoomcamp,2022,
314791,"That's a very good idea. We were actually thinking of doing something like this for December. December will be somewhat of a slow month – Christmas holidays and New Year holidays, we'll have a break. But we were thinking maybe we can start a competition at the beginning of December and finish it at the end of January, with other things. 
Actually, I even had an idea of using this generated dataset of dinosaurs and dragons. There are dinosaurs and you can also have dragons here – then you can build a model that tells the dinosaurs apart from the dragons. So that's one idea. Maybe it's boring. I don't know. We haven't come up with a better idea yet, so if you have some suggestions of what could be a good dataset of a good problem for this Kaggle competition, please let us know.",Machine Learning Zoomcamp,2022,
939441,"We had a talk about this on our YouTube channel. As far as I remember, it was called Machine Learning Design Patterns. These patterns were mostly about engineering – engineering patterns – but the first pattern, as far as I remember, was about rebalancing. So check it out. Sara explains when to use these techniques. Here, the important part is that you apply these techniques only to your training dataset. You keep your validation dataset intact – you don't change it – because you want to have a reliable way of evaluating the performance of your model. You apply different techniques to your training dataset and then you see how exactly it changes the performance on your validation dataset. 
So you apply these techniques to train, experiment, try different ones, and then go with the one that has the best uplift for your score on validation. That's usually how you do this. This will tell you if you actually need any of these techniques, or whether just throwing all the data that you have into the model is good enough, so you don't need to make the process more complicated than it should be.",Machine Learning Zoomcamp,2022,
473934,"Go to our date engineering course page. There should be a part with projects. Here, we have a peer review assignment. Here, you can actually see projects from the previous iteration. For example, this one. I think this is one actually from Lois who is one of our teaching assistants. This is his project. You can see what he did with this project. This is how you do this. You go through this, click on the project and you can see, for example, what Ilya (one of our students) did. You can see how they approached it. This is how you can find projects from the previous iteration.",Data Engineering Zoomcamp,2023,
634846,"As I said last time, you don't have to run it. I do encourage you to run it, to try it, to learn as much as possible. But understand that we have lives and not everyone can have time, or can deal with, problems in somebody else's models. I don't require you to run everything, I encourage you to run. Here “reproducibility” just means checking the code for basic errors, checking that the data is there, checking that it's clear what to do in the readme, there are no errors in the code – maybe you see stack traces, which is a clear indication that there are errors. You can also see the Docker file and check that there are no errors and things like that. So you don't have to run it, but if you have time, if you want to learn, please do run it.",Machine Learning Zoomcamp,2022,
415398,"No, we don't cover Flink",Data Engineering Zoomcamp,2023,
245374,"Jeff
See if this maybe helps a little bit. We have a diagram here talking about your execution environment. This is your infrastructure and your agent running (in this assumption) locally and using Prefect Cloud or the Prefect Orion server. There is an API here on the server. That's what you're interfacing with. And there's a UI that displays information from that API server. The agent opens a connection with the API server and scheduled flow runs get returned and all the information that is needed there and that schedule flow run. Then the agent kicks off the flow run with the flow code in the infrastructure that's specified. Then the results of how that goes gets sent to the API server and that information gets displayed in the cloud API server. I'm just guessing here – maybe a little bit of this question is coming from, (just one thing that came up a time or two) like, “The stuff’s on GitHub. Why do I also need to have a path to the local file to show where my flow code is when I'm making the deployment?” The reason for that is – just to make sure that all of the needed files are available in the place where the deployment is built. This is kind of how things are assumed and needed to work at this point with Prefect.",Data Engineering Zoomcamp,2023,
667338,I guess this is related to the other question. I don't know. Just check it and then you'll see.,Machine Learning Zoomcamp,2022,
597569,"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me",Machine Learning Zoomcamp,2022,
659646,"Yeah. Thanks a lot for adding that. Pair your learning with business analysis. That's always a good suggestion. If you’re already working and have some colleagues with experience that you want to get, just talk to these colleagues. That's always a great idea.",Machine Learning Zoomcamp,2022,
572874,"Well, to me, you have to know how to set up a validation framework. You have to validate your data. If you asked me, “What is better – random forest or logistic regression?” I would say, “Okay, just do validation and check it.” If you don't know how to do this, I would suggest learning how to do this. Maybe “unforgivable” is a strong word. For example, if a candidate that I interview doesn't know about validation, then this candidate will probably be rejected. 
So I think this is one of the most important skills – knowing how to validate your model. Then once you know how to do this, you can answer any question by just testing it. If you have an environment where you can experiment – and you can experiment if you have a validation framework – then you can answer all the questions, like, “What if I increase the learning rate?” You just increase it and see what happens. Because it's all case-dependent, it's all data dependent. You need to know how to experiment and be comfortable with experimenting. I think this is the most important thing for a data scientist.",Machine Learning Zoomcamp,2022,
334171,"It's both. If you have any questions you want to ask, then you can use Office Hours for that.",Data Engineering Zoomcamp,2023,
81354,"[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.",Machine Learning Zoomcamp,2022,
708344,"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this.",Machine Learning Zoomcamp,2022,691501_qa1_1.jpg
155856,"That's a very good question. I do recommend it for your pet project. But for the midterm project, you will not have enough time to actually do this. For the midterm project, it’s better if you already have a dataset from somewhere. You will simply not have time (two weeks) to collect your dataset and then do the rest of things. You can start collecting your dataset right now. If this process will finish by the time the project starts. Otherwise, I would not recommend doing this. But if you want to do it for your portfolio project, for future employment, for a future job search, then it's a great idea and please do it. But be mindful of time, because you might not have enough time to finish the project.",Machine Learning Zoomcamp,2022,
369078,"Again, set up your cross validation and then test it on your validation. If your validation says it's fine to drop it, just drop it.",Machine Learning Zoomcamp,2022,
985321,"If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.",Data Engineering Zoomcamp,2023,
235952,"With the ROC curve, it's not easy to find the best thresholds. I usually use accuracy or something else. First, you need to ask yourself what kind of metric you want to optimize. Usually, for example, if we want to optimize for precision and recall, we combine them in F1 score. And then, for example, I would evaluate my classifier against different thresholds and pick the one with the highest F1 score. 
It usually comes from your needs. If you want to have good precision, but not so bad recall, then you probably plot this precision versus recall on different thresholds, like you had to do for the homework. Based on that, you decide what the best threshold for your specific case is. This is how I usually do this.",Machine Learning Zoomcamp,2022,
511886,"Yeah, you definitely need to use cross-validation here. Tree models overfit. That's why you need to use cross-validation – to make sure that this doesn't happen. Usually, when you have such a small dataset, it helps to use very simple models. Trees – when you let them grow indefinitely deep, they do tend to overfit. 
So maybe you stick to the depth of three, or use something like linear regression or logistic regression with very few features. Because you have a small dataset, you need to be very careful. I guess, that’s the general rule – use simpler models and then use cross-validation.",Machine Learning Zoomcamp,2022,
660384,"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things.",Data Engineering Zoomcamp,2023,
296131,"Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.",Machine Learning Zoomcamp,2022,
432897,"Yes, definitely. Many people who did Python took the course and succeeded at the end.",Data Engineering Zoomcamp,2023,
567321,Maybe. I don't know what that means. But yeah. Maybe?,Machine Learning Zoomcamp,2022,
884683,"That is definitely okay. As a data scientist, I often Google. I end up on StackOverflow and I see a code snippet – I copy it and use it in my work. I see nothing wrong if you do the same. That's a usual thing. But if you copy the entire project, that's a different situation, right? [chuckles]",Machine Learning Zoomcamp,2022,
455195,"Just take your time. Go through the materials at your own pace. If we talk about the syllabus, you will only need the second, third, fourth – up to the midterm project. This should be sufficient – up to the seventh module. This should be sufficient to finish the midterm project and the capstone project. You can treat the modules after 7 as extra. They are useful – they are very useful. But if you're really short on time, you can just skip them and use the materials from the first seven modules to complete your capstone project. Once you complete your capstone project, once you must submit everything you did, then you can take the modules after 8 at your own pace. That could be one approach. Another approach could be just taking them without caring about the certificate. Just focus on learning and do things at your own pace. Don't rush.",Machine Learning Zoomcamp,2022,
71745,"I guess? I think yes, because data scientists do not live in isolation from the rest of the company – the rest of the team. The article I referred to actually describes how people work in a team. MLOps is not just about tools, it's also about processes – how exactly your work should be organized in such a way that you can easily maintain it, scale it, and so on. 
This is why data scientists should know some things about this. I guess that answers your question? There are also tools for experiment tracking, so data scientists definitely need to use them. I think I'm just trying to say “Yes, it is important.”",Machine Learning Zoomcamp,2022,
550116,"Docker allows you to run a Docker image – a set of prepared instructions – this is what you put inside a Docker file. Then like when you do Docker build, Docker takes the Docker file and then creates a Docker image and then you can run this image. You can run this image with Docker locally and that's fine. But what Kubernetes does is it allows you to run many different Docker images. Kubernetes is a container orchestration platform. It allows you to take all these Docker images that you’ve built on your local machine with Docker and it allows them to execute somewhere in a cluster. It doesn't have to be on your computer. It can automatically provision more machines, if you're running out of machines and all that. It's not 100% correct what I will say, but you can think about this as “Kubernetes orchestrates Docker images.” You have some images and then you just put them in Kubernetes, and Kubernetes figures out on which machine to run it, how many resources to give it and then it just runs (orchestrates) all these things. So you can think about this as Docker is for local use and Kubernetes is for deployment in the cloud.",Machine Learning Zoomcamp,2022,
912509,"Maybe go check out module two, where we talked about that. I think during Office Hours, we also covered that. I don't remember, to be honest, what I answered to that, but usually, when you have a long tail that goes to plus infinity (to the right) this is when you want to apply a logarithmic transformation and usually when you don't have negative values in your original data  is when you use it.",Machine Learning Zoomcamp,2022,
788623,"You will get out of tutorial hell when you start doing the project in this course, because it will be just a set of guidelines – this matrix – but you will have to do the rest yourself. You will have to find the problem yourself, you will have to do exploratory data analysis yourself, you will have to do data preparation yourself, you will have to do everything yourself – there will be no homework with the exact steps that you need to do to have a project. And this is how you get out of tutorial hell. 
Let's not think about this particular course, but in any setting – you want to find the problem that you want to solve. Once you have the problem, then try to think “What is the best way to solve this? What is the shortest way to solve it?” Or, at least, “What is the next step I need to take to solve it?” And then try to work your way through solving this problem. So focus on the problem. If you just do tutorials, you're not solving problems – you're just doing tutorials. So focus on the problem.",Machine Learning Zoomcamp,2022,
619839,"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that.",Machine Learning Zoomcamp,2022,
123896,"Working, I guess. So yeah – work after courses. That's where I see you.",Machine Learning Zoomcamp,2022,
86769,"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio.",Machine Learning Zoomcamp,2022,
630862,"I will defer this question to Ankush. He's unfortunately not here right now, so maybe ask him in Slack. From what I understood, it better describes the internals of Kafka. You don't need to know any Java. He explains everything you need. No, you will not need to use Java for your project. You can, but you don't have to.",Data Engineering Zoomcamp,2023,
867705,"Alexey
I guess that's a question about our ETL that we wrote – why we wrote first to GCS and didn’t skip it.
Jeff
I think this comes up at some point when talking about having your data lakes. A lot of times, it's good to put the raw (or fairly close to raw) data into a data lake and then you have the opportunity to go and get it if you need to do something with it. That’s kind of the really raw unprocessed data. And then you can take it from there into BigQuery and use it in BigQuery for people who want to go and do some analytics work with it. There are a lot of different workflows these days. A lot of things are happening more directly with BigQuery. Things going right into data warehouses or “data lake houses” as are often referred to. These are both Google products – GCS and BigQuery – and they're just getting more and more tightly integrated as time goes by. We're able to do more and more and just actually store BigQuery data in Google Cloud Storage, and just kind of access it directly. There are just so many different ways that it happens, but a common workflow has been to first put the fairly raw data into the lake and then do things with it later in the warehouse. That's the flow that we're showing here. Ankush, any comments?
Ankush
I think, as you said, this is just about different patterns. One pattern is obviously writing directly, one is writing to GCS and then to BigQuery. And obviously, everything has its advantages and disadvantages. But writing to a data lake always gives you a certain backup or an advantage, per se. Because now if you want to move away from BigQuery and use Postgres or use Snowflake, you don't really have to care – you can start from GCS all over again. What happens if BigQuery is not accessible or it's getting expensive for machine learning load, and then you can directly read from GCS other than actually doing it through BigQuery. Right now, our example is very small and it does make sense to directly load it to BigQuery. 
But what happens if your data size is increasing on a daily basis, or a weekly basis, and you just cannot put that into BigQuery? All your data is very unstructured and you need to spend the time to figure out what kind of data you want to put into BigQuery. So all of these questions really come up when you have a lot of data coming and a big variety of data coming in. Using something like a GCS bucket, where you can just dump your unstructured raw data format, and then later on, maybe six months down the line, you figure out, “Hey, this data is really important. I want to put it into analytics. Let me pull this into BigQuery with some work.” And I think that's a great pattern to start with. I would always suggest looking into this pattern this way, but obviously, there are also companies and some data sources, where it does make sense to directly put into your data warehouse or BigQuery.",Data Engineering Zoomcamp,2023,
366701,"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have.",Data Engineering Zoomcamp,2023,
521840,"Alexey
Ankush will probably be a better person to answer that. Maybe ask that in Slack and see what the answer is. From what I know (the way we organize it where my work is) each department or team has its own S3 bucket. Inside this S3 bucket, we have different folders, or prefixes, for different data sources – for different projects, let's say. And then inside each project, there could be different specific data sources. And in these data sources, data is usually partitioned by day, but there could be other partitions, too. So there are partitions inside these folders and then, eventually, there are parquet files. I hope that answers your question. There are probably articles about this. What I described is something that our data engineers did. I did not take part in that. I just use it. Maybe if anyone knows any good articles about that, please share them in Slack.",Data Engineering Zoomcamp,2023,
770524,"For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.",Data Engineering Zoomcamp,2023,
647021,"Alexey
I guess this is related to Prefect, right? Please ask this in Slack. Jeff is monitoring Slack, so he will answer that.",Data Engineering Zoomcamp,2023,
944884,"I need more information to actually help you here. I don't think I will have enough time to explain it here. What you can do is maybe check out “regularization Andrew Ng” via Google. He is really good at explaining regularization. He's explaining it from a different point of view, but the explanation he gives is really good. So check it out. 
The problem we had here in this course is – sometimes you have correlated variables, when you have one variable and another variable mean the same thing. When this happens, you cannot invert the matrix. That's why we use regularization here. But it has a much wider scope. And Andrew Ng’s explanation will help you. If you have specific things that you did not understand, please ask in Slack, or maybe next time. But this is such a broad question that I do not know how to best answer and help you.",Machine Learning Zoomcamp,2022,
